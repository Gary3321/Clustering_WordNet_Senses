{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7\n"
     ]
    }
   ],
   "source": [
    "#WordNet 2.1\n",
    "import nltk\n",
    "from nltk.corpus import WordNetCorpusReader\n",
    "wn2 = WordNetCorpusReader(\"/Users/gary/Documents/perl/package/WordNet-2.1/dict\", nltk.data.find(\"/Users/gary/Documents/perl/package/WordNet-2.1/dict\"))\n",
    "\n",
    "wn17 = WordNetCorpusReader(\"/Users/gary/Documents/NLP/WordNetVersions/wordnet-1.7/dict\", nltk.data.find(\"/Users/gary/Documents/NLP/WordNetVersions/wordnet-1.6/dict\"))\n",
    "\n",
    "print(wn17.get_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1619"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "799+820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract sense cluster from the first file\n",
    "path='/Users/gary/Documents/2020Fall/IntroNLP/project/DataSet/Senseval-2/'\n",
    "\n",
    "senseval_file1=path+'corpora/english-lex-sample/train/eng-lex-sample.training.key'\n",
    "\n",
    "sf1_senses=[]\n",
    "sf1=open(senseval_file1,'r')\n",
    "lines = sf1.readlines()\n",
    "#extract sense to a list\n",
    "for line in lines:\n",
    "    line=line.strip('\\n')\n",
    "    #print(line.strip('\\n'))\n",
    "    line=line.split(' ')[2:]\n",
    "    #print(line)\n",
    "    sf1_senses.append(line) #list of list\n",
    "    \n",
    "#remove duplicated sense keys\n",
    "senses_sf1=[]\n",
    "for s1 in sf1_senses:\n",
    "    if s1 not in senses_sf1:\n",
    "        senses_sf1.append(s1)\n",
    "#assign a group id to each sense key\n",
    "df_sensekeys_sf1=pd.DataFrame(columns=['SenseKeys','Group'])\n",
    "groupid=1\n",
    "sense_dict={}\n",
    "\n",
    "for ss in senses_sf1:\n",
    "    flag_in=False\n",
    "    #print(ss)\n",
    "    for s in ss:\n",
    "        if s in sense_dict.keys():\n",
    "            flag_in=True\n",
    "            gpid = sense_dict[s]\n",
    "            break\n",
    "    for s in ss:\n",
    "        if len(s)>5:\n",
    "            #print(s,groupid)\n",
    "            if flag_in:\n",
    "                sense_dict[s]=gpid\n",
    "            else:\n",
    "                sense_dict[s]=groupid\n",
    "            \n",
    "                \n",
    "            #df_sensekeys_sf1=df_sensekeys_sf1.append({'SenseKeys':s,'Group':groupid},ignore_index=True)\n",
    "    groupid+=1\n",
    "\n",
    "df_sensekeys_sf1=pd.DataFrame(columns=['SenseKeys','Group'])\n",
    "for key, value in sense_dict.items():\n",
    "    df_sensekeys_sf1=df_sensekeys_sf1.append({'SenseKeys':key,'Group':value},ignore_index=True)\n",
    "\n",
    "df_sensekeys_sf1.to_csv('df_senseval_sf1.csv',index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SenseKeys</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>art%1:06:00::</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art_gallery%1:06:00::</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>art%1:04:00::</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts%1:09:00::</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>art_form%1:09:00::</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>work_on%2:36:00::</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>work%2:41:04::</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>work_on%2:41:00::</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>work%2:35:14::</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>work%2:38:02::</td>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 SenseKeys  Group\n",
       "0            art%1:06:00::      1\n",
       "1    art_gallery%1:06:00::      2\n",
       "2            art%1:04:00::      1\n",
       "3           arts%1:09:00::      1\n",
       "4       art_form%1:09:00::      5\n",
       "..                     ...    ...\n",
       "794      work_on%2:36:00::   1048\n",
       "795         work%2:41:04::   1049\n",
       "796      work_on%2:41:00::   1050\n",
       "797         work%2:35:14::   1053\n",
       "798         work%2:38:02::   1054\n",
       "\n",
       "[799 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sensekeys_sf1=pd.read_csv('df_senseval_sf1.csv')\n",
    "df_sensekeys_sf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('art.n.01')\n"
     ]
    }
   ],
   "source": [
    "headword='fine_art'\n",
    "syn=''\n",
    "sensekey='fine_art%1:06:00::'\n",
    "for syt in wn17.synsets(headword):\n",
    "        if len(syn)>2:\n",
    "            break\n",
    "        #if syt.pos() in ['a','s','n']:\n",
    "        for lemma in syt.lemmas():\n",
    "            if lemma.key()==sensekey:\n",
    "                print(syt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SenseKeys</th>\n",
       "      <th>Group</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Synset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>art%1:06:00::</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>art#n#01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art_gallery%1:06:00::</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>gallery#n#03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>art%1:04:00::</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>art#n#02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts%1:09:00::</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>humanistic_discipline#n#01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>art_form%1:09:00::</td>\n",
       "      <td>5</td>\n",
       "      <td>n</td>\n",
       "      <td>art_form#n#01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>work_on%2:36:00::</td>\n",
       "      <td>1048</td>\n",
       "      <td>v</td>\n",
       "      <td>work#v#05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>work%2:41:04::</td>\n",
       "      <td>1049</td>\n",
       "      <td>v</td>\n",
       "      <td>work#v#08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>work_on%2:41:00::</td>\n",
       "      <td>1050</td>\n",
       "      <td>v</td>\n",
       "      <td>work_at#v#01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>work%2:35:14::</td>\n",
       "      <td>1053</td>\n",
       "      <td>v</td>\n",
       "      <td>work#v#21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>work%2:38:02::</td>\n",
       "      <td>1054</td>\n",
       "      <td>v</td>\n",
       "      <td>work#v#10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 SenseKeys Group Pos                      Synset\n",
       "0            art%1:06:00::     1   n                    art#n#01\n",
       "1    art_gallery%1:06:00::     2   n                gallery#n#03\n",
       "2            art%1:04:00::     1   n                    art#n#02\n",
       "3           arts%1:09:00::     1   n  humanistic_discipline#n#01\n",
       "4       art_form%1:09:00::     5   n               art_form#n#01\n",
       "..                     ...   ...  ..                         ...\n",
       "794      work_on%2:36:00::  1048   v                   work#v#05\n",
       "795         work%2:41:04::  1049   v                   work#v#08\n",
       "796      work_on%2:41:00::  1050   v                work_at#v#01\n",
       "797         work%2:35:14::  1053   v                   work#v#21\n",
       "798         work%2:38:02::  1054   v                   work#v#10\n",
       "\n",
       "[799 rows x 4 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add synset by sensekey\n",
    "#based on ReadMe file, the sensekys are from nouns and adjectives.\n",
    "def add_synset(df_data):\n",
    "    for i in range(len(df_data)):\n",
    "        sensekey=df_data.loc[df_data.index[i],'SenseKeys']\n",
    "        headword = sensekey.split('%')[0]\n",
    "        syn=''\n",
    "        for syt in wn17.synsets(headword):\n",
    "            if len(syn)>2:\n",
    "                break\n",
    "            #if syt.pos() in ['a','s','n']:\n",
    "            for lemma in syt.lemmas():\n",
    "                if lemma.key()==sensekey:\n",
    "                    syn=syt.name()\n",
    "                    pos=syn.split('.')[1]\n",
    "                    syn=syn.replace('.','#')\n",
    "                    df_data.loc[df_data.index[i],'Pos']=pos\n",
    "                    df_data.loc[df_data.index[i],'Synset']=syn\n",
    "                    break\n",
    "    return df_data\n",
    "\n",
    "df_sensekeys_sf1 = add_synset(df_sensekeys_sf1)\n",
    "\n",
    "df_sensekeys_sf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NaN synset\n",
    "df_sensekeys_sf1 = df_sensekeys_sf1[df_sensekeys_sf1['Synset'].notna()]\n",
    "\n",
    "df_sf1_duplicate=df_sensekeys_sf1.groupby(by=['Synset'],as_index=False, sort=False)['Pos'].count()\n",
    "synsf1_dp_list=df_sf1_duplicate[df_sf1_duplicate['Pos']>1]['Synset'].to_list()\n",
    "\n",
    "#make duplicated synset have the same group\n",
    "for i in range(len(synsf1_dp_list)):\n",
    "    gpsf1_dp_list=df_sensekeys_sf1[df_sensekeys_sf1['Synset']==synsf1_dp_list[i]]['Group'].to_list()\n",
    "    for p in gpsf1_dp_list[1:]:\n",
    "        df_sensekeys_sf1['Group'].replace(p,gpsf1_dp_list[0],inplace=True)\n",
    "\n",
    "#drop duplicated synset\n",
    "df_sensekeys_sf1= df_sensekeys_sf1.drop_duplicates(subset=['Synset'], keep='first')\n",
    "\n",
    "df_sensekeys_sf1['WordPos']=df_sensekeys_sf1['Synset'].str.split('#').str[0]+'-'+df_sensekeys_sf1['Pos']\n",
    "\n",
    "df_sensekeys_sf1.to_csv('Senseval2_sf1_raw.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100/473\n",
      "processing 200/473\n",
      "processing 300/473\n",
      "processing 400/473\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Sense1</th>\n",
       "      <th>Sense2</th>\n",
       "      <th>Merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>wander#v#01</td>\n",
       "      <td>wander#v#03</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#20</td>\n",
       "      <td>draw#v#08</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#20</td>\n",
       "      <td>draw#v#11</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#20</td>\n",
       "      <td>draw#v#06</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>n</td>\n",
       "      <td>workday#n#02</td>\n",
       "      <td>workday#n#01</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>v</td>\n",
       "      <td>discover#v#04</td>\n",
       "      <td>discover#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pos         Sense1         Sense2       Merge\n",
       "0     v    wander#v#01    wander#v#03  not-merged\n",
       "1     v   observe#v#08   observe#v#09  not-merged\n",
       "2     v      draw#v#20      draw#v#08  not-merged\n",
       "3     v      draw#v#20      draw#v#11  not-merged\n",
       "4     v      draw#v#20      draw#v#06  not-merged\n",
       "..   ..            ...            ...         ...\n",
       "908   n   workday#n#02   workday#n#01  not-merged\n",
       "909   v   draw_up#v#02   draw_up#v#03  not-merged\n",
       "910   v   draw_up#v#02   draw_up#v#05  not-merged\n",
       "911   v   draw_up#v#03   draw_up#v#05  not-merged\n",
       "912   v  discover#v#04  discover#v#02  not-merged\n",
       "\n",
       "[913 rows x 4 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate sense pairs\n",
    "df_sensekeys_sf1=pd.read_csv('Senseval2_sf1_raw.csv')\n",
    "df_sensevasf1_pair=pd.DataFrame(columns=['Pos','Sense1','Sense2','Merge'])\n",
    "df_sensevasf1_pair=generate_sensepair(df_sensekeys_sf1,df_sensevasf1_pair)\n",
    "df_sensevasf1_pair.to_csv('Senseval2_sf1_sense_pair.csv',index=False)\n",
    "df_sensevasf1_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sensevasf1_pair['sens12']=df_sensevasf1_pair['Sense1']+'-'+df_sensevasf1_pair['Sense2']\n",
    "len(set(df_sensevasf1_pair['sens12'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 16 10 6 0.375\n",
      "n 127 104 23 0.18110236220472442\n",
      "v 739 724 15 0.02029769959404601\n"
     ]
    }
   ],
   "source": [
    "#analysis of paris\n",
    "df_sensevasf1_pair=pd.read_csv('Senseval2_sf1_sense_pair.csv')\n",
    "total_a=len(df_sensevasf1_pair[df_sensevasf1_pair['Pos']=='a'])\n",
    "total_n=len(df_sensevasf1_pair[df_sensevasf1_pair['Pos']=='n'])\n",
    "total_v=len(df_sensevasf1_pair[df_sensevasf1_pair['Pos']=='v'])\n",
    "\n",
    "notmerge_a=len(df_sensevasf1_pair[(df_sensevasf1_pair['Pos']=='a')&(df_sensevasf1_pair['Merge']=='not-merged')])\n",
    "notmerge_n=len(df_sensevasf1_pair[(df_sensevasf1_pair['Pos']=='n')&(df_sensevasf1_pair['Merge']=='not-merged')])\n",
    "notmerge_v=len(df_sensevasf1_pair[(df_sensevasf1_pair['Pos']=='v')&(df_sensevasf1_pair['Merge']=='not-merged')])\n",
    "\n",
    "merge_a=len(df_sensevasf1_pair[(df_sensevasf1_pair['Pos']=='a')&(df_sensevasf1_pair['Merge']=='merged')])\n",
    "merge_n=len(df_sensevasf1_pair[(df_sensevasf1_pair['Pos']=='n')&(df_sensevasf1_pair['Merge']=='merged')])\n",
    "merge_v=len(df_sensevasf1_pair[(df_sensevasf1_pair['Pos']=='v')&(df_sensevasf1_pair['Merge']=='merged')])\n",
    "\n",
    "rate_a=merge_a/total_a\n",
    "rate_n=merge_n/total_n\n",
    "rate_v=merge_v/total_v\n",
    "print('a',total_a, notmerge_a, merge_a, rate_a)\n",
    "print('n',total_n, notmerge_n, merge_n, rate_n)\n",
    "print('v',total_v, notmerge_v, merge_v, rate_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0/913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>sense1_key17</th>\n",
       "      <th>sense1_wn17</th>\n",
       "      <th>sense2_wn17</th>\n",
       "      <th>sense2_key17</th>\n",
       "      <th>sense1_wn21</th>\n",
       "      <th>sense2_wn21</th>\n",
       "      <th>offset1_wn21</th>\n",
       "      <th>offset2_wn21</th>\n",
       "      <th>sense1_wn17_def</th>\n",
       "      <th>sense1_wn21_def</th>\n",
       "      <th>sense2_wn17_def</th>\n",
       "      <th>sense2_wn21_def</th>\n",
       "      <th>Merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>wander%2:38:00::</td>\n",
       "      <td>wander.v.01</td>\n",
       "      <td>wander.v.03</td>\n",
       "      <td>wander%2:38:02::</td>\n",
       "      <td>roll.v.12</td>\n",
       "      <td>wander.v.03</td>\n",
       "      <td>01863577-v</td>\n",
       "      <td>02083938-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>move about aimlessly or without any destinatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>go via an indirect route or at no set pace</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe%2:31:00::</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>observe%2:41:04::</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>00724225-v</td>\n",
       "      <td>02554084-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>observe correctly or closely</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conform one's action or practice to</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>draw%2:36:02::</td>\n",
       "      <td>draw.v.20</td>\n",
       "      <td>draw.v.08</td>\n",
       "      <td>draw%2:38:00::</td>\n",
       "      <td>draw.v.19</td>\n",
       "      <td>draw.v.07</td>\n",
       "      <td>01628770-v</td>\n",
       "      <td>01836719-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engage in drawing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>take liquid out of a container or well</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>draw%2:36:02::</td>\n",
       "      <td>draw.v.20</td>\n",
       "      <td>draw.v.11</td>\n",
       "      <td>draw%2:37:01::</td>\n",
       "      <td>draw.v.19</td>\n",
       "      <td>draw.v.10</td>\n",
       "      <td>01628770-v</td>\n",
       "      <td>01743542-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engage in drawing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elicit responses, such as objections, criticis...</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>draw%2:36:02::</td>\n",
       "      <td>draw.v.20</td>\n",
       "      <td>draw.v.06</td>\n",
       "      <td>draw%2:38:01::</td>\n",
       "      <td>draw.v.19</td>\n",
       "      <td>draw.v.05</td>\n",
       "      <td>01628770-v</td>\n",
       "      <td>01976850-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engage in drawing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bring, take, or pull out of a container or fro...</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>n</td>\n",
       "      <td>workday%1:28:00::</td>\n",
       "      <td>workday.n.02</td>\n",
       "      <td>workday.n.01</td>\n",
       "      <td>workday%1:28:01::</td>\n",
       "      <td>workday.n.02</td>\n",
       "      <td>workday.n.01</td>\n",
       "      <td>14937834-n</td>\n",
       "      <td>14938034-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the amount of time that a worker must work for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a day on which work is done</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up%2:38:01::</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>01845990-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cause (a vehicle) to stop</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up.v.05</td>\n",
       "      <td>draw_up%2:38:00::</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up.v.05</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>01845738-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>NaN</td>\n",
       "      <td>come to a halt after driving somewhere</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:01::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.05</td>\n",
       "      <td>draw_up%2:38:00::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.05</td>\n",
       "      <td>01845990-v</td>\n",
       "      <td>01845738-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cause (a vehicle) to stop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>come to a halt after driving somewhere</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>v</td>\n",
       "      <td>discover%2:31:00::</td>\n",
       "      <td>discover.v.04</td>\n",
       "      <td>discover.v.02</td>\n",
       "      <td>discover%2:36:00::</td>\n",
       "      <td>discover.v.04</td>\n",
       "      <td>discover.v.02</td>\n",
       "      <td>00713143-v</td>\n",
       "      <td>01623484-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a discovery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a discovery, make a new finding</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pos        sense1_key17    sense1_wn17    sense2_wn17        sense2_key17  \\\n",
       "0     v    wander%2:38:00::    wander.v.01    wander.v.03    wander%2:38:02::   \n",
       "1     v   observe%2:31:00::   observe.v.08   observe.v.09   observe%2:41:04::   \n",
       "2     v      draw%2:36:02::      draw.v.20      draw.v.08      draw%2:38:00::   \n",
       "3     v      draw%2:36:02::      draw.v.20      draw.v.11      draw%2:37:01::   \n",
       "4     v      draw%2:36:02::      draw.v.20      draw.v.06      draw%2:38:01::   \n",
       "..   ..                 ...            ...            ...                 ...   \n",
       "908   n   workday%1:28:00::   workday.n.02   workday.n.01   workday%1:28:01::   \n",
       "909   v   draw_up%2:38:02::   draw_up.v.02   draw_up.v.03   draw_up%2:38:01::   \n",
       "910   v   draw_up%2:38:02::   draw_up.v.02   draw_up.v.05   draw_up%2:38:00::   \n",
       "911   v   draw_up%2:38:01::   draw_up.v.03   draw_up.v.05   draw_up%2:38:00::   \n",
       "912   v  discover%2:31:00::  discover.v.04  discover.v.02  discover%2:36:00::   \n",
       "\n",
       "       sense1_wn21    sense2_wn21 offset1_wn21 offset2_wn21 sense1_wn17_def  \\\n",
       "0        roll.v.12    wander.v.03   01863577-v   02083938-v             NaN   \n",
       "1     observe.v.08   observe.v.09   00724225-v   02554084-v             NaN   \n",
       "2        draw.v.19      draw.v.07   01628770-v   01836719-v             NaN   \n",
       "3        draw.v.19      draw.v.10   01628770-v   01743542-v             NaN   \n",
       "4        draw.v.19      draw.v.05   01628770-v   01976850-v             NaN   \n",
       "..             ...            ...          ...          ...             ...   \n",
       "908   workday.n.02   workday.n.01   14937834-n   14938034-n             NaN   \n",
       "909   draw_up.v.02   draw_up.v.03   01964369-v   01845990-v             NaN   \n",
       "910   draw_up.v.02   draw_up.v.05   01964369-v   01845738-v             NaN   \n",
       "911   draw_up.v.03   draw_up.v.05   01845990-v   01845738-v             NaN   \n",
       "912  discover.v.04  discover.v.02   00713143-v   01623484-v             NaN   \n",
       "\n",
       "                                       sense1_wn21_def sense2_wn17_def  \\\n",
       "0    move about aimlessly or without any destinatio...             NaN   \n",
       "1                         observe correctly or closely             NaN   \n",
       "2                                    engage in drawing             NaN   \n",
       "3                                    engage in drawing             NaN   \n",
       "4                                    engage in drawing             NaN   \n",
       "..                                                 ...             ...   \n",
       "908  the amount of time that a worker must work for...             NaN   \n",
       "909                                 straighten oneself             NaN   \n",
       "910                                 straighten oneself             NaN   \n",
       "911                          cause (a vehicle) to stop             NaN   \n",
       "912                                   make a discovery             NaN   \n",
       "\n",
       "                                       sense2_wn21_def       Merge  \n",
       "0           go via an indirect route or at no set pace  not-merged  \n",
       "1                  conform one's action or practice to  not-merged  \n",
       "2               take liquid out of a container or well  not-merged  \n",
       "3    elicit responses, such as objections, criticis...  not-merged  \n",
       "4    bring, take, or pull out of a container or fro...  not-merged  \n",
       "..                                                 ...         ...  \n",
       "908                        a day on which work is done  not-merged  \n",
       "909                          cause (a vehicle) to stop  not-merged  \n",
       "910             come to a halt after driving somewhere  not-merged  \n",
       "911             come to a halt after driving somewhere  not-merged  \n",
       "912               make a discovery, make a new finding  not-merged  \n",
       "\n",
       "[913 rows x 14 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map senseva wn1.7 to wn2.1 \n",
    "df_pairs=pd.read_csv('Senseval2_sf1_sense_pair.csv')\n",
    "\n",
    "df_wn17wn21_sf1=pd.DataFrame(columns=['Pos','sense1_key17','sense1_wn17','sense2_wn17','sense2_key17','sense1_wn21','sense2_wn21','offset1_wn21','offset2_wn21','sense1_wn17_def','sense1_wn21_def','sense2_wn17_def','sense2_wn21_def','Merge'])\n",
    "\n",
    "#df_pairs_n=df_pairs[df_pairs['Pos']=='n']\n",
    "#total_pair=len(df_pairs_n)\n",
    "total_pair=len(df_pairs)\n",
    "df_senseval2=pd.read_csv('Senseval2_sf1_raw.csv')\n",
    "#df_senseval2[df_senseval2['Synset']=='pull#v#12']['SenseKeys'].values[0]\n",
    "\n",
    "for i in range(total_pair):\n",
    "    if (i%1000 ==0):\n",
    "        print('processing {}/{}'.format(i,total_pair))\n",
    "    sense1_wn17 = df_pairs.loc[df_pairs.index[i],'Sense1']\n",
    "    headword=sense1_wn17.split('#')[0]\n",
    "    sense1_wn17 = sense1_wn17.replace('#','.')\n",
    "    sense2_wn17 = df_pairs.loc[df_pairs.index[i],'Sense2']\n",
    "    merge= df_pairs.loc[df_pairs.index[i],'Merge'] \n",
    "    sense2_wn17 = sense2_wn17.replace('#','.')\n",
    "    pos = df_pairs.loc[df_pairs.index[i],'Pos']\n",
    "    \n",
    "    lemmas=''\n",
    "    sense1_wn2=''\n",
    "    sense1_wn2_def=''\n",
    "    offset1_wn2=''\n",
    "    for lemma1 in wn17.synset(sense1_wn17).lemmas():\n",
    "        if len(sense1_wn2)>2:\n",
    "            break\n",
    "        sense1_wn17_def = wn17.synset(sense1_wn17).definition()\n",
    "        #get sense key for wn21 sense1\n",
    "        wn17_sense1_key=lemma1.key()\n",
    "        #enumerate all wn synsests for headword#pos,find the same sense key\n",
    "        for synset1 in wn2.synsets(headword, pos):\n",
    "            lemmas = synset1.lemmas()\n",
    "            if len(sense1_wn2)>2:\n",
    "                break\n",
    "            for l1 in lemmas:\n",
    "                #if sense key equal, keep the wn16 synset name\n",
    "                if l1.key() == wn17_sense1_key:\n",
    "                    sense1_wn2=synset1.name()\n",
    "                    sense1_wn2_def= synset1.definition()\n",
    "                    offset1_wn2 = str(synset1.offset()).zfill(8) +'-'+ synset1.pos()\n",
    "                    break\n",
    "\n",
    "    lemmas=''\n",
    "    sense2_wn2=''\n",
    "    offset2_wn2=''\n",
    "    \n",
    "    for lemma2 in wn17.synset(sense2_wn17).lemmas():\n",
    "        if len(sense2_wn2)>2:\n",
    "            break\n",
    "        sense2_wn17_def = wn17.synset(sense2_wn17).definition()\n",
    "        wn17_sense2_key=lemma2.key()\n",
    "        for synset2 in wn2.synsets(headword, pos):\n",
    "            lemmas = synset2.lemmas()\n",
    "            if len(sense2_wn2)>2:\n",
    "                break\n",
    "            for l2 in lemmas:\n",
    "                if l2.key() == wn17_sense2_key:\n",
    "                    sense2_wn2=synset2.name()\n",
    "                    sense2_wn2_def= synset2.definition()\n",
    "                    offset2_wn2 = str(synset2.offset()).zfill(8) +'-'+ synset2.pos()\n",
    "                    break\n",
    "\n",
    "    df_wn17wn21_sf1=df_wn17wn21_sf1.append({'Pos':pos,'sense1_key17':wn17_sense1_key,'sense1_wn17':sense1_wn17,'sense2_wn17':sense2_wn17,'sense2_key17':wn17_sense2_key,'sense1_wn21':sense1_wn2,'sense2_wn21':sense2_wn2,\n",
    "                                    'offset1_wn21':offset1_wn2,'offset2_wn21':offset2_wn2,'sense1_wn21_def':sense1_wn2_def,'sense2_wn21_def':sense2_wn2_def,'Merge':merge},\n",
    "                               ignore_index=True)\n",
    "   \n",
    "    \n",
    "df_wn17wn21_sf1.to_csv('Senseval_WN17WN21full_sf1.csv',index=False)\n",
    "\n",
    "df_wn17wn21_sf1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>sense1_key17</th>\n",
       "      <th>sense1_wn17</th>\n",
       "      <th>sense2_wn17</th>\n",
       "      <th>sense2_key17</th>\n",
       "      <th>sense1_wn21</th>\n",
       "      <th>sense2_wn21</th>\n",
       "      <th>offset1_wn21</th>\n",
       "      <th>offset2_wn21</th>\n",
       "      <th>sense1_wn17_def</th>\n",
       "      <th>sense1_wn21_def</th>\n",
       "      <th>sense2_wn17_def</th>\n",
       "      <th>sense2_wn21_def</th>\n",
       "      <th>Merge</th>\n",
       "      <th>selse2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>wander%2:38:00::</td>\n",
       "      <td>wander.v.01</td>\n",
       "      <td>wander.v.03</td>\n",
       "      <td>wander%2:38:02::</td>\n",
       "      <td>roll#v#12</td>\n",
       "      <td>wander#v#03</td>\n",
       "      <td>01863577-v</td>\n",
       "      <td>02083938-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>move about aimlessly or without any destinatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>go via an indirect route or at no set pace</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>roll#v#12-wander#v#03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe%2:31:00::</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>observe%2:41:04::</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>00724225-v</td>\n",
       "      <td>02554084-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>observe correctly or closely</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conform one's action or practice to</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>observe#v#08-observe#v#09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>draw%2:36:02::</td>\n",
       "      <td>draw.v.20</td>\n",
       "      <td>draw.v.08</td>\n",
       "      <td>draw%2:38:00::</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#07</td>\n",
       "      <td>01628770-v</td>\n",
       "      <td>01836719-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engage in drawing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>take liquid out of a container or well</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw#v#19-draw#v#07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>draw%2:36:02::</td>\n",
       "      <td>draw.v.20</td>\n",
       "      <td>draw.v.11</td>\n",
       "      <td>draw%2:37:01::</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#10</td>\n",
       "      <td>01628770-v</td>\n",
       "      <td>01743542-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engage in drawing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elicit responses, such as objections, criticis...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw#v#19-draw#v#10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>draw%2:36:02::</td>\n",
       "      <td>draw.v.20</td>\n",
       "      <td>draw.v.06</td>\n",
       "      <td>draw%2:38:01::</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#05</td>\n",
       "      <td>01628770-v</td>\n",
       "      <td>01976850-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engage in drawing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bring, take, or pull out of a container or fro...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw#v#19-draw#v#05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>n</td>\n",
       "      <td>workday%1:28:00::</td>\n",
       "      <td>workday.n.02</td>\n",
       "      <td>workday.n.01</td>\n",
       "      <td>workday%1:28:01::</td>\n",
       "      <td>workday#n#02</td>\n",
       "      <td>workday#n#01</td>\n",
       "      <td>14937834-n</td>\n",
       "      <td>14938034-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the amount of time that a worker must work for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a day on which work is done</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>workday#n#02-workday#n#01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up%2:38:01::</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>01845990-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cause (a vehicle) to stop</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw_up#v#02-draw_up#v#03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up.v.05</td>\n",
       "      <td>draw_up%2:38:00::</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>01845738-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>NaN</td>\n",
       "      <td>come to a halt after driving somewhere</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw_up#v#02-draw_up#v#05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:01::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.05</td>\n",
       "      <td>draw_up%2:38:00::</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>01845990-v</td>\n",
       "      <td>01845738-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cause (a vehicle) to stop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>come to a halt after driving somewhere</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw_up#v#03-draw_up#v#05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>v</td>\n",
       "      <td>discover%2:31:00::</td>\n",
       "      <td>discover.v.04</td>\n",
       "      <td>discover.v.02</td>\n",
       "      <td>discover%2:36:00::</td>\n",
       "      <td>discover#v#04</td>\n",
       "      <td>discover#v#02</td>\n",
       "      <td>00713143-v</td>\n",
       "      <td>01623484-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a discovery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a discovery, make a new finding</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>discover#v#04-discover#v#02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>874 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pos        sense1_key17    sense1_wn17    sense2_wn17        sense2_key17  \\\n",
       "0     v    wander%2:38:00::    wander.v.01    wander.v.03    wander%2:38:02::   \n",
       "1     v   observe%2:31:00::   observe.v.08   observe.v.09   observe%2:41:04::   \n",
       "2     v      draw%2:36:02::      draw.v.20      draw.v.08      draw%2:38:00::   \n",
       "3     v      draw%2:36:02::      draw.v.20      draw.v.11      draw%2:37:01::   \n",
       "4     v      draw%2:36:02::      draw.v.20      draw.v.06      draw%2:38:01::   \n",
       "..   ..                 ...            ...            ...                 ...   \n",
       "908   n   workday%1:28:00::   workday.n.02   workday.n.01   workday%1:28:01::   \n",
       "909   v   draw_up%2:38:02::   draw_up.v.02   draw_up.v.03   draw_up%2:38:01::   \n",
       "910   v   draw_up%2:38:02::   draw_up.v.02   draw_up.v.05   draw_up%2:38:00::   \n",
       "911   v   draw_up%2:38:01::   draw_up.v.03   draw_up.v.05   draw_up%2:38:00::   \n",
       "912   v  discover%2:31:00::  discover.v.04  discover.v.02  discover%2:36:00::   \n",
       "\n",
       "       sense1_wn21    sense2_wn21 offset1_wn21 offset2_wn21  sense1_wn17_def  \\\n",
       "0        roll#v#12    wander#v#03   01863577-v   02083938-v              NaN   \n",
       "1     observe#v#08   observe#v#09   00724225-v   02554084-v              NaN   \n",
       "2        draw#v#19      draw#v#07   01628770-v   01836719-v              NaN   \n",
       "3        draw#v#19      draw#v#10   01628770-v   01743542-v              NaN   \n",
       "4        draw#v#19      draw#v#05   01628770-v   01976850-v              NaN   \n",
       "..             ...            ...          ...          ...              ...   \n",
       "908   workday#n#02   workday#n#01   14937834-n   14938034-n              NaN   \n",
       "909   draw_up#v#02   draw_up#v#03   01964369-v   01845990-v              NaN   \n",
       "910   draw_up#v#02   draw_up#v#05   01964369-v   01845738-v              NaN   \n",
       "911   draw_up#v#03   draw_up#v#05   01845990-v   01845738-v              NaN   \n",
       "912  discover#v#04  discover#v#02   00713143-v   01623484-v              NaN   \n",
       "\n",
       "                                       sense1_wn21_def  sense2_wn17_def  \\\n",
       "0    move about aimlessly or without any destinatio...              NaN   \n",
       "1                         observe correctly or closely              NaN   \n",
       "2                                    engage in drawing              NaN   \n",
       "3                                    engage in drawing              NaN   \n",
       "4                                    engage in drawing              NaN   \n",
       "..                                                 ...              ...   \n",
       "908  the amount of time that a worker must work for...              NaN   \n",
       "909                                 straighten oneself              NaN   \n",
       "910                                 straighten oneself              NaN   \n",
       "911                          cause (a vehicle) to stop              NaN   \n",
       "912                                   make a discovery              NaN   \n",
       "\n",
       "                                       sense2_wn21_def       Merge  \\\n",
       "0           go via an indirect route or at no set pace  not-merged   \n",
       "1                  conform one's action or practice to  not-merged   \n",
       "2               take liquid out of a container or well  not-merged   \n",
       "3    elicit responses, such as objections, criticis...  not-merged   \n",
       "4    bring, take, or pull out of a container or fro...  not-merged   \n",
       "..                                                 ...         ...   \n",
       "908                        a day on which work is done  not-merged   \n",
       "909                          cause (a vehicle) to stop  not-merged   \n",
       "910             come to a halt after driving somewhere  not-merged   \n",
       "911             come to a halt after driving somewhere  not-merged   \n",
       "912               make a discovery, make a new finding  not-merged   \n",
       "\n",
       "                          selse2  \n",
       "0          roll#v#12-wander#v#03  \n",
       "1      observe#v#08-observe#v#09  \n",
       "2            draw#v#19-draw#v#07  \n",
       "3            draw#v#19-draw#v#10  \n",
       "4            draw#v#19-draw#v#05  \n",
       "..                           ...  \n",
       "908    workday#n#02-workday#n#01  \n",
       "909    draw_up#v#02-draw_up#v#03  \n",
       "910    draw_up#v#02-draw_up#v#05  \n",
       "911    draw_up#v#03-draw_up#v#05  \n",
       "912  discover#v#04-discover#v#02  \n",
       "\n",
       "[874 rows x 15 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove sense pair in OntoNotes\n",
    "features_path='/Users/gary/Documents/2020Fall/IntroNLP/project/'\n",
    "df_onetonote_features= pd.read_csv(features_path+'feature_space.csv')\n",
    "df_otf=df_onetonote_features[['Sense1','Sense2']]\n",
    "df_otf['se1se2']=df_otf['Sense1']+'-'+df_otf['Sense2']\n",
    "\n",
    "otf_set=set(df_otf['se1se2'].to_list())\n",
    "\n",
    "df_senseva_WN21=pd.read_csv('Senseval_WN17WN21full_sf1.csv')\n",
    "df_senseva_WN21['sense1_wn21']=df_senseva_WN21['sense1_wn21'].str.replace('.','#')\n",
    "df_senseva_WN21['sense2_wn21']=df_senseva_WN21['sense2_wn21'].str.replace('.','#')\n",
    "\n",
    "df_senseva_WN21['selse2']=df_senseva_WN21['sense1_wn21']+'-'+df_senseva_WN21['sense2_wn21']\n",
    "\n",
    "senseva_set=set(df_senseva_WN21['selse2'].to_list())\n",
    "\n",
    "#delete overlap from senseva set\n",
    "\n",
    "\n",
    "senseva_set - otf_set\n",
    "\n",
    "df_senseva_WN21=df_senseva_WN21[df_senseva_WN21['selse2'].isin(list(senseva_set - otf_set))]\n",
    "\n",
    "\n",
    "df_senseva_WN21=df_senseva_WN21[df_senseva_WN21['sense1_wn21'].notna()]\n",
    "df_senseva_WN21=df_senseva_WN21[df_senseva_WN21['sense2_wn21'].notna()]\n",
    "df_senseva_WN21.to_csv('Senseval2_sense_pair_W21_nooverlap_sf1.csv',index=False)\n",
    "\n",
    "df_senseva_WN21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features using perl language\n",
    "df_senseva_WN21_sf1=pd.read_csv('Senseval2_sense_pair_W21_nooverlap_sf1.csv')\n",
    "df_senseva_pair21_sf1=df_senseva_WN21_sf1[['Pos','sense1_wn21','sense2_wn21','Merge']]\n",
    "df_senseva_pair21_sf1.columns=['Pos','Sense1','Sense2','Merge']\n",
    "df_senseva_pair21_sf1.to_csv('Senseva2_sensepairs_wn21_sf1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build feature space:\n",
    "1, twin: the number of shared synonyms between two synsets\n",
    "2, antonym: whether two synsets share an antonym\n",
    "3, pertainym: whether two synsets share an pertainym\n",
    "4, deriv: whether two synsets share derivationally related forms\n",
    "5, verbgrp: whether two verb synsets are linked in a VERBGROUP (indicating semantic similarity)\n",
    "6, verbfrm: whether two verb synsets share a VERBFRAM (indicating syntactic similarity) \n",
    "\n",
    "'''\n",
    "#wordnet 2.1\n",
    "df_pairs=pd.read_csv('Senseva2_sensepairs_wn21_sf1.csv')\n",
    "#df_pairs\n",
    "for i in range(len(df_pairs)):\n",
    "    pertainyms1=[]\n",
    "    antonyms1=[]\n",
    "    deriv1=[]\n",
    "    lemmas1=[]\n",
    "    verbgroup1=[]\n",
    "    verbframe1=[]\n",
    "    pertainyms2=[]\n",
    "    antonyms2=[]\n",
    "    deriv2=[]\n",
    "    lemmas2=[]\n",
    "    verbgroup2=[]\n",
    "    verbframe2=[]\n",
    "    pertainymsflag=''\n",
    "    antonymsflag=''\n",
    "    derivflag=''\n",
    "    lemmasflag=''\n",
    "    verbgroupflag=''\n",
    "    verbframeflag=''\n",
    "    hyper_min=''\n",
    "    hyper_max=''\n",
    "\n",
    "    sense1 = df_pairs.loc[df_pairs.index[i],'Sense1']\n",
    "    sense1 = sense1.replace('#','.')\n",
    "    sense2 = df_pairs.loc[df_pairs.index[i],'Sense2']\n",
    "    sense2 = sense2.replace('#','.')\n",
    "    pos = df_pairs.loc[df_pairs.index[i],'Pos']\n",
    "    \n",
    "    # sense1's twin, pertainyms, antonyms, derivationally_related_forms,\n",
    "    # verb group, frame id, hyper_min, hyper_max\n",
    "    try:\n",
    "        \n",
    "        #WN, WNMax feature\n",
    "        #calculate hyper distance\n",
    "        sense1_hyper = wn2.synset(sense1)\n",
    "        sense1_hypers = lambda s: s.hypernyms()\n",
    "        hyper1= list(sense1_hyper.closure(sense1_hypers))\n",
    "        \n",
    "        sense2_hyper = wn2.synset(sense2)\n",
    "        sense2_hypers = lambda s: s.hypernyms()\n",
    "        hyper2 = list(sense2_hyper.closure(sense2_hypers))\n",
    "           \n",
    "        #find the nearest hyper, and average the distance as the least distance\n",
    "        for h1 in hyper1:\n",
    "            if h1 in hyper2:\n",
    "                hyper_min = (hyper1.index(h1)+hyper2.index(h1))/2\n",
    "                break\n",
    "        \n",
    "\n",
    "\n",
    "        #find the farest hyper, and average the distance as the largest distance\n",
    "        for h1 in reversed(hyper1):\n",
    "            if h1 in hyper2:\n",
    "                hyper_max = (hyper1.index(h1)+hyper2.index(h1))/2\n",
    "                break\n",
    "                \n",
    "        \n",
    "        for lemma in wn2.synset(sense1).lemmas():\n",
    "            \n",
    "            pertainyms1=pertainyms1+lemma.pertainyms()\n",
    "            antonyms1=antonyms1+lemma.antonyms()\n",
    "            deriv1=deriv1+lemma.derivationally_related_forms()\n",
    "            lemmas1.append(lemma.name())\n",
    "            \n",
    "            if pos=='v':\n",
    "                verbgroup1=verbgroup1+lemma.verb_groups()\n",
    "                verbframe1=verbframe1+lemma.frame_ids()\n",
    "\n",
    "                \n",
    "        # sense2's pertainyms, antonyms, derivationally_related_forms\n",
    "        for lemma in wn2.synset(sense2).lemmas():\n",
    "            \n",
    "            pertainyms2=pertainyms2+lemma.pertainyms()\n",
    "            antonyms2=antonyms2+lemma.antonyms()\n",
    "            deriv2=deriv2+lemma.derivationally_related_forms()\n",
    "            lemmas2.append(lemma.name())\n",
    "             \n",
    "            #verb group, verb frame\n",
    "            if pos=='v':\n",
    "                verbgroup2=verbgroup2+lemma.verb_groups()\n",
    "                verbframe2=verbframe2+lemma.frame_ids()\n",
    "                \n",
    "      \n",
    "        if (len(set(pertainyms1)&set(pertainyms2))>0):\n",
    "            pertainymsflag=1\n",
    "        else:\n",
    "            pertainymsflag=0\n",
    "\n",
    "        if (len(set(antonyms1)&set(antonyms2))>0):\n",
    "            antonymsflag=1\n",
    "        else:\n",
    "            antonymsflag=0 \n",
    "\n",
    "        if (len(set(deriv1)&set(deriv2))>0):\n",
    "            derivflag=1\n",
    "        else:\n",
    "            derivflag=0 \n",
    "        \n",
    "        #verb group, verb frame\n",
    "        if pos=='v':\n",
    "            if (len(set(verbgroup1)&set(verbgroup2))>0):\n",
    "                verbgroupflag=1\n",
    "            else:\n",
    "                verbgroupflag=0\n",
    "                \n",
    "            if (len(set(verbframe1)&set(verbframe2))>0):\n",
    "                verbframeflag=1\n",
    "            else:\n",
    "                verbframeflag=0\n",
    "            \n",
    "\n",
    "        \n",
    "        lemmasflag=len(set(lemmas1)&set(lemmas2))\n",
    "       \n",
    "        \n",
    "        df_pairs.loc[df_pairs.index[i],'pertainyms']=pertainymsflag\n",
    "        df_pairs.loc[df_pairs.index[i],'antonyms']=antonymsflag\n",
    "        df_pairs.loc[df_pairs.index[i],'deriv']=derivflag\n",
    "        df_pairs.loc[df_pairs.index[i],'lemmas']=lemmasflag\n",
    "        \n",
    "        df_pairs.loc[df_pairs.index[i],'verbgroup']=verbgroupflag\n",
    "        df_pairs.loc[df_pairs.index[i],'verbframe']=verbframeflag\n",
    "         \n",
    "        df_pairs.loc[df_pairs.index[i],'hyper_min']=hyper_min\n",
    "        df_pairs.loc[df_pairs.index[i],'hyper_max']=hyper_max\n",
    "\n",
    "        \n",
    "    except:\n",
    "        None\n",
    "    \n",
    "df_pairs.to_csv('Senseva2_SensesPairs_WNFeatures_sf1.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0/874\n",
      "processing 0/874\n"
     ]
    }
   ],
   "source": [
    "#create feature based on mapping between WN and OED, the mapping is based on WN21\n",
    "#read mapping file\n",
    "file_map_wnoed=open('/Users/gary/Documents/2020Fall/IntroNLP/project/FeatureSpace/sense_clusters-21.senses','r')\n",
    "\n",
    "#read sysnset pairs\n",
    "df_pairs=pd.read_csv('Senseva2_sensepairs_wn21_sf1.csv')\n",
    "\n",
    "\n",
    "#df_pairs_n=df_pairs[df_pairs['Pos']=='n']\n",
    "#total_pair=len(df_pairs_n)\n",
    "total_pair=len(df_pairs)\n",
    "for i in range(total_pair):\n",
    "    if (i%1000 ==0):\n",
    "        print('processing {}/{}'.format(i,total_pair))\n",
    "    sense1_wn21 = df_pairs.loc[df_pairs.index[i],'Sense1']\n",
    "    headword=sense1_wn21.split('#')[0]\n",
    "    sense1_wn21 = sense1_wn21.replace('#','.')\n",
    "    sense2_wn21 = df_pairs.loc[df_pairs.index[i],'Sense2']\n",
    "    sense2_wn21 = sense2_wn21.replace('#','.')\n",
    "    pos = df_pairs.loc[df_pairs.index[i],'Pos']\n",
    "    \n",
    "    sense1_keys=''\n",
    "    sense2_keys=''\n",
    "    try:\n",
    "        for lemma1 in wn2.synset(sense1_wn21).lemmas():\n",
    "            #get sense key for wn21 sense1\n",
    "            sense1_keys=sense1_keys+lemma1.key()+';'\n",
    "        for lemma2 in wn2.synset(sense2_wn21).lemmas():\n",
    "            #get sense key for wn21 sense1\n",
    "            sense2_keys=sense2_keys+lemma1.key()+';'\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    df_pairs.loc[df_pairs.index[i],'Sense1_keys']=sense1_keys\n",
    "    df_pairs.loc[df_pairs.index[i],'Sense2_keys']=sense2_keys\n",
    "\n",
    "#filter sense keys is null\n",
    "df_pairs = df_pairs[(df_pairs['Sense1_keys'].str.len()>5)&(df_pairs['Sense1_keys'].str.len()>5)]\n",
    "\n",
    "#get sense clusterings from the mapping between WN and OED\n",
    "lines = file_map_wnoed.readlines()\n",
    "count=0\n",
    "wnoed_lst=[] # a list of sets.\n",
    "for line in lines:\n",
    "    count+=1\n",
    "    line=line.strip('\\n')\n",
    "    if ' ' in line:\n",
    "        wnoed_lst.append(set(line.split(' ')))\n",
    "\n",
    "for i in range(len(df_pairs)):\n",
    "    if i%1000 ==0:\n",
    "        print('processing {}/{}'.format(i,total_pair))\n",
    "    sense1_key=df_pairs.loc[df_pairs.index[i],'Sense1_keys']\n",
    "    sk1_set=set(sense1_key.split(';'))\n",
    "    \n",
    "    sense2_key=df_pairs.loc[df_pairs.index[i],'Sense2_keys']\n",
    "    sk2_set=set(sense2_key.split(';'))\n",
    "    \n",
    "    feature=0\n",
    "    for  w in wnoed_lst:\n",
    "        sk1_len=len(sk1_set & w)\n",
    "        sk2_len=len(sk2_set & w)\n",
    "        if (sk1_len>0 and sk2_len>0):\n",
    "            feature=1\n",
    "            break\n",
    "    \n",
    "    df_pairs.loc[df_pairs.index[i],'wn_oed_feature']=feature\n",
    "    \n",
    "df_pairs.to_csv('WN_OED_Map_Feature_Senseva2_SensesPairs_sf1.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sensepari 874\n",
      "df_wnpackage 875\n",
      "df_features_tmp 874\n",
      "df_wncorpus 874\n",
      "df_features_tmp1 874\n",
      "df_wnoedfea 874\n",
      "df_features 874\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Sense1</th>\n",
       "      <th>Sense2</th>\n",
       "      <th>Merge</th>\n",
       "      <th>lch</th>\n",
       "      <th>hso</th>\n",
       "      <th>jcn</th>\n",
       "      <th>leskvalue</th>\n",
       "      <th>linvalue</th>\n",
       "      <th>resvalue</th>\n",
       "      <th>...</th>\n",
       "      <th>wupvalue</th>\n",
       "      <th>pertainyms</th>\n",
       "      <th>antonyms</th>\n",
       "      <th>deriv</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>verbgroup</th>\n",
       "      <th>verbframe</th>\n",
       "      <th>hyper_min</th>\n",
       "      <th>hyper_max</th>\n",
       "      <th>wn_oed_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>roll#v#12</td>\n",
       "      <td>wander#v#03</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.318246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>2.233592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#07</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.540445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047931</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#10</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045832</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#05</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.540445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048413</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>n</td>\n",
       "      <td>workday#n#02</td>\n",
       "      <td>workday#n#01</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.558145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.514798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046550</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.722767</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.046550</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>v</td>\n",
       "      <td>discover#v#04</td>\n",
       "      <td>discover#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>874 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pos         Sense1         Sense2       Merge       lch  hso       jcn  \\\n",
       "0     v      roll#v#12    wander#v#03  not-merged  1.945910  4.0  0.000000   \n",
       "1     v   observe#v#08   observe#v#09  not-merged  2.233592  0.0  0.053396   \n",
       "2     v      draw#v#19      draw#v#07  not-merged  1.540445  0.0  0.047931   \n",
       "3     v      draw#v#19      draw#v#10  not-merged  1.386294  0.0  0.045832   \n",
       "4     v      draw#v#19      draw#v#05  not-merged  1.540445  0.0  0.048413   \n",
       "..   ..            ...            ...         ...       ...  ...       ...   \n",
       "869   n   workday#n#02   workday#n#01  not-merged  1.558145  0.0  0.000000   \n",
       "870   v   draw_up#v#02   draw_up#v#03  not-merged  1.386294  0.0  0.046550   \n",
       "871   v   draw_up#v#02   draw_up#v#05  not-merged  1.386294  0.0  0.051186   \n",
       "872   v   draw_up#v#03   draw_up#v#05  not-merged  1.722767  4.0  0.046550   \n",
       "873   v  discover#v#04  discover#v#02  not-merged  1.386294  0.0  0.064222   \n",
       "\n",
       "     leskvalue  linvalue  resvalue  ...  wupvalue  pertainyms  antonyms  \\\n",
       "0         10.0       0.0  3.318246  ...  0.571429         0.0       0.0   \n",
       "1          9.0       0.0  0.000000  ...  0.500000         0.0       0.0   \n",
       "2          8.0       0.0  0.000000  ...  0.285714         0.0       0.0   \n",
       "3          2.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "4          5.0       0.0  0.000000  ...  0.285714         0.0       0.0   \n",
       "..         ...       ...       ...  ...       ...         ...       ...   \n",
       "869        7.0       0.0  3.514798  ...  0.666667         0.0       0.0   \n",
       "870        4.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "871        4.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "872       23.0       0.0  0.000000  ...  0.333333         0.0       0.0   \n",
       "873       12.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "\n",
       "     deriv  lemmas  verbgroup  verbframe  hyper_min  hyper_max  wn_oed_feature  \n",
       "0      0.0     1.0        0.0        1.0        0.5        0.5             0.0  \n",
       "1      0.0     2.0        0.0        1.0        NaN        NaN             0.0  \n",
       "2      1.0     1.0        0.0        0.0        NaN        NaN             1.0  \n",
       "3      0.0     1.0        0.0        0.0        NaN        NaN             1.0  \n",
       "4      1.0     1.0        0.0        0.0        NaN        NaN             1.0  \n",
       "..     ...     ...        ...        ...        ...        ...             ...  \n",
       "869    0.0     2.0        NaN        NaN        2.5        7.5             1.0  \n",
       "870    0.0     2.0        0.0        0.0        NaN        NaN             0.0  \n",
       "871    0.0     2.0        0.0        0.0        NaN        NaN             0.0  \n",
       "872    0.0     2.0        0.0        0.0        NaN        NaN             0.0  \n",
       "873    1.0     2.0        0.0        1.0        NaN        NaN             1.0  \n",
       "\n",
       "[874 rows x 21 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatnate all the features together.\n",
    "\n",
    "features_path='/Users/gary/Documents/2020Fall/IntroNLP/project/'\n",
    "feature_files=['Senseva2_sensepairs_wn21_sf1.csv','Senseva2_pairs_wnsimilarity_sf1.csv','Senseva2_SensesPairs_WNFeatures_sf1.csv','WN_OED_Map_Feature_Senseva2_SensesPairs_sf1.csv']\n",
    "\n",
    "\n",
    "df_sensepari=pd.read_csv(features_path+feature_files[0])\n",
    "df_sensepari= df_sensepari[['Pos', 'Sense1', 'Sense2', 'Merge']]\n",
    "df_sensepari=df_sensepari.drop_duplicates()\n",
    "\n",
    "print('df_sensepari',len(df_sensepari))\n",
    "df_wnpackage=pd.read_csv(features_path+feature_files[1])\n",
    "df_wnpackage=df_wnpackage.drop_duplicates()\n",
    "print('df_wnpackage',len(df_wnpackage))\n",
    "\n",
    "df_features_tmp = df_sensepari.merge(df_wnpackage,left_on=['Sense1', 'Sense2','Merge'], \n",
    "                              right_on=['sense1', 'sense2','merge'], how='left')\n",
    "\n",
    "\n",
    "df_features_tmp=df_features_tmp[['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "       'lch', 'hso', 'jcn', 'leskvalue', 'linvalue', 'resvalue', 'vecvalue',\n",
    "       'wupvalue']]\n",
    "df_features_tmp=df_features_tmp.drop_duplicates()\n",
    "print('df_features_tmp',len(df_features_tmp))\n",
    "\n",
    "df_wncorpus=pd.read_csv(features_path+feature_files[2])\n",
    "\n",
    "print('df_wncorpus',len(df_wncorpus))\n",
    "\n",
    "df_features_tmp1 = df_features_tmp.merge(df_wncorpus,left_on=['Pos', 'Sense1', 'Sense2', 'Merge'], \n",
    "                              right_on=['Pos', 'Sense1', 'Sense2', 'Merge'], how='left')\n",
    "\n",
    "df_features_tmp1.columns\n",
    "print('df_features_tmp1',len(df_features_tmp1))\n",
    "\n",
    "\n",
    "df_features_tmp1=df_features_tmp1[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
    "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', \n",
    "        'pertainyms', 'antonyms', 'deriv', 'lemmas',\n",
    "       'verbgroup', 'verbframe', 'hyper_min', 'hyper_max']]\n",
    "\n",
    "df_features_tmp1=df_features_tmp1.drop_duplicates() \n",
    "\n",
    "#wn-oed mapping feature\n",
    "df_wnoedfea=pd.read_csv(features_path+feature_files[3])\n",
    "df_wnoedfea\n",
    "print('df_wnoedfea',len(df_wnoedfea))\n",
    "df_features = df_features_tmp1.merge(df_wnoedfea[['Pos', 'Sense1', 'Sense2', 'Merge','wn_oed_feature']],left_on=['Pos', 'Sense1', 'Sense2','Merge'], \n",
    "                              right_on=['Pos', 'Sense1', 'Sense2', 'Merge'], how='left')\n",
    "\n",
    "df_features=df_features[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
    "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', 'pertainyms',\n",
    "       'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe', 'hyper_min',\n",
    "       'hyper_max', 'wn_oed_feature']]\n",
    "\n",
    "df_features=df_features.drop_duplicates()\n",
    "df_features\n",
    "print('df_features',len(df_features))\n",
    "df_features.to_csv('senseval2_feature_space_sf1.csv',index=False)\n",
    "df_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attack%5:00:00:offensive:03']\n",
      "['bum%5:00:00:inferior:02']\n",
      "['county%3:01:00::']\n",
      "['material%5:00:00:physical:00', '3', 'material%5:00:00:worldly:00']\n",
      "material%5:00:00:physical:00 3\n",
      "material%5:00:00:worldly:00 3\n",
      "material%3:00:03:: 3\n",
      "['material%3:00:04::', '2', 'material%3:00:01::']\n",
      "material%3:00:04:: 4\n",
      "material%3:00:01:: 4\n",
      "['material%3:00:02::']\n",
      "['post%5:00:00:succeeding(a):00']\n",
      "['present%3:00:01::']\n",
      "['present%3:00:02::']\n",
      "['present%3:01:00::']\n",
      "['stone%3:01:00::']\n",
      "['stone%5:00:00:chromatic:00']\n",
      "['air%1:15:00::', '4', 'air%1:27:00::']\n",
      "air%1:15:00:: 12\n",
      "air%1:27:00:: 12\n",
      "air%1:19:00:: 12\n",
      "air%1:27:01:: 12\n",
      "['air%1:04:00::']\n",
      "['air%1:10:02::']\n",
      "['air%1:07:00::']\n",
      "['air%1:10:01::']\n",
      "['appeal%1:04:00::', '3', 'appeal%1:10:00::']\n",
      "appeal%1:04:00:: 17\n",
      "appeal%1:10:00:: 17\n",
      "appeal%1:10:02:: 17\n",
      "['appeal%1:07:00::']\n",
      "['art%1:10:00::', '2', 'art%1:06:00::']\n",
      "art%1:10:00:: 19\n",
      "art%1:06:00:: 19\n",
      "['art%1:09:00::', '2', 'art%1:04:00::']\n",
      "art%1:09:00:: 20\n",
      "art%1:04:00:: 20\n",
      "['attack%1:10:00::', '4', 'attack%1:04:00::']\n",
      "attack%1:10:00:: 21\n",
      "attack%1:04:00:: 21\n",
      "attack%1:04:02:: 21\n",
      "attack%1:04:04:: 21\n",
      "['attack%1:22:00::', '2', 'attack%1:26:00::']\n",
      "attack%1:22:00:: 22\n",
      "attack%1:26:00:: 22\n",
      "['attack%1:04:03::']\n",
      "['attack%1:04:01::']\n",
      "['authority%1:07:02::', '2', 'authority%1:07:00::']\n",
      "authority%1:07:02:: 25\n",
      "authority%1:07:00:: 25\n",
      "['authority%1:14:00::', '2', 'authority%1:18:01::']\n",
      "authority%1:14:00:: 26\n",
      "authority%1:18:01:: 26\n",
      "['authority%1:10:00::', '2', 'authority%1:18:00::']\n",
      "authority%1:10:00:: 27\n",
      "authority%1:18:00:: 27\n",
      "['authority%1:09:00::']\n",
      "['bar%1:06:05::', '3', 'bar%1:06:04::']\n",
      "bar%1:06:05:: 29\n",
      "bar%1:06:04:: 29\n",
      "bar%1:06:07:: 29\n",
      "['bar%1:06:02::', '3', 'bar%1:06:00::']\n",
      "bar%1:06:02:: 30\n",
      "bar%1:06:00:: 30\n",
      "bar%1:06:06:: 30\n",
      "['bar%1:14:00::']\n",
      "['bar%1:10:00::']\n",
      "['bar%1:04:00::']\n",
      "['bar%1:23:00::']\n",
      "['bar%1:17:00::']\n",
      "['bar%1:14:00::']\n",
      "['bar%1:06:01::']\n",
      "['bar%1:06:08::']\n",
      "['basin%1:06:01::', '2', 'basin%1:06:00::']\n",
      "basin%1:06:01:: 39\n",
      "basin%1:06:00:: 39\n",
      "['basin%1:15:00::', '2', 'basin%1:17:00::']\n",
      "basin%1:15:00:: 40\n",
      "basin%1:17:00:: 40\n",
      "['basin%1:23:00::']\n",
      "['bum%1:18:01::', '3', 'bum%1:18:02::']\n",
      "bum%1:18:01:: 42\n",
      "bum%1:18:02:: 42\n",
      "bum%1:18:03:: 42\n",
      "['bum%1:08:00::']\n",
      "['call%2:32:00::', '4', 'call%2:32:02::']\n",
      "call%2:32:00:: 44\n",
      "call%2:32:02:: 44\n",
      "call%2:31:05:: 44\n",
      "call%2:41:14:: 44\n",
      "['call%2:32:04::', '3', 'call%2:32:01::']\n",
      "call%2:32:04:: 45\n",
      "call%2:32:01:: 45\n",
      "call%2:29:05:: 45\n",
      "['call%2:41:04::', '4', 'call%2:32:05::']\n",
      "call%2:41:04:: 46\n",
      "call%2:32:05:: 46\n",
      "call%2:32:15:: 46\n",
      "call%2:32:14:: 46\n",
      "['call%2:32:13::', '3', 'call%2:32:09::']\n",
      "call%2:32:13:: 47\n",
      "call%2:32:09:: 47\n",
      "call%2:31:13:: 47\n",
      "['call%2:41:12::', '2', 'call%2:41:00::']\n",
      "call%2:41:12:: 48\n",
      "call%2:41:00:: 48\n",
      "['call%2:32:08::', '4', 'call%2:32:06::']\n",
      "call%2:32:08:: 49\n",
      "call%2:32:06:: 49\n",
      "call%2:38:00:: 49\n",
      "call%2:40:05:: 49\n",
      "['call%2:32:11::', '2', 'call%2:32:03::']\n",
      "call%2:32:11:: 50\n",
      "call%2:32:03:: 50\n",
      "['call%2:31:00::', '2', 'call%2:32:07::']\n",
      "call%2:31:00:: 51\n",
      "call%2:32:07:: 51\n",
      "['call%2:32:12::', '2', 'call%2:40:09::']\n",
      "call%2:32:12:: 52\n",
      "call%2:40:09:: 52\n",
      "['call%2:42:09::']\n",
      "['call%2:32:10::']\n",
      "['call_in%2:32:02::', '2', 'call_in%2:35:00::']\n",
      "call_in%2:32:02:: 55\n",
      "call_in%2:35:00:: 55\n",
      "['call_in%2:40:00::', '2', 'call_in%2:41:02::']\n",
      "call_in%2:40:00:: 56\n",
      "call_in%2:41:02:: 56\n",
      "['call_in%2:41:00::']\n",
      "['call_in%2:32:00::']\n",
      "['call_in%2:40:02::']\n",
      "['carry%2:35:08::', '4', 'carry%2:35:02::']\n",
      "carry%2:35:08:: 60\n",
      "carry%2:35:02:: 60\n",
      "carry%2:32:07:: 60\n",
      "carry%2:35:12:: 60\n",
      "['carry%2:42:03::', '4', 'carry%2:42:01::']\n",
      "carry%2:42:03:: 61\n",
      "carry%2:42:01:: 61\n",
      "carry%2:42:12:: 61\n",
      "carry%2:29:00:: 61\n",
      "['carry%2:32:02::', '2', 'carry%2:38:03::']\n",
      "carry%2:32:02:: 62\n",
      "carry%2:38:03:: 62\n",
      "['carry%2:42:14::', '3', 'carry%2:35:09::']\n",
      "carry%2:42:14:: 63\n",
      "carry%2:35:09:: 63\n",
      "carry%2:41:06:: 63\n",
      "['carry%2:41:01::', '2', 'carry%2:35:01::']\n",
      "carry%2:41:01:: 64\n",
      "carry%2:35:01:: 64\n",
      "['carry%2:30:05::', '2', 'carry%2:41:00::']\n",
      "carry%2:30:05:: 65\n",
      "carry%2:41:00:: 65\n",
      "['carry%2:42:13::', '3', 'carry%2:42:02::']\n",
      "carry%2:42:13:: 66\n",
      "carry%2:42:02:: 66\n",
      "carry%2:40:10:: 66\n",
      "['carry%2:41:02::', '5', 'carry%2:33:01::']\n",
      "carry%2:41:02:: 67\n",
      "carry%2:33:01:: 67\n",
      "carry%2:33:02:: 67\n",
      "carry%2:40:04:: 67\n",
      "carry%2:33:12:: 67\n",
      "['carry%2:40:00::', '3', 'carry%2:31:12::']\n",
      "carry%2:40:00:: 68\n",
      "carry%2:31:12:: 68\n",
      "carry%2:36:00:: 68\n",
      "['carry%2:33:04::', '3', 'carry%2:42:15::']\n",
      "carry%2:33:04:: 69\n",
      "carry%2:42:15:: 69\n",
      "carry%2:33:03:: 69\n",
      "['carry%2:38:10::', '2', 'carry%2:40:13::']\n",
      "carry%2:38:10:: 70\n",
      "carry%2:40:13:: 70\n",
      "['carry%2:34:01::', '2', 'carry%2:36:08::']\n",
      "carry%2:34:01:: 71\n",
      "carry%2:36:08:: 71\n",
      "['carry%2:41:12::']\n",
      "['carry%2:38:07::']\n",
      "['carry%2:34:00::']\n",
      "['carry%2:32:00::']\n",
      "['chair%1:06:01::', '2', 'chair%1:06:00::']\n",
      "chair%1:06:01:: 76\n",
      "chair%1:06:00:: 76\n",
      "['chair%1:18:00::', '2', 'chair%1:04:00::']\n",
      "chair%1:18:00:: 77\n",
      "chair%1:04:00:: 77\n",
      "['channel%1:06:02::', '2', 'channel%1:10:00::']\n",
      "channel%1:06:02:: 78\n",
      "channel%1:10:00:: 78\n",
      "['channel%1:17:00::', '3', 'channel%1:06:00::']\n",
      "channel%1:17:00:: 79\n",
      "channel%1:06:00:: 79\n",
      "channel%1:08:00:: 79\n",
      "['channel%1:25:00::']\n",
      "['channel%1:10:01::']\n",
      "['child%1:18:06::', '2', 'child%1:18:00::']\n",
      "child%1:18:06:: 82\n",
      "child%1:18:00:: 82\n",
      "['child%1:18:07::', '2', 'child%1:18:01::']\n",
      "child%1:18:07:: 83\n",
      "child%1:18:01:: 83\n",
      "['church%1:14:00::']\n",
      "['church%1:06:00::']\n",
      "['church%1:04:00::']\n",
      "['circuit%1:15:00::', '2', 'circuit%1:04:00::']\n",
      "circuit%1:15:00:: 87\n",
      "circuit%1:04:00:: 87\n",
      "['circuit%1:04:01::', '2', 'circuit%1:06:01::']\n",
      "circuit%1:04:01:: 88\n",
      "circuit%1:06:01:: 88\n",
      "['circuit%1:06:00::']\n",
      "['circuit%1:14:00::']\n",
      "['county%1:15:00::']\n",
      "['day%1:28:07::', '3', 'day%1:28:00::']\n",
      "day%1:28:07:: 92\n",
      "day%1:28:00:: 92\n",
      "day%1:28:06:: 92\n",
      "['day%1:28:04::', '2', 'day%1:28:02::']\n",
      "day%1:28:04:: 93\n",
      "day%1:28:02:: 93\n",
      "['day%1:26:00::', '2', 'day%1:28:05::']\n",
      "day%1:26:00:: 94\n",
      "day%1:28:05:: 94\n",
      "['day%1:28:03::']\n",
      "['day%1:28:01::']\n",
      "['day%1:18:00::']\n",
      "['detention%1:26:00::']\n",
      "['detention%1:04:00::']\n",
      "['develop%2:36:00::', '2', 'develop%2:36:01::']\n",
      "develop%2:36:00:: 100\n",
      "develop%2:36:01:: 100\n",
      "['develop%2:29:00::', '2', 'develop%2:30:06::']\n",
      "develop%2:29:00:: 101\n",
      "develop%2:30:06:: 101\n",
      "['develop%2:30:05::', '5', 'develop%2:42:00::']\n",
      "develop%2:30:05:: 102\n",
      "develop%2:42:00:: 102\n",
      "develop%2:30:00:: 102\n",
      "develop%2:30:10:: 102\n",
      "develop%2:30:03:: 102\n",
      "['develop%2:32:00::', '6', 'develop%2:30:04::']\n",
      "develop%2:32:00:: 103\n",
      "develop%2:30:04:: 103\n",
      "develop%2:31:00:: 103\n",
      "develop%2:30:09:: 103\n",
      "develop%2:30:01:: 103\n",
      "develop%2:36:09:: 103\n",
      "['develop%2:30:13::', '2', 'develop%2:32:00::']\n",
      "develop%2:30:13:: 104\n",
      "develop%2:32:00:: 104\n",
      "['develop%2:33:02::', '2', 'develop%2:33:03::']\n",
      "develop%2:33:02:: 105\n",
      "develop%2:33:03:: 105\n",
      "['develop%2:39:00::']\n",
      "['develop%2:35:04::']\n",
      "['develop%2:30:12::']\n",
      "['distress%1:26:01::', '3', 'distress%1:12:02::']\n",
      "distress%1:26:01:: 109\n",
      "distress%1:12:02:: 109\n",
      "distress%1:26:00:: 109\n",
      "['distress%1:04:00::']\n",
      "['draw%2:35:02::', '3', 'draw%2:35:03::']\n",
      "draw%2:35:02:: 111\n",
      "draw%2:35:03:: 111\n",
      "draw%2:35:09:: 111\n",
      "['draw%2:30:14::', '2', 'draw%2:37:03::']\n",
      "draw%2:30:14:: 112\n",
      "draw%2:37:03:: 112\n",
      "['draw%2:37:01::', '3', 'draw%2:40:00::']\n",
      "draw%2:37:01:: 113\n",
      "draw%2:40:00:: 113\n",
      "draw%2:35:04:: 113\n",
      "['draw%2:31:00::', '5', 'draw%2:35:01::']\n",
      "draw%2:31:00:: 114\n",
      "draw%2:35:01:: 114\n",
      "draw%2:36:00:: 114\n",
      "draw%2:32:00:: 114\n",
      "draw%2:36:02:: 114\n",
      "['draw%2:38:00::', '9', 'draw%2:38:01::']\n",
      "draw%2:38:00:: 115\n",
      "draw%2:38:01:: 115\n",
      "draw%2:31:13:: 115\n",
      "draw%2:34:01:: 115\n",
      "draw%2:40:01:: 115\n",
      "draw%2:33:01:: 115\n",
      "draw%2:38:07:: 115\n",
      "draw%2:42:00:: 115\n",
      "draw%2:29:00:: 115\n",
      "['draw%2:35:13::', '2', 'draw%2:35:14::']\n",
      "draw%2:35:13:: 116\n",
      "draw%2:35:14:: 116\n",
      "['draw%2:30:15::', '3', 'draw%2:30:08::']\n",
      "draw%2:30:15:: 117\n",
      "draw%2:30:08:: 117\n",
      "draw%2:30:04:: 117\n",
      "['draw%2:38:02::']\n",
      "['draw%2:36:01::']\n",
      "['draw%2:36:07::']\n",
      "['draw%2:42:02::']\n",
      "['draw%2:41:13::']\n",
      "['draw%2:33:00::']\n",
      "['draw%2:30:00::']\n",
      "['draw%2:30:05::']\n",
      "['draw_in%2:35:01::', '2', 'draw_in%2:35:02::']\n",
      "draw_in%2:35:01:: 126\n",
      "draw_in%2:35:02:: 126\n",
      "['draw_in%2:38:03::', '2', 'draw_in%2:35:03::']\n",
      "draw_in%2:38:03:: 127\n",
      "draw_in%2:35:03:: 127\n",
      "['draw_in%2:38:00::', '2', 'draw_in%2:38:01::']\n",
      "draw_in%2:38:00:: 128\n",
      "draw_in%2:38:01:: 128\n",
      "['draw_in%2:35:00::']\n",
      "['draw_out%2:37:00::', '2', 'draw_out%2:32:00::']\n",
      "draw_out%2:37:00:: 130\n",
      "draw_out%2:32:00:: 130\n",
      "['draw_out%2:35:05::', '2', 'draw_out%2:36:00::']\n",
      "draw_out%2:35:05:: 131\n",
      "draw_out%2:36:00:: 131\n",
      "['draw_out%2:30:00::']\n",
      "['draw_up%2:38:00::', '2', 'draw_up%2:38:01::']\n",
      "draw_up%2:38:00:: 133\n",
      "draw_up%2:38:01:: 133\n",
      "['draw_up%2:41:00::']\n",
      "['draw_up%2:38:02::']\n",
      "['draw_up%2:31:00::']\n",
      "['dress%2:29:01::', '4', 'dress%2:29:00::']\n",
      "dress%2:29:01:: 137\n",
      "dress%2:29:00:: 137\n",
      "dress%2:29:07:: 137\n",
      "dress%2:29:05:: 137\n",
      "['dress%2:36:00::', '3', 'dress%2:30:00::']\n",
      "dress%2:36:00:: 138\n",
      "dress%2:30:00:: 138\n",
      "dress%2:35:00:: 138\n",
      "['dress%2:35:03::', '4', 'dress%2:35:04::']\n",
      "dress%2:35:03:: 139\n",
      "dress%2:35:04:: 139\n",
      "dress%2:29:06:: 139\n",
      "dress%2:29:04:: 139\n",
      "['dress%2:35:02::']\n",
      "['dress%2:38:07::']\n",
      "['dress%2:30:08::']\n",
      "['dress%2:29:02::']\n",
      "['dress_up%2:29:04::', '3', 'dress_up%2:29:00::']\n",
      "dress_up%2:29:04:: 144\n",
      "dress_up%2:29:00:: 144\n",
      "dress_up%2:29:06:: 144\n",
      "['dress_up%2:30:00::']\n",
      "['dress_up%2:36:00::']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drift%2:38:06::', '2', 'drift%2:38:02::']\n",
      "drift%2:38:06:: 147\n",
      "drift%2:38:02:: 147\n",
      "['drift%2:38:01::', '2', 'drift%2:38:04::']\n",
      "drift%2:38:01:: 148\n",
      "drift%2:38:04:: 148\n",
      "['drift%2:30:10::', '2', 'drift%2:42:03::']\n",
      "drift%2:30:10:: 149\n",
      "drift%2:42:03:: 149\n",
      "['drift%2:38:05::', '2', 'drift%2:42:00::']\n",
      "drift%2:38:05:: 150\n",
      "drift%2:42:00:: 150\n",
      "['drift%2:35:03::']\n",
      "['drift%2:30:00::']\n",
      "['drive%2:38:00::', '7', 'drive%2:38:01::']\n",
      "drive%2:38:00:: 153\n",
      "drive%2:38:01:: 153\n",
      "drive%2:38:02:: 153\n",
      "drive%2:42:00:: 153\n",
      "drive%2:41:13:: 153\n",
      "drive%2:38:09:: 153\n",
      "drive%2:38:11:: 153\n",
      "['drive%2:41:02::', '3', 'drive%2:35:00::']\n",
      "drive%2:41:02:: 154\n",
      "drive%2:35:00:: 154\n",
      "drive%2:36:00:: 154\n",
      "['drive%2:35:01::', '4', 'drive%2:35:11::']\n",
      "drive%2:35:01:: 155\n",
      "drive%2:35:11:: 155\n",
      "drive%2:35:03:: 155\n",
      "drive%2:35:02:: 155\n",
      "['drive%2:33:01::', '3', 'drive%2:35:07::']\n",
      "drive%2:33:01:: 156\n",
      "drive%2:35:07:: 156\n",
      "drive%2:33:00:: 156\n",
      "['drive%2:41:00::']\n",
      "['drive%2:32:00::']\n",
      "['drive%2:35:06::']\n",
      "['drive%2:34:07::']\n",
      "['dyke%1:18:00::']\n",
      "['dyke%1:06:00::']\n",
      "['extent%1:07:00::', '2', 'extent%1:26:00::']\n",
      "extent%1:07:00:: 163\n",
      "extent%1:26:00:: 163\n",
      "['face%2:33:00::', '3', 'face%2:32:00::']\n",
      "face%2:33:00:: 164\n",
      "face%2:32:00:: 164\n",
      "face%2:32:03:: 164\n",
      "['face%2:42:02::', '2', 'face%2:42:00::']\n",
      "face%2:42:02:: 165\n",
      "face%2:42:00:: 165\n",
      "['face%2:39:04::', '2', 'face%2:38:00::']\n",
      "face%2:39:04:: 166\n",
      "face%2:38:00:: 166\n",
      "['face%2:35:01::', '2', 'face%2:35:00::']\n",
      "face%2:35:01:: 167\n",
      "face%2:35:00:: 167\n",
      "['facility%1:04:01::', '3', 'facility%1:06:00::']\n",
      "facility%1:04:01:: 168\n",
      "facility%1:06:00:: 168\n",
      "facility%1:04:00:: 168\n",
      "['facility%1:07:00::', '2', 'facility%1:09:00::']\n",
      "facility%1:07:00:: 169\n",
      "facility%1:09:00:: 169\n",
      "['fatigue%1:12:00::', '2', 'fatigue%1:26:00::']\n",
      "fatigue%1:12:00:: 170\n",
      "fatigue%1:26:00:: 170\n",
      "['fatigue%1:26:02::']\n",
      "['fatigue%1:04:00::']\n",
      "['feeling%1:09:00::', '2', 'feeling%1:09:03::']\n",
      "feeling%1:09:00:: 173\n",
      "feeling%1:09:03:: 173\n",
      "['feeling%1:09:02::', '2', 'feeling%1:09:01::']\n",
      "feeling%1:09:02:: 174\n",
      "feeling%1:09:01:: 174\n",
      "['feeling%1:03:00::']\n",
      "['feeling%1:26:00::']\n",
      "['find%2:32:00::', '4', 'find%2:39:02::']\n",
      "find%2:32:00:: 177\n",
      "find%2:39:02:: 177\n",
      "find%2:36:00:: 177\n",
      "find%2:31:09:: 177\n",
      "['find%2:40:01::', '4', 'find%2:40:00::']\n",
      "find%2:40:01:: 178\n",
      "find%2:40:00:: 178\n",
      "find%2:40:03:: 178\n",
      "find%2:40:15:: 178\n",
      "['find%2:32:01::', '2', 'find%2:31:10::']\n",
      "find%2:32:01:: 179\n",
      "find%2:31:10:: 179\n",
      "['find%2:39:05::', '2', 'find%2:39:01::']\n",
      "find%2:39:05:: 180\n",
      "find%2:39:01:: 180\n",
      "['find%2:40:02::']\n",
      "['find%2:30:00::']\n",
      "['find%2:38:10::']\n",
      "['find%2:30:13::']\n",
      "['fish%1:05:00::']\n",
      "['fish%1:13:00::']\n",
      "['grip%1:07:00::', '2', 'grip%1:04:00::']\n",
      "grip%1:07:00:: 187\n",
      "grip%1:04:00:: 187\n",
      "['grip%1:06:00::', '2', 'grip%1:06:01::']\n",
      "grip%1:06:00:: 188\n",
      "grip%1:06:01:: 188\n",
      "['grip%1:19:00::']\n",
      "['grip%1:18:00::']\n",
      "['grip%1:06:02::']\n",
      "['hearth%1:15:00::', '2', 'hearth%1:06:01::']\n",
      "hearth%1:15:00:: 192\n",
      "hearth%1:06:01:: 192\n",
      "['hearth%1:06:00::']\n",
      "['holiday%1:28:01::', '2', 'holiday%1:28:00::']\n",
      "holiday%1:28:01:: 194\n",
      "holiday%1:28:00:: 194\n",
      "['importance%1:26:00::', '2', 'importance%1:07:00::']\n",
      "importance%1:26:00:: 195\n",
      "importance%1:07:00:: 195\n",
      "['keep%2:41:02::', '2', 'keep%2:42:00::']\n",
      "keep%2:41:02:: 196\n",
      "keep%2:42:00:: 196\n",
      "['keep%2:40:09::', '2', 'keep%2:40:00::']\n",
      "keep%2:40:09:: 197\n",
      "keep%2:40:00:: 197\n",
      "['keep%2:41:00::', '3', 'keep%2:41:01::']\n",
      "keep%2:41:00:: 198\n",
      "keep%2:41:01:: 198\n",
      "keep%2:35:10:: 198\n",
      "['keep%2:41:05::', '2', 'keep%2:41:03::']\n",
      "keep%2:41:05:: 199\n",
      "keep%2:41:03:: 199\n",
      "['keep%2:42:02::', '3', 'keep%2:40:01::']\n",
      "keep%2:42:02:: 200\n",
      "keep%2:40:01:: 200\n",
      "keep%2:42:12:: 200\n",
      "['keep%2:34:00::', '2', 'keep%2:42:01::']\n",
      "keep%2:34:00:: 201\n",
      "keep%2:42:01:: 201\n",
      "['keep%2:30:10::', '2', 'keep%2:42:03::']\n",
      "keep%2:30:10:: 202\n",
      "keep%2:42:03:: 202\n",
      "['keep%2:40:12::', '3', 'keep%2:40:13::']\n",
      "keep%2:40:12:: 203\n",
      "keep%2:40:13:: 203\n",
      "keep%2:40:10:: 203\n",
      "['keep%2:42:07::']\n",
      "['keep%2:31:00::']\n",
      "['keep%2:32:00::']\n",
      "['keep_up%2:40:00::', '2', 'keep_up%2:33:00::']\n",
      "keep_up%2:40:00:: 207\n",
      "keep_up%2:33:00:: 207\n",
      "['keep_up%2:42:00::']\n",
      "['keep_up%2:30:00::']\n",
      "['keep_up%2:29:00::']\n",
      "['lady%1:18:01::', '3', 'lady%1:18:02::']\n",
      "lady%1:18:01:: 211\n",
      "lady%1:18:02:: 211\n",
      "lady%1:18:00:: 211\n",
      "['lap%1:08:00::']\n",
      "['lap%1:26:00::']\n",
      "['lap%1:06:01::']\n",
      "['lap%1:06:00::']\n",
      "['lap%1:04:01::']\n",
      "['lap%1:04:00::']\n",
      "['lap%2:35:00::']\n",
      "['leave%2:38:00::', '3', 'leave%2:38:01::']\n",
      "leave%2:38:00:: 219\n",
      "leave%2:38:01:: 219\n",
      "leave%2:41:00:: 219\n",
      "['leave%2:30:03::', '3', 'leave%2:31:05::']\n",
      "leave%2:30:03:: 220\n",
      "leave%2:31:05:: 220\n",
      "leave%2:31:02:: 220\n",
      "['leave%2:42:00::', '2', 'leave%2:42:02::']\n",
      "leave%2:42:00:: 221\n",
      "leave%2:42:02:: 221\n",
      "['leave%2:40:01::', '3', 'leave%2:40:06::']\n",
      "leave%2:40:01:: 222\n",
      "leave%2:40:06:: 222\n",
      "leave%2:40:02:: 222\n",
      "['leave%2:30:02::']\n",
      "['leave%2:42:01::']\n",
      "['leave%2:42:03::']\n",
      "['leave%2:42:01::']\n",
      "['live%2:42:07::', '2', 'live%2:42:06::']\n",
      "live%2:42:07:: 227\n",
      "live%2:42:06:: 227\n",
      "['live%2:42:01::', '3', 'live%2:42:04::']\n",
      "live%2:42:01:: 228\n",
      "live%2:42:04:: 228\n",
      "live%2:42:00:: 228\n",
      "['live%2:42:08::']\n",
      "['live%2:31:00::']\n",
      "['match%2:42:01::', '3', 'match%2:42:00::']\n",
      "match%2:42:01:: 231\n",
      "match%2:42:00:: 231\n",
      "match%2:41:00:: 231\n",
      "['match%2:30:07::', '2', 'match%2:40:00::']\n",
      "match%2:30:07:: 232\n",
      "match%2:40:00:: 232\n",
      "['match%2:30:00::', '2', 'match%2:30:01::']\n",
      "match%2:30:00:: 233\n",
      "match%2:30:01:: 233\n",
      "['match%2:35:08::']\n",
      "['match%2:33:00::']\n",
      "['material%1:06:00::', '2', 'material%1:27:00::']\n",
      "material%1:06:00:: 236\n",
      "material%1:27:00:: 236\n",
      "['material%1:06:01::', '2', 'material%1:10:00::']\n",
      "material%1:06:01:: 237\n",
      "material%1:10:00:: 237\n",
      "['material%1:18:00::']\n",
      "['mouth%1:08:00::', '2', 'mouth%1:08:01::']\n",
      "mouth%1:08:00:: 239\n",
      "mouth%1:08:01:: 239\n",
      "['mouth%1:17:00::', '3', 'mouth%1:17:01::']\n",
      "mouth%1:17:00:: 240\n",
      "mouth%1:17:01:: 240\n",
      "mouth%1:06:00:: 240\n",
      "['mouth%1:18:00::']\n",
      "['mouth%1:18:01::']\n",
      "['mouth%1:10:00::']\n",
      "['nation%1:14:02::', '2', 'nation%1:14:00::']\n",
      "nation%1:14:02:: 244\n",
      "nation%1:14:00:: 244\n",
      "['nation%1:14:01::']\n",
      "['nation%1:18:00::']\n",
      "['nature%1:07:01::', '2', 'nature%1:07:02::']\n",
      "nature%1:07:01:: 247\n",
      "nature%1:07:02:: 247\n",
      "['nature%1:17:00::', '2', 'nature%1:18:00::']\n",
      "nature%1:17:00:: 248\n",
      "nature%1:18:00:: 248\n",
      "['nature%1:09:00::']\n",
      "['occupation%1:04:04::', '2', 'occupation%1:04:00::']\n",
      "occupation%1:04:04:: 250\n",
      "occupation%1:04:00:: 250\n",
      "['occupation%1:04:01::', '2', 'occupation%1:04:02::']\n",
      "occupation%1:04:01:: 251\n",
      "occupation%1:04:02:: 251\n",
      "['play%2:33:02::', '2', 'play%2:33:00::']\n",
      "play%2:33:02:: 252\n",
      "play%2:33:00:: 252\n",
      "['play%2:41:02::', '2', 'play%2:41:12::']\n",
      "play%2:41:02:: 253\n",
      "play%2:41:12:: 253\n",
      "['play%2:36:00::', '3', 'play%2:36:12::']\n",
      "play%2:36:00:: 254\n",
      "play%2:36:12:: 254\n",
      "play%2:36:01:: 254\n",
      "['play%2:36:03::', '4', 'play%2:36:02::']\n",
      "play%2:36:03:: 255\n",
      "play%2:36:02:: 255\n",
      "play%2:36:10:: 255\n",
      "play%2:36:04:: 255\n",
      "['play%2:41:00::', '3', 'play%2:41:03::']\n",
      "play%2:41:00:: 256\n",
      "play%2:41:03:: 256\n",
      "play%2:36:06:: 256\n",
      "['play%2:33:07::', '3', 'play%2:33:01::']\n",
      "play%2:33:07:: 257\n",
      "play%2:33:01:: 257\n",
      "play%2:33:08:: 257\n",
      "['play%2:36:05::', '2', 'play%2:36:11::']\n",
      "play%2:36:05:: 258\n",
      "play%2:36:11:: 258\n",
      "['play%2:33:09::', '4', 'play%2:33:03::']\n",
      "play%2:33:09:: 259\n",
      "play%2:33:03:: 259\n",
      "play%2:33:13:: 259\n",
      "play%2:33:12:: 259\n",
      "['play%2:34:13::', '4', 'play%2:29:00::']\n",
      "play%2:34:13:: 260\n",
      "play%2:29:00:: 260\n",
      "play%2:31:01:: 260\n",
      "play%2:41:13:: 260\n",
      "['play%2:29:01::']\n",
      "['play%2:38:00::']\n",
      "['play%2:35:00::']\n",
      "['play%2:42:00::']\n",
      "['play%2:38:03::']\n",
      "['play%2:36:13::']\n",
      "['play%2:35:07::']\n",
      "['play%2:29:04::']\n",
      "['post%1:06:01::', '2', 'post%1:15:00::']\n",
      "post%1:06:01:: 269\n",
      "post%1:15:00:: 269\n",
      "['post%1:10:02::', '2', 'post%1:06:00::']\n",
      "post%1:10:02:: 270\n",
      "post%1:06:00:: 270\n",
      "['post%1:10:00::', '3', 'post%1:14:00::']\n",
      "post%1:10:00:: 271\n",
      "post%1:14:00:: 271\n",
      "post%1:04:01:: 271\n",
      "['post%1:04:00::']\n",
      "['present%1:28:00::']\n",
      "['present%1:21:00::']\n",
      "['present%1:10:00::']\n",
      "['pull%2:35:04::', '5', 'pull%2:35:00::']\n",
      "pull%2:35:04:: 276\n",
      "pull%2:35:00:: 276\n",
      "pull%2:38:13:: 276\n",
      "pull%2:38:02:: 276\n",
      "pull%2:35:01:: 276\n",
      "['pull%2:37:00::', '2', 'pull%2:35:02::']\n",
      "pull%2:37:00:: 277\n",
      "pull%2:35:02:: 277\n",
      "['pull%2:38:00::', '2', 'pull%2:38:01::']\n",
      "pull%2:38:00:: 278\n",
      "pull%2:38:01:: 278\n",
      "['pull%2:35:10::', '3', 'pull%2:38:03::']\n",
      "pull%2:35:10:: 279\n",
      "pull%2:38:03:: 279\n",
      "pull%2:30:11:: 279\n",
      "['pull%2:41:00::']\n",
      "['pull%2:29:00::']\n",
      "['pull%2:38:04::']\n",
      "['pull%2:35:01::']\n",
      "['pull%2:35:03::']\n",
      "['pull%2:35:05::']\n",
      "['pull%2:33:13::']\n",
      "['replace%2:41:00::', '3', 'replace%2:30:00::']\n",
      "replace%2:41:00:: 287\n",
      "replace%2:30:00:: 287\n",
      "replace%2:40:00:: 287\n",
      "['replace%2:35:00::']\n",
      "['restraint%1:26:00::', '3', 'restraint%1:04:00::']\n",
      "restraint%1:26:00:: 289\n",
      "restraint%1:04:00:: 289\n",
      "restraint%1:09:00:: 289\n",
      "['restraint%1:07:00::']\n",
      "['restraint%1:07:01::']\n",
      "['restraint%1:06:00::']\n",
      "['royalty%1:21:00::']\n",
      "['royalty%1:14:00::']\n",
      "['see%2:39:03::', '4', 'see%2:39:00::']\n",
      "see%2:39:03:: 295\n",
      "see%2:39:00:: 295\n",
      "see%2:39:11:: 295\n",
      "see%2:39:13:: 295\n",
      "['see%2:36:00::', '2', 'see%2:31:01::']\n",
      "see%2:36:00:: 296\n",
      "see%2:31:01:: 296\n",
      "['see%2:31:02::', '2', 'see%2:39:02::']\n",
      "see%2:31:02:: 297\n",
      "see%2:39:02:: 297\n",
      "['see%2:31:13::', '2', 'see%2:31:00::']\n",
      "see%2:31:13:: 298\n",
      "see%2:31:00:: 298\n",
      "['see%2:31:03::', '3', 'see%2:32:00::']\n",
      "see%2:31:03:: 299\n",
      "see%2:32:00:: 299\n",
      "see%2:41:14:: 299\n",
      "['see%2:35:09::', '2', 'see%2:41:11::']\n",
      "see%2:35:09:: 300\n",
      "see%2:41:11:: 300\n",
      "['see%2:41:12::', '4', 'see%2:38:00::']\n",
      "see%2:41:12:: 301\n",
      "see%2:38:00:: 301\n",
      "see%2:41:00:: 301\n",
      "see%2:38:03:: 301\n",
      "['see%2:41:10::']\n",
      "['see%2:39:05::']\n",
      "['see%2:31:08::']\n",
      "['see%2:39:12::']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['see%2:33:00::']\n",
      "['sense%1:09:02::', '2', 'sense%1:09:05::']\n",
      "sense%1:09:02:: 307\n",
      "sense%1:09:05:: 307\n",
      "['sense%1:10:00::']\n",
      "['sense%1:09:04::']\n",
      "['sense%1:09:06::']\n",
      "['serve%2:42:01::', '3', 'serve%2:42:03::']\n",
      "serve%2:42:01:: 311\n",
      "serve%2:42:03:: 311\n",
      "serve%2:42:12:: 311\n",
      "['serve%2:41:13::', '2', 'serve%2:33:00::']\n",
      "serve%2:41:13:: 312\n",
      "serve%2:33:00:: 312\n",
      "['serve%2:42:02::', '3', 'serve%2:41:00::']\n",
      "serve%2:42:02:: 313\n",
      "serve%2:41:00:: 313\n",
      "serve%2:35:01:: 313\n",
      "['serve%2:34:00::', '3', 'serve%2:34:01::']\n",
      "serve%2:34:00:: 314\n",
      "serve%2:34:01:: 314\n",
      "serve%2:35:00:: 314\n",
      "['serve%2:41:01::', '2', 'serve%2:41:02::']\n",
      "serve%2:41:01:: 315\n",
      "serve%2:41:02:: 315\n",
      "['serve%2:42:00::']\n",
      "['serve%2:33:01::']\n",
      "['shade%1:26:01::', '2', 'shade%1:26:00::']\n",
      "shade%1:26:01:: 318\n",
      "shade%1:26:00:: 318\n",
      "['shade%1:23:00::', '2', 'shade%1:10:00::']\n",
      "shade%1:23:00:: 319\n",
      "shade%1:10:00:: 319\n",
      "['shade%1:07:00::']\n",
      "['shade%1:06:00::']\n",
      "['shade%1:09:01::']\n",
      "['shade%1:06:01::']\n",
      "['shock%1:26:00::', '3', 'shock%1:12:01::']\n",
      "shock%1:26:00:: 324\n",
      "shock%1:12:01:: 324\n",
      "shock%1:11:01:: 324\n",
      "['shock%1:04:01::']\n",
      "['shock%1:04:00::']\n",
      "['shock%1:11:00::']\n",
      "['shock%1:14:00::']\n",
      "['shock%1:14:01::']\n",
      "['shock%1:06:00::']\n",
      "['slot%1:28:00::', '3', 'slot%1:10:00::']\n",
      "slot%1:28:00:: 331\n",
      "slot%1:10:00:: 331\n",
      "slot%1:26:00:: 331\n",
      "['slot%1:06:00::']\n",
      "['slot%1:17:00::']\n",
      "['slot%1:06:02::']\n",
      "['slot%1:06:01::']\n",
      "['spade%1:06:01::']\n",
      "['spade%1:06:00::']\n",
      "['spade%1:18:00::']\n",
      "['stone%1:27:00::', '4', 'stone%1:17:00::']\n",
      "stone%1:27:00:: 339\n",
      "stone%1:17:00:: 339\n",
      "stone%1:20:00:: 339\n",
      "stone%1:17:01:: 339\n",
      "['stone%1:27:01::', '2', 'stone%1:06:00::']\n",
      "stone%1:27:01:: 340\n",
      "stone%1:06:00:: 340\n",
      "['stone%1:18:03::', '5', 'stone%1:18:04::']\n",
      "stone%1:18:03:: 341\n",
      "stone%1:18:04:: 341\n",
      "stone%1:18:02:: 341\n",
      "stone%1:18:01:: 341\n",
      "stone%1:18:00:: 341\n",
      "['stone%1:23:00::']\n",
      "['stone%1:07:00::']\n",
      "['stress%1:26:03::', '2', 'stress%1:10:01::']\n",
      "stress%1:26:03:: 344\n",
      "stress%1:10:01:: 344\n",
      "['stress%1:26:02::', '2', 'stress%1:26:01::']\n",
      "stress%1:26:02:: 345\n",
      "stress%1:26:01:: 345\n",
      "['stress%1:19:00::']\n",
      "['strike%2:35:00::', '4', 'strike%2:35:01::']\n",
      "strike%2:35:00:: 347\n",
      "strike%2:35:01:: 347\n",
      "strike%2:42:00:: 347\n",
      "strike%2:35:09:: 347\n",
      "['strike%2:39:00::', '2', 'strike%2:37:00::']\n",
      "strike%2:39:00:: 348\n",
      "strike%2:37:00:: 348\n",
      "['strike%2:35:02::', '2', 'strike%2:33:00::']\n",
      "strike%2:35:02:: 349\n",
      "strike%2:33:00:: 349\n",
      "['strike%2:38:08::', '2', 'strike%2:41:02::']\n",
      "strike%2:38:08:: 350\n",
      "strike%2:41:02:: 350\n",
      "['strike%2:36:00::', '2', 'strike%2:30:00::']\n",
      "strike%2:36:00:: 351\n",
      "strike%2:30:00:: 351\n",
      "['strike%2:31:00::', '2', 'strike%2:40:00::']\n",
      "strike%2:31:00:: 352\n",
      "strike%2:40:00:: 352\n",
      "['strike%2:32:00::']\n",
      "['strike%2:41:00::']\n",
      "['strike%2:35:03::']\n",
      "['strike%2:35:08::']\n",
      "['strike%2:36:02::']\n",
      "['strike%2:35:10::']\n",
      "['strike_out%2:35:01::', '2', 'strike_out%2:35:00::']\n",
      "strike_out%2:35:01:: 359\n",
      "strike_out%2:35:00:: 359\n",
      "['strike_out%2:32:00::']\n",
      "['strike_out%2:41:00::']\n",
      "['strike_out%2:30:03::']\n",
      "['text%1:10:03::', '3', 'text%1:10:00::']\n",
      "text%1:10:03:: 363\n",
      "text%1:10:00:: 363\n",
      "text%1:10:01:: 363\n",
      "['text%1:10:02::']\n",
      "['train%2:41:01::', '2', 'train%2:31:01::']\n",
      "train%2:41:01:: 365\n",
      "train%2:31:01:: 365\n",
      "['train%2:29:00::', '2', 'train%2:31:00::']\n",
      "train%2:29:00:: 366\n",
      "train%2:31:00:: 366\n",
      "['train%2:41:00::', '4', 'train%2:41:02::']\n",
      "train%2:41:00:: 367\n",
      "train%2:41:02:: 367\n",
      "train%2:32:00:: 367\n",
      "train%2:41:03:: 367\n",
      "['train%2:33:00::']\n",
      "['train%2:38:00::']\n",
      "['train%2:35:04::']\n",
      "['treat%2:31:00::', '2', 'treat%2:41:00::']\n",
      "treat%2:31:00:: 371\n",
      "treat%2:41:00:: 371\n",
      "['treat%2:34:00::', '2', 'treat%2:40:00::']\n",
      "treat%2:34:00:: 372\n",
      "treat%2:40:00:: 372\n",
      "['treat%2:32:00::']\n",
      "['treat%2:30:01::']\n",
      "['treat%2:29:00::']\n",
      "['treat%2:32:03::']\n",
      "['turn%2:38:02::', '4', 'turn%2:38:00::']\n",
      "turn%2:38:02:: 377\n",
      "turn%2:38:00:: 377\n",
      "turn%2:38:04:: 377\n",
      "turn%2:38:13:: 377\n",
      "['turn%2:30:04::', '7', 'turn%2:42:00::']\n",
      "turn%2:30:04:: 378\n",
      "turn%2:42:00:: 378\n",
      "turn%2:30:03:: 378\n",
      "turn%2:30:00:: 378\n",
      "turn%2:30:09:: 378\n",
      "turn%2:42:13:: 378\n",
      "turn%2:30:01:: 378\n",
      "['turn%2:41:10::', '2', 'turn%2:35:06::']\n",
      "turn%2:41:10:: 379\n",
      "turn%2:35:06:: 379\n",
      "['turn%2:33:13::', '3', 'turn%2:38:14::']\n",
      "turn%2:33:13:: 380\n",
      "turn%2:38:14:: 380\n",
      "turn%2:32:14:: 380\n",
      "['turn%2:38:06::']\n",
      "['turn%2:38:03::']\n",
      "['turn%2:38:01::']\n",
      "['turn%2:36:00::']\n",
      "['turn%2:35:08::']\n",
      "['turn%2:29:00::']\n",
      "['turn%2:40:13::']\n",
      "['turn%2:35:11::']\n",
      "['turn%2:35:10::']\n",
      "['turn%2:30:14::']\n",
      "['turn_down%2:41:00::', '3', 'turn_down%2:40:00::']\n",
      "turn_down%2:41:00:: 391\n",
      "turn_down%2:40:00:: 391\n",
      "turn_down%2:32:00:: 391\n",
      "['turn_down%2:30:03::']\n",
      "['turn_down%2:30:00::']\n",
      "['turn_on%2:39:00::', '2', 'turn_on%2:35:00::']\n",
      "turn_on%2:39:00:: 394\n",
      "turn_on%2:35:00:: 394\n",
      "['turn_on%2:37:00::', '2', 'turn_on%2:37:02::']\n",
      "turn_on%2:37:00:: 395\n",
      "turn_on%2:37:02:: 395\n",
      "['turn_on%2:42:00::']\n",
      "['turn_on%2:30:00::']\n",
      "['turn_on%2:34:00::']\n",
      "['turn_out%2:42:01::', '3', 'turn_out%2:42:00::']\n",
      "turn_out%2:42:01:: 399\n",
      "turn_out%2:42:00:: 399\n",
      "turn_out%2:42:02:: 399\n",
      "['turn_out%2:36:00::', '2', 'turn_out%2:36:01::']\n",
      "turn_out%2:36:00:: 400\n",
      "turn_out%2:36:01:: 400\n",
      "['turn_out%2:30:00::']\n",
      "['turn_out%2:35:00::']\n",
      "['turn_out%2:38:00::']\n",
      "['turn_out%2:35:01::']\n",
      "['turn_out%2:29:00::']\n",
      "['turn_over%2:38:00::', '2', 'turn_over%2:38:02::']\n",
      "turn_over%2:38:00:: 406\n",
      "turn_over%2:38:02:: 406\n",
      "['turn_over%2:38:09::', '3', 'turn_over%2:38:03::']\n",
      "turn_over%2:38:09:: 407\n",
      "turn_over%2:38:03:: 407\n",
      "turn_over%2:35:01:: 407\n",
      "['turn_over%2:40:00::']\n",
      "['turn_over%2:35:00::']\n",
      "['turn_over%2:40:01::']\n",
      "['turn_over%2:32:00::']\n",
      "['turn_up%2:35:03::', '2', 'turn_up%2:40:00::']\n",
      "turn_up%2:35:03:: 412\n",
      "turn_up%2:40:00:: 412\n",
      "['turn_up%2:30:00::']\n",
      "['turn_up%2:35:00::']\n",
      "['turn_up%2:42:03::']\n",
      "['use%2:41:14::', '3', 'use%2:34:01::']\n",
      "use%2:41:14:: 416\n",
      "use%2:34:01:: 416\n",
      "use%2:41:04:: 416\n",
      "['use%2:34:00::', '2', 'use%2:34:02::']\n",
      "use%2:34:00:: 417\n",
      "use%2:34:02:: 417\n",
      "['use%2:41:03::']\n",
      "['visit%1:14:00::', '2', 'visit%1:04:02::']\n",
      "visit%1:14:00:: 419\n",
      "visit%1:04:02:: 419\n",
      "['wander%2:38:02::', '3', 'wander%2:38:00::']\n",
      "wander%2:38:02:: 420\n",
      "wander%2:38:00:: 420\n",
      "wander%2:38:01:: 420\n",
      "['wander%2:41:00::']\n",
      "['wander%2:32:00::']\n",
      "['wash%2:29:00::', '5', 'wash%2:35:02::']\n",
      "wash%2:29:00:: 423\n",
      "wash%2:35:02:: 423\n",
      "wash%2:35:00:: 423\n",
      "wash%2:42:00:: 423\n",
      "wash%2:29:02:: 423\n",
      "['wash%2:30:09::', '2', 'wash%2:38:01::']\n",
      "wash%2:30:09:: 424\n",
      "wash%2:38:01:: 424\n",
      "['wash%2:30:05::', '2', 'wash%2:35:01::']\n",
      "wash%2:30:05:: 425\n",
      "wash%2:35:01:: 425\n",
      "['wash%2:42:12::']\n",
      "['wash%2:35:10::']\n",
      "['wash%2:30:06::']\n",
      "['wash%2:29:02::']\n",
      "['wash%2:30:05::']\n",
      "['wash_out%2:30:04::', '2', 'wash_out%2:35:01::']\n",
      "wash_out%2:30:04:: 431\n",
      "wash_out%2:35:01:: 431\n",
      "['wash_out%2:30:03::', '3', 'wash_out%2:30:00::']\n",
      "wash_out%2:30:03:: 432\n",
      "wash_out%2:30:00:: 432\n",
      "wash_out%2:30:02:: 432\n",
      "['wash_out%2:41:00::']\n",
      "['wash_out%2:35:00::']\n",
      "['wash_up%2:35:02::', '2', 'wash_up%2:29:00::']\n",
      "wash_up%2:35:02:: 435\n",
      "wash_up%2:29:00:: 435\n",
      "['wash_up%2:30:00::', '2', 'wash_up%2:38:00::']\n",
      "wash_up%2:30:00:: 436\n",
      "wash_up%2:38:00:: 436\n",
      "['wash_up%2:29:01::']\n",
      "['water%1:17:00::', '3', 'water%1:27:00::']\n",
      "water%1:17:00:: 438\n",
      "water%1:27:00:: 438\n",
      "water%1:27:02:: 438\n",
      "['water%1:06:00::']\n",
      "['water%1:27:01::']\n",
      "['work%2:41:00::', '5', 'work%2:41:02::']\n",
      "work%2:41:00:: 441\n",
      "work%2:41:02:: 441\n",
      "work%2:29:00:: 441\n",
      "work%2:41:04:: 441\n",
      "work%2:41:01:: 441\n",
      "['work%2:36:01::', '2', 'work%2:41:03::']\n",
      "work%2:36:01:: 442\n",
      "work%2:41:03:: 442\n",
      "['work%2:41:11::', '3', 'work%2:35:02::']\n",
      "work%2:41:11:: 443\n",
      "work%2:35:02:: 443\n",
      "work%2:41:12:: 443\n",
      "['work%2:42:10::', '4', 'work%2:36:00::']\n",
      "work%2:42:10:: 444\n",
      "work%2:36:00:: 444\n",
      "work%2:36:12:: 444\n",
      "work%2:35:00:: 444\n",
      "['work%2:41:05::', '4', 'work%2:38:00::']\n",
      "work%2:41:05:: 445\n",
      "work%2:38:00:: 445\n",
      "work%2:35:14:: 445\n",
      "work%2:31:13:: 445\n",
      "['work%2:39:10::', '4', 'work%2:41:10::']\n",
      "work%2:39:10:: 446\n",
      "work%2:41:10:: 446\n",
      "work%2:37:10:: 446\n",
      "work%2:34:10:: 446\n",
      "['work%2:30:13::', '2', 'work%2:30:14::']\n",
      "work%2:30:13:: 447\n",
      "work%2:30:14:: 447\n",
      "['work%2:38:02::']\n",
      "['work%2:36:09::']\n",
      "['work%2:30:10::']\n",
      "['work_out%2:30:01::', '2', 'work_out%2:30:00::']\n",
      "work_out%2:30:01:: 451\n",
      "work_out%2:30:00:: 451\n",
      "['work_out%2:29:01::', '2', 'work_out%2:29:00::']\n",
      "work_out%2:29:01:: 452\n",
      "work_out%2:29:00:: 452\n",
      "['work_out%2:31:06::', '3', 'work_out%2:31:07::']\n",
      "work_out%2:31:06:: 453\n",
      "work_out%2:31:07:: 453\n",
      "work_out%2:31:04:: 453\n",
      "['work_out%2:42:00::']\n",
      "['yew%1:20:00::', '2', 'yew%1:20:02::']\n",
      "yew%1:20:00:: 455\n",
      "yew%1:20:02:: 455\n"
     ]
    }
   ],
   "source": [
    "#deal with file 3\n",
    "path='/Users/gary/Documents/2020Fall/IntroNLP/project/DataSet/Senseval-2/'\n",
    "\n",
    "senseval_file1=path+'corpora/english-lex-sample/train/eng-lex-sample.training.key'\n",
    "senseval_file2=path+'Sval2.keys/Senseval2.key'\n",
    "senseval_file3=path+'corpora/english-lex-sample/train/eng-lex-sample.training.senses'\n",
    "\n",
    "df_senseval=pd.DataFrame(columns=['SenseKeys','Group'])\n",
    "\n",
    "\n",
    "sf3=open(senseval_file3,'r')\n",
    "lines = sf3.readlines()\n",
    "group=0\n",
    "flag=0 #control to skip loop\n",
    "for i in range(len(lines)):\n",
    "    if flag>0:\n",
    "        flag=flag-1\n",
    "        continue\n",
    "    line=lines[i].replace('\\n','').split(' ')\n",
    "    print(line)\n",
    "    if len(line)==1:\n",
    "        #print(line[0],group)\n",
    "        \n",
    "        df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "        group+=1\n",
    "    else:\n",
    "        if line[1]=='2':\n",
    "            print(line[0],group)\n",
    "            print(line[2],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[2],'Group':group},ignore_index=True)\n",
    "            group+=1\n",
    "            \n",
    "        if line[1]=='3':\n",
    "            print(line[0],group)\n",
    "            print(line[2],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[2],'Group':group},ignore_index=True)\n",
    "            line=lines[i+1].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            group+=1\n",
    "            flag+=1\n",
    "        if line[1]=='4':\n",
    "            print(line[0],group)\n",
    "            print(line[2],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[2],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+1].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+2].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "   \n",
    "            group+=1\n",
    "            flag+=2\n",
    "        if line[1]=='5':\n",
    "            print(line[0],group)\n",
    "            print(line[2],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[2],'Group':group},ignore_index=True)\n",
    "\n",
    "\n",
    "            line=lines[i+1].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+2].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+3].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            group+=1\n",
    "            flag+=3\n",
    "        if line[1]=='6':\n",
    "            print(line[0],group)\n",
    "            print(line[2],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[2],'Group':group},ignore_index=True)\n",
    "\n",
    "\n",
    "            line=lines[i+1].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+2].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+3].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+4].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    " \n",
    "            group+=1\n",
    "            flag+=4\n",
    "        if line[1]=='7':\n",
    "            print(line[0],group)\n",
    "            print(line[2],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[2],'Group':group},ignore_index=True)\n",
    "\n",
    "\n",
    "            line=lines[i+1].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+2].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+3].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+4].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+5].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            group+=1\n",
    "            flag+=5\n",
    "        if line[1]=='8':\n",
    "            print(line[0],group)\n",
    "            print(line[2],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[2],'Group':group},ignore_index=True)\n",
    "\n",
    "\n",
    "            line=lines[i+1].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+2].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "\n",
    "            line=lines[i+3].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    " \n",
    "            line=lines[i+4].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            line=lines[i+5].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            line=lines[i+6].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            group+=1\n",
    "            flag+=6\n",
    "        if line[1]=='9':\n",
    "            print(line[0],group)\n",
    "            print(line[2],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[2],'Group':group},ignore_index=True)\n",
    "\n",
    "\n",
    "            line=lines[i+1].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            line=lines[i+2].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            line=lines[i+3].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            line=lines[i+4].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            line=lines[i+5].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            line=lines[i+6].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            line=lines[i+7].replace('\\n','').split(' ')\n",
    "            print(line[0],group)\n",
    "            df_senseval=df_senseval.append({'SenseKeys':line[0],'Group':group},ignore_index=True)\n",
    "            group+=1\n",
    "            flag+=7\n",
    "            \n",
    "df_senseval.to_csv('df_senseval_an.csv',index=False)          \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "814"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_senseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SenseKeys</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attack%5:00:00:offensive:03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bum%5:00:00:inferior:02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>county%3:01:00::</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>material%5:00:00:physical:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>material%5:00:00:worldly:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>work_out%2:31:07::</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>work_out%2:31:04::</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>work_out%2:42:00::</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>yew%1:20:00::</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>yew%1:20:02::</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        SenseKeys Group\n",
       "0     attack%5:00:00:offensive:03     0\n",
       "1         bum%5:00:00:inferior:02     1\n",
       "2                county%3:01:00::     2\n",
       "3    material%5:00:00:physical:00     3\n",
       "4     material%5:00:00:worldly:00     3\n",
       "..                            ...   ...\n",
       "815            work_out%2:31:07::   453\n",
       "816            work_out%2:31:04::   453\n",
       "817            work_out%2:42:00::   454\n",
       "818                 yew%1:20:00::   455\n",
       "819                 yew%1:20:02::   455\n",
       "\n",
       "[820 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_senseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep#v#1 1001\n",
      "keep#v#10 1001\n",
      "keep#v#3 1002\n",
      "keep#v#17 1002\n",
      "keep#v#4 1003\n",
      "keep#v#14 1003\n",
      "keep#v#21 1003\n",
      "keep#v#5 1004\n",
      "keep#v#13 1004\n",
      "keep#v#7 1005\n",
      "keep#v#15 1005\n",
      "keep#v#16 1005\n",
      "keep#v#9 1006\n",
      "keep#v#11 1006\n",
      "keep#v#12 1007\n",
      "keep#v#22 1007\n",
      "keep#v#18 1008\n",
      "keep#v#19 1008\n",
      "keep#v#20 1008\n",
      "keep#v#2 1008\n",
      "keep#v#6 1008\n",
      "keep#v#8 1008\n",
      "develop#v#1 1009\n",
      "develop#v#2 1009\n",
      "develop#v#3 1010\n",
      "develop#v#4 1010\n",
      "develop#v#5 1011\n",
      "develop#v#9 1011\n",
      "develop#v#10 1011\n",
      "develop#v#14 1011\n",
      "develop#v#20 1011\n",
      "develop#v#6 1012\n",
      "develop#v#7 1012\n",
      "develop#v#8 1012\n",
      "develop#v#11 1012\n",
      "develop#v#12 1012\n",
      "develop#v#13 1012\n",
      "develop#v#7 1013\n",
      "develop#v#19 1013\n",
      "develop#v#17 1014\n",
      "develop#v#18 1014\n",
      "develop#v#15 1014\n",
      "develop#v#16 1014\n",
      "develop#v#21 1014\n",
      "face#v#1 1015\n",
      "face#v#2 1015\n",
      "face#v#6 1015\n",
      "face#v#3 1016\n",
      "face#v#4 1016\n",
      "face#v#5 1017\n",
      "face#v#7 1017\n",
      "face#v#8 1018\n",
      "face#v#9 1018\n",
      "match#v#1 1019\n",
      "match#v#4 1019\n",
      "match#v#6 1019\n",
      "match#v#2 1020\n",
      "match#v#9 1020\n",
      "match#v#5 1021\n",
      "match#v#8 1021\n",
      "match#v#3 1021\n",
      "match#v#7 1021\n",
      "dress#v#1 1022\n",
      "dress#v#2 1022\n",
      "dress#v#4 1022\n",
      "dress#v#5 1022\n",
      "dress#v#6 1023\n",
      "dress#v#8 1023\n",
      "dress#v#10 1023\n",
      "dress#v#9 1024\n",
      "dress#v#11 1024\n",
      "dress#v#14 1024\n",
      "dress#v#15 1024\n",
      "dress#v#3 1024\n",
      "dress#v#7 1024\n",
      "dress#v#12 1024\n",
      "dress#v#13 1024\n",
      "pull#v#1 1025\n",
      "pull#v#4 1025\n",
      "pull#v#9 1025\n",
      "pull#v#10 1025\n",
      "pull#v#13 1025\n",
      "pull#v#2 1026\n",
      "pull#v#12 1026\n",
      "pull#v#3 1027\n",
      "pull#v#7 1027\n",
      "pull#v#6 1028\n",
      "pull#v#16 1028\n",
      "pull#v#18 1028\n",
      "pull#v#5 1028\n",
      "pull#v#8 1028\n",
      "pull#v#11 1028\n",
      "pull#v#13 1028\n",
      "pull#v#14 1028\n",
      "pull#v#15 1028\n",
      "pull#v#17 1028\n",
      "see#v#1 1029\n",
      "see#v#7 1029\n",
      "see#v#19 1029\n",
      "see#v#20 1029\n",
      "see#v#2 1030\n",
      "see#v#4 1030\n",
      "see#v#3 1031\n",
      "see#v#6 1031\n",
      "see#v#5 1032\n",
      "see#v#24 1032\n",
      "see#v#8 1033\n",
      "see#v#10 1033\n",
      "see#v#14 1033\n",
      "see#v#11 1034\n",
      "see#v#15 1034\n",
      "see#v#9 1035\n",
      "see#v#12 1035\n",
      "see#v#16 1035\n",
      "see#v#22 1035\n",
      "see#v#13 1035\n",
      "see#v#17 1035\n",
      "see#v#18 1035\n",
      "see#v#21 1035\n",
      "see#v#23 1035\n",
      "error see\n",
      "treat#v#1 1036\n",
      "treat#v#8 1036\n",
      "treat#v#5 1037\n",
      "treat#v#6 1037\n",
      "treat#v#4 1037\n",
      "treat#v#2 1037\n",
      "treat#v#3 1037\n",
      "treat#v#7 1037\n",
      "turn#v#1 1038\n",
      "turn#v#4 1038\n",
      "turn#v#8 1038\n",
      "turn#v#20 1038\n",
      "turn#v#2 1039\n",
      "turn#v#3 1039\n",
      "turn#v#5 1039\n",
      "turn#v#10 1039\n",
      "turn#v#12 1039\n",
      "turn#v#13 1039\n",
      "turn#v#17 1039\n",
      "turn#v#16 1040\n",
      "turn#v#18 1040\n",
      "turn#v#21 1041\n",
      "turn#v#24 1041\n",
      "turn#v#25 1041\n",
      "turn#v#6 1041\n",
      "turn#v#7 1041\n",
      "turn#v#9 1041\n",
      "turn#v#11 1041\n",
      "turn#v#14 1041\n",
      "turn#v#15 1041\n",
      "turn#v#19 1041\n",
      "turn#v#22 1041\n",
      "turn#v#23 1041\n",
      "turn#v#26 1041\n",
      "wash_out#v#2 1042\n",
      "wash_out#v#4 1042\n",
      "wash_out#v#5 1043\n",
      "wash_out#v#6 1043\n",
      "wash_out#v#7 1043\n",
      "wash_out#v#1 1043\n",
      "wash_out#v#3 1043\n",
      "error wash_out\n",
      "drift#v#1 1044\n",
      "drift#v#7 1044\n",
      "drift#v#2 1045\n",
      "drift#v#3 1045\n",
      "drift#v#4 1046\n",
      "drift#v#9 1046\n",
      "drift#v#5 1047\n",
      "drift#v#6 1047\n",
      "drift#v#8 1047\n",
      "drift#v#10 1047\n",
      "error drift\n",
      "use#v#1 1048\n",
      "use#v#3 1048\n",
      "use#v#5 1048\n",
      "use#v#2 1049\n",
      "use#v#4 1049\n",
      "use#v#6 1049\n",
      "carry#v#1 1050\n",
      "carry#v#16 1050\n",
      "carry#v#17 1050\n",
      "carry#v#31 1050\n",
      "carry#v#2 1051\n",
      "carry#v#7 1051\n",
      "carry#v#21 1051\n",
      "carry#v#39 1051\n",
      "carry#v#3 1052\n",
      "carry#v#4 1052\n",
      "carry#v#5 1053\n",
      "carry#v#20 1053\n",
      "carry#v#23 1053\n",
      "carry#v#6 1054\n",
      "carry#v#13 1054\n",
      "carry#v#8 1055\n",
      "carry#v#9 1055\n",
      "carry#v#10 1056\n",
      "carry#v#18 1056\n",
      "carry#v#25 1056\n",
      "carry#v#11 1057\n",
      "carry#v#22 1057\n",
      "carry#v#36 1057\n",
      "carry#v#26 1057\n",
      "carry#v#37 1057\n",
      "carry#v#12 1058\n",
      "carry#v#14 1058\n",
      "carry#v#15 1058\n",
      "carry#v#19 1059\n",
      "carry#v#34 1059\n",
      "carry#v#35 1059\n",
      "carry#v#27 1060\n",
      "carry#v#28 1060\n",
      "carry#v#30 1061\n",
      "carry#v#33 1061\n",
      "carry#v#24 1061\n",
      "carry#v#29 1061\n",
      "carry#v#32 1061\n",
      "carry#v#38 1061\n",
      "call#v#1 1062\n",
      "call#v#3 1062\n",
      "call#v#19 1062\n",
      "call#v#22 1062\n",
      "call#v#2 1063\n",
      "call#v#13 1063\n",
      "call#v#28 1063\n",
      "call#v#4 1064\n",
      "call#v#7 1064\n",
      "call#v#8 1064\n",
      "call#v#9 1064\n",
      "call#v#5 1065\n",
      "call#v#12 1065\n",
      "call#v#16 1065\n",
      "call#v#6 1066\n",
      "call#v#23 1066\n",
      "call#v#10 1067\n",
      "call#v#14 1067\n",
      "call#v#21 1067\n",
      "call#v#24 1067\n",
      "call#v#15 1068\n",
      "call#v#26 1068\n",
      "call#v#18 1069\n",
      "call#v#27 1069\n",
      "call#v#20 1070\n",
      "call#v#25 1070\n",
      "call#v#11 1070\n",
      "call#v#17 1070\n",
      "work#v#1 1071\n",
      "work#v#2 1071\n",
      "work#v#6 1071\n",
      "work#v#8 1071\n",
      "work#v#12 1071\n",
      "work#v#3 1072\n",
      "work#v#11 1072\n",
      "work#v#4 1073\n",
      "work#v#16 1073\n",
      "work#v#17 1073\n",
      "work#v#5 1074\n",
      "work#v#14 1074\n",
      "work#v#20 1074\n",
      "work#v#22 1074\n",
      "work#v#7 1075\n",
      "work#v#9 1075\n",
      "work#v#21 1075\n",
      "work#v#24 1075\n",
      "work#v#15 1076\n",
      "work#v#18 1076\n",
      "work#v#19 1076\n",
      "work#v#23 1076\n",
      "work#v#25 1077\n",
      "work#v#26 1077\n",
      "work#v#10 1077\n",
      "work#v#13 1077\n",
      "work#v#27 1077\n",
      "live#v#2 1078\n",
      "live#v#7 1078\n",
      "live#v#3 1079\n",
      "live#v#4 1079\n",
      "live#v#5 1079\n",
      "live#v#1 1079\n",
      "live#v#6 1079\n",
      "error live\n",
      "train#v#1 1080\n",
      "train#v#5 1080\n",
      "train#v#2 1081\n",
      "train#v#8 1081\n",
      "train#v#3 1082\n",
      "train#v#4 1082\n",
      "train#v#7 1082\n",
      "train#v#9 1082\n",
      "train#v#6 1082\n",
      "train#v#10 1082\n",
      "train#v#11 1082\n",
      "wash#v#1 1083\n",
      "wash#v#2 1083\n",
      "wash#v#3 1083\n",
      "wash#v#5 1083\n",
      "wash#v#12 1083\n",
      "wash#v#4 1084\n",
      "wash#v#10 1084\n",
      "wash#v#7 1085\n",
      "wash#v#9 1085\n",
      "wash#v#6 1085\n",
      "wash#v#8 1085\n",
      "wash#v#11 1085\n",
      "wash#v#12 1085\n",
      "find#v#2 1086\n",
      "find#v#4 1086\n",
      "find#v#8 1086\n",
      "find#v#9 1086\n",
      "find#v#3 1087\n",
      "find#v#7 1087\n",
      "find#v#10 1087\n",
      "find#v#14 1087\n",
      "find#v#5 1088\n",
      "find#v#11 1088\n",
      "find#v#6 1089\n",
      "find#v#13 1089\n",
      "find#v#1 1089\n",
      "find#v#12 1089\n",
      "find#v#15 1089\n",
      "find#v#16 1089\n",
      "replace#v#1 1090\n",
      "replace#v#2 1090\n",
      "replace#v#3 1090\n",
      "replace#v#4 1091\n",
      "error replace\n",
      "serve#v#1 1092\n",
      "serve#v#3 1092\n",
      "serve#v#12 1092\n",
      "serve#v#2 1093\n",
      "serve#v#13 1093\n",
      "serve#v#4 1094\n",
      "serve#v#8 1094\n",
      "serve#v#14 1094\n",
      "serve#v#5 1095\n",
      "serve#v#6 1095\n",
      "serve#v#11 1095\n",
      "serve#v#7 1096\n",
      "serve#v#10 1096\n",
      "serve#v#9 1096\n",
      "serve#v#15 1096\n",
      "play#v#1 1097\n",
      "play#v#34 1097\n",
      "play#v#2 1098\n",
      "play#v#17 1098\n",
      "play#v#3 1099\n",
      "play#v#6 1099\n",
      "play#v#7 1099\n",
      "play#v#4 1100\n",
      "play#v#14 1100\n",
      "play#v#25 1100\n",
      "play#v#26 1100\n",
      "play#v#5 1101\n",
      "play#v#11 1101\n",
      "play#v#12 1101\n",
      "play#v#10 1102\n",
      "play#v#29 1102\n",
      "play#v#30 1102\n",
      "play#v#13 1103\n",
      "play#v#18 1103\n",
      "play#v#15 1104\n",
      "play#v#31 1104\n",
      "play#v#32 1104\n",
      "play#v#33 1104\n",
      "play#v#16 1105\n",
      "play#v#20 1105\n",
      "play#v#21 1105\n",
      "play#v#23 1105\n",
      "play#v#8 1105\n",
      "play#v#9 1105\n",
      "play#v#19 1105\n",
      "play#v#22 1105\n",
      "play#v#24 1105\n",
      "play#v#27 1105\n",
      "play#v#28 1105\n",
      "play#v#35 1105\n",
      "drive#v#1 1106\n",
      "drive#v#2 1106\n",
      "drive#v#3 1106\n",
      "drive#v#12 1106\n",
      "drive#v#13 1106\n",
      "drive#v#14 1106\n",
      "drive#v#15 1106\n",
      "drive#v#4 1107\n",
      "drive#v#5 1107\n",
      "drive#v#7 1107\n",
      "drive#v#6 1108\n",
      "drive#v#9 1108\n",
      "drive#v#16 1108\n",
      "drive#v#17 1108\n",
      "drive#v#8 1109\n",
      "drive#v#20 1109\n",
      "drive#v#21 1109\n",
      "drive#v#10 1109\n",
      "drive#v#11 1109\n",
      "drive#v#18 1109\n",
      "drive#v#19 1109\n",
      "strike#v#1 1110\n",
      "strike#v#2 1110\n",
      "strike#v#8 1110\n",
      "strike#v#19 1110\n",
      "strike#v#3 1111\n",
      "strike#v#15 1111\n",
      "strike#v#4 1112\n",
      "strike#v#6 1112\n",
      "strike#v#9 1113\n",
      "strike#v#16 1113\n",
      "strike#v#11 1114\n",
      "strike#v#13 1114\n",
      "strike#v#12 1115\n",
      "strike#v#20 1115\n",
      "strike#v#5 1115\n",
      "strike#v#7 1115\n",
      "strike#v#10 1115\n",
      "strike#v#14 1115\n",
      "strike#v#17 1115\n",
      "strike#v#18 1115\n",
      "draw#v#1 1116\n",
      "draw#v#21 1116\n",
      "draw#v#27 1116\n",
      "draw#v#2 1117\n",
      "draw#v#17 1117\n",
      "draw#v#3 1118\n",
      "draw#v#11 1118\n",
      "draw#v#25 1118\n",
      "draw#v#4 1119\n",
      "draw#v#5 1119\n",
      "draw#v#7 1119\n",
      "draw#v#9 1119\n",
      "draw#v#20 1119\n",
      "draw#v#6 1120\n",
      "draw#v#8 1120\n",
      "draw#v#10 1120\n",
      "draw#v#12 1120\n",
      "draw#v#14 1120\n",
      "draw#v#15 1120\n",
      "draw#v#18 1120\n",
      "draw#v#22 1120\n",
      "draw#v#35 1120\n",
      "draw#v#26 1121\n",
      "draw#v#28 1121\n",
      "draw#v#31 1122\n",
      "draw#v#32 1122\n",
      "draw#v#34 1122\n",
      "draw#v#13 1122\n",
      "draw#v#16 1122\n",
      "draw#v#19 1122\n",
      "draw#v#23 1122\n",
      "draw#v#24 1122\n",
      "draw#v#29 1122\n",
      "draw#v#30 1122\n",
      "draw#v#33 1122\n",
      "leave#v#1 1123\n",
      "leave#v#5 1123\n",
      "leave#v#8 1123\n",
      "leave#v#2 1124\n",
      "leave#v#12 1124\n",
      "leave#v#14 1124\n",
      "leave#v#3 1125\n",
      "leave#v#7 1125\n",
      "leave#v#9 1126\n",
      "leave#v#10 1126\n",
      "leave#v#13 1126\n",
      "leave#v#4 1126\n",
      "leave#v#6 1126\n",
      "leave#v#11 1126\n",
      "wander#v#1 1127\n",
      "wander#v#3 1127\n",
      "wander#v#4 1127\n",
      "wander#v#2 1127\n",
      "wander#v#5 1127\n",
      "error wander\n"
     ]
    }
   ],
   "source": [
    "#get verb\n",
    "verb_files=path+'corpora/english-lex-sample/train/verb-groups/'\n",
    "output_path=path+'verbs/'\n",
    "'''\n",
    "filelist = [f for f in listdir(verb_files) if isfile(join(verb_files, f))] #get all files' name\n",
    " \n",
    "for f in filelist:\n",
    "    print(f)\n",
    "    fv = open(verb_files+f,'r')\n",
    "    output=open(output_path+f,'w')\n",
    "    v_lines=fv.readlines()\n",
    "    for line in v_lines:\n",
    "        line=line.replace('\\n','')\n",
    "        if line.startswith('GROUP '):\n",
    "            output.write(line+'\\n')\n",
    "        if line.startswith('Sense '):\n",
    "            output.write(line+'\\n')\n",
    "    output.close()\n",
    "''' \n",
    "\n",
    "filelist = [f for f in listdir(output_path) if isfile(join(output_path, f))] #get all files' name\n",
    "\n",
    "df_senseval_v=pd.DataFrame(columns=['SenseKeys','Group','Synset'])\n",
    "group=1001\n",
    "for f in filelist:   \n",
    " \n",
    "    flag=0\n",
    "    fv=open(output_path+f,'r')\n",
    "    v_lines=fv.readlines()\n",
    "    for i in range(len(v_lines)):\n",
    "        if flag>1:\n",
    "            flag-=1\n",
    "            continue\n",
    "        line=v_lines[i].replace('\\n','')\n",
    "        if line.startswith('GROUP '):\n",
    "\n",
    "            flag=1\n",
    "            line=v_lines[i+flag].replace('\\n','')\n",
    "            if not(line.startswith('Sense ')):\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    while line.startswith('Sense'):\n",
    "                        print(f+'#v#'+line.split(' ')[1],group)\n",
    "                        df_senseval_v=df_senseval_v.append({'SenseKeys':'','Group':group,'Synset':f+'#v#'+line.split(' ')[1]},ignore_index=True)\n",
    "                        flag+=1\n",
    "                        line=v_lines[i+flag].replace('\\n','')\n",
    "                    group+=1\n",
    "                except:\n",
    "                    print('error',f)\n",
    "                    group+=1\n",
    "df_senseval_v.to_csv('df_senseval_v.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SenseKeys</th>\n",
       "      <th>Group</th>\n",
       "      <th>Synset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1001</td>\n",
       "      <td>keep#v#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1001</td>\n",
       "      <td>keep#v#10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>1002</td>\n",
       "      <td>keep#v#3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>1002</td>\n",
       "      <td>keep#v#17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>1003</td>\n",
       "      <td>keep#v#4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td></td>\n",
       "      <td>1127</td>\n",
       "      <td>wander#v#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td></td>\n",
       "      <td>1127</td>\n",
       "      <td>wander#v#3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td></td>\n",
       "      <td>1127</td>\n",
       "      <td>wander#v#4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td></td>\n",
       "      <td>1127</td>\n",
       "      <td>wander#v#2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td></td>\n",
       "      <td>1127</td>\n",
       "      <td>wander#v#5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>467 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SenseKeys Group      Synset\n",
       "0              1001    keep#v#1\n",
       "1              1001   keep#v#10\n",
       "2              1002    keep#v#3\n",
       "3              1002   keep#v#17\n",
       "4              1003    keep#v#4\n",
       "..        ...   ...         ...\n",
       "462            1127  wander#v#1\n",
       "463            1127  wander#v#3\n",
       "464            1127  wander#v#4\n",
       "465            1127  wander#v#2\n",
       "466            1127  wander#v#5\n",
       "\n",
       "[467 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_senseval_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:6746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "#add synset by sensekey\n",
    "#based on ReadMe file, the sensekys are from nouns and adjectives.\n",
    "def add_synset():\n",
    "    for i in range(len(df_senseval)):\n",
    "        sensekey=df_senseval.loc[df_senseval.index[i],'SenseKeys']\n",
    "        headword = sensekey.split('%')[0]\n",
    "        syn=''\n",
    "        for syt in wn17.synsets(headword):\n",
    "            if len(syn)>2:\n",
    "                break\n",
    "            #if syt.pos() in ['a','s','n']:\n",
    "            for lemma in syt.lemmas():\n",
    "                if lemma.key()==sensekey:\n",
    "                    syn=syt.name()\n",
    "                    pos=syn.split('.')[1]\n",
    "                    syn=syn.replace('.','#')\n",
    "                    df_senseval.loc[df_senseval.index[i],'Pos']=pos\n",
    "                    df_senseval.loc[df_senseval.index[i],'Synset']=syn\n",
    "                    break\n",
    "\n",
    "#add sensekey by synset\n",
    "def add_sensekey():\n",
    "    for i in range(len(df_senseval_v)):\n",
    "        syn=df_senseval_v.loc[df_senseval_v.index[i],'Synset']\n",
    "        headword = syn.split('#')[0]\n",
    "        pos = syn.split('#')[1]\n",
    "        syn=syn.replace('#','.')\n",
    "        sens=''\n",
    "        sensekey=[]\n",
    "        for lemma1 in wn17.synset(syn).lemmas():\n",
    "            sensekey.append(lemma1.key()) \n",
    "\n",
    "        for s in sensekey:\n",
    "            if s.split('%')[0]==headword:\n",
    "                sens=s\n",
    "        if len(sens)==0:\n",
    "            sens=sensekey[0]\n",
    "        df_senseval_v.loc[df_senseval_v.index[i],'SenseKeys']=sens\n",
    "        df_senseval_v.loc[df_senseval_v.index[i],'Pos']=pos\n",
    "\n",
    "#df_senseval_v =pd.read_csv('df_senseval_v.csv')\n",
    "df_senseval = pd.read_csv('df_senseval_an.csv') \n",
    "\n",
    "add_synset()\n",
    "#add_sensekey()\n",
    "\n",
    "#df_senseval2=pd.concat([df_senseval,df_senseval_v])\n",
    "#add PoS to verb\n",
    "#df_senseval2.loc[df_senseval2['SenseKeys'].isna(),'Pos']='v'\n",
    "df_senseval2= df_senseval\n",
    "#drop NaN synset\n",
    "df_senseval2=df_senseval2[df_senseval2['Synset'].notna()]\n",
    "\n",
    "#get duplicated synset\n",
    "df_sva2_duplicate=df_senseval2.groupby(by=['Synset'],as_index=False, sort=False)['Pos'].count()\n",
    "syns_dp_list=df_sva2_duplicate[df_sva2_duplicate['Pos']>1]['Synset'].to_list()\n",
    "#make duplicated synset have the same group\n",
    "for i in range(len(syns_dp_list)):\n",
    "    gp_dp_list=df_senseval2[df_senseval2['Synset']==syns_dp_list[i]]['Group'].to_list()\n",
    "    for p in gp_dp_list[1:]:\n",
    "        df_senseval2['Group'].replace(p,gp_dp_list[0],inplace=True)\n",
    "#drop duplicated synset\n",
    "df_senseval2= df_senseval2.drop_duplicates(subset=['Synset'], keep='first')\n",
    "\n",
    "df_senseval2['WordPos']=df_senseval2['Synset'].str.split('#').str[0]+'-'+df_senseval2['Pos']\n",
    "\n",
    "df_senseval2.to_csv('Senseval2_raw.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SenseKeys</th>\n",
       "      <th>Group</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Synset</th>\n",
       "      <th>WordPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attack%5:00:00:offensive:03</td>\n",
       "      <td>0</td>\n",
       "      <td>s</td>\n",
       "      <td>assault#s#01</td>\n",
       "      <td>assault-s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bum%5:00:00:inferior:02</td>\n",
       "      <td>1</td>\n",
       "      <td>s</td>\n",
       "      <td>bum#s#01</td>\n",
       "      <td>bum-s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>county%3:01:00::</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>county#a#01</td>\n",
       "      <td>county-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>material%5:00:00:physical:00</td>\n",
       "      <td>3</td>\n",
       "      <td>s</td>\n",
       "      <td>material#s#04</td>\n",
       "      <td>material-s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>material%5:00:00:worldly:00</td>\n",
       "      <td>3</td>\n",
       "      <td>s</td>\n",
       "      <td>material#s#01</td>\n",
       "      <td>material-s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>work_out%2:31:06::</td>\n",
       "      <td>445</td>\n",
       "      <td>v</td>\n",
       "      <td>calculate#v#01</td>\n",
       "      <td>calculate-v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>work_out%2:31:07::</td>\n",
       "      <td>445</td>\n",
       "      <td>v</td>\n",
       "      <td>work_out#v#05</td>\n",
       "      <td>work_out-v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>work_out%2:42:00::</td>\n",
       "      <td>454</td>\n",
       "      <td>v</td>\n",
       "      <td>work_out#v#02</td>\n",
       "      <td>work_out-v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>yew%1:20:00::</td>\n",
       "      <td>455</td>\n",
       "      <td>n</td>\n",
       "      <td>yew#n#02</td>\n",
       "      <td>yew-n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>yew%1:20:02::</td>\n",
       "      <td>455</td>\n",
       "      <td>n</td>\n",
       "      <td>yew#n#01</td>\n",
       "      <td>yew-n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>794 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        SenseKeys  Group Pos          Synset      WordPos\n",
       "0     attack%5:00:00:offensive:03      0   s    assault#s#01    assault-s\n",
       "1         bum%5:00:00:inferior:02      1   s        bum#s#01        bum-s\n",
       "2                county%3:01:00::      2   a     county#a#01     county-a\n",
       "3    material%5:00:00:physical:00      3   s   material#s#04   material-s\n",
       "4     material%5:00:00:worldly:00      3   s   material#s#01   material-s\n",
       "..                            ...    ...  ..             ...          ...\n",
       "814            work_out%2:31:06::    445   v  calculate#v#01  calculate-v\n",
       "815            work_out%2:31:07::    445   v   work_out#v#05   work_out-v\n",
       "817            work_out%2:42:00::    454   v   work_out#v#02   work_out-v\n",
       "818                 yew%1:20:00::    455   n        yew#n#02        yew-n\n",
       "819                 yew%1:20:02::    455   n        yew#n#01        yew-n\n",
       "\n",
       "[794 rows x 5 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_senseval2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794\n",
      "794\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SenseKeys</th>\n",
       "      <th>Group</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Synset</th>\n",
       "      <th>WordPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SenseKeys, Group, Pos, Synset, WordPos]\n",
       "Index: []"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(set(df_senseval2['Synset'].to_list()))) #1095\n",
    "print(len(df_senseval2))\n",
    "df_senseval2[df_senseval2['Synset'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100/344\n",
      "processing 200/344\n",
      "processing 300/344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Sense1</th>\n",
       "      <th>Sense2</th>\n",
       "      <th>Merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>wander#v#01</td>\n",
       "      <td>wander#v#03</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#06</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#06</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>wash_up#v#03</td>\n",
       "      <td>wash_up#v#01</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#01</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#01</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>v</td>\n",
       "      <td>discover#v#02</td>\n",
       "      <td>discover#v#04</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>v</td>\n",
       "      <td>attract#v#02</td>\n",
       "      <td>attract#v#01</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2644 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pos         Sense1         Sense2       Merge\n",
       "0      v    wander#v#01    wander#v#03      merged\n",
       "1      v   observe#v#06   observe#v#09      merged\n",
       "2      v   observe#v#06   observe#v#08  not-merged\n",
       "3      v   observe#v#09   observe#v#08  not-merged\n",
       "4      v   wash_up#v#03   wash_up#v#01      merged\n",
       "...   ..            ...            ...         ...\n",
       "2639   v   draw_up#v#03   draw_up#v#01  not-merged\n",
       "2640   v   draw_up#v#03   draw_up#v#02  not-merged\n",
       "2641   v   draw_up#v#01   draw_up#v#02  not-merged\n",
       "2642   v  discover#v#02  discover#v#04      merged\n",
       "2643   v   attract#v#02   attract#v#01      merged\n",
       "\n",
       "[2644 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sensepair(df_data,df_pairs):\n",
    "    \"\"\"\n",
    "    Purpos: generate sense pairs\n",
    "    Auguments:\n",
    "    df_data: input data\n",
    "    df_pairs: output data\n",
    "    \"\"\"\n",
    "    #\n",
    "    wordpos_list=list(set(df_data['WordPos'].to_list()))\n",
    "    count=1\n",
    "    for w in wordpos_list:\n",
    "        if count%100==0:\n",
    "            print('processing {}/{}'.format(count,len(wordpos_list)))\n",
    "        count+=1\n",
    "        df_word = df_data[df_data['WordPos']==w]\n",
    "        for i in range(len(df_word)):\n",
    "            pos =df_word.loc[df_word.index[i],'Pos']\n",
    "            gp_i=df_word.loc[df_word.index[i],'Group']\n",
    "            sense1=df_word.loc[df_word.index[i],'Synset']\n",
    "\n",
    "            for j in range(i+1,len(df_word)):\n",
    "                gp_j=df_word.loc[df_word.index[j],'Group']\n",
    "                sense2 = df_word.loc[df_word.index[j],'Synset']\n",
    "\n",
    "                if gp_i==gp_j:\n",
    "\n",
    "                    df_pairs=df_pairs.append({'Pos':pos,'Sense1':sense1,'Sense2':sense2,'Merge':'merged'},\n",
    "                                                      ignore_index=True)\n",
    "                else:\n",
    "\n",
    "                    df_pairs=df_pairs.append({'Pos':pos,'Sense1':sense1,'Sense2':sense2,'Merge':'not-merged'},\n",
    "                                                      ignore_index=True)\n",
    "    return df_pairs\n",
    "\n",
    "df_senseval2=pd.read_csv('Senseval2_raw.csv')\n",
    "df_senseva_pair=pd.DataFrame(columns=['Pos','Sense1','Sense2','Merge'])\n",
    "df_senseva_pair=generate_sensepair(df_senseval2,df_senseva_pair)\n",
    "df_senseva_pair.to_csv('Senseval2_sense_pair.csv',index=False)\n",
    "df_senseva_pair\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['sens12']=df_senseva_pair['Sense1']+'-'+df_senseva_pair['Sense2']\n",
    "len(set(df_senseva_pair['sens12'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 4 4 0 0.0\n",
      "n 305 243 62 0.20327868852459016\n",
      "v 2334 2111 223 0.09554413024850043\n"
     ]
    }
   ],
   "source": [
    "#analysis of paris\n",
    "df_senseva_pair=pd.read_csv('Senseval2_sense_pair.csv')\n",
    "total_a=len(df_senseva_pair[df_senseva_pair['Pos']=='a'])\n",
    "total_n=len(df_senseva_pair[df_senseva_pair['Pos']=='n'])\n",
    "total_v=len(df_senseva_pair[df_senseva_pair['Pos']=='v'])\n",
    "\n",
    "notmerge_a=len(df_senseva_pair[(df_senseva_pair['Pos']=='a')&(df_senseva_pair['Merge']=='not-merged')])\n",
    "notmerge_n=len(df_senseva_pair[(df_senseva_pair['Pos']=='n')&(df_senseva_pair['Merge']=='not-merged')])\n",
    "notmerge_v=len(df_senseva_pair[(df_senseva_pair['Pos']=='v')&(df_senseva_pair['Merge']=='not-merged')])\n",
    "\n",
    "merge_a=len(df_senseva_pair[(df_senseva_pair['Pos']=='a')&(df_senseva_pair['Merge']=='merged')])\n",
    "merge_n=len(df_senseva_pair[(df_senseva_pair['Pos']=='n')&(df_senseva_pair['Merge']=='merged')])\n",
    "merge_v=len(df_senseva_pair[(df_senseva_pair['Pos']=='v')&(df_senseva_pair['Merge']=='merged')])\n",
    "\n",
    "rate_a=merge_a/total_a\n",
    "rate_n=merge_n/total_n\n",
    "rate_v=merge_v/total_v\n",
    "print('a',total_a, notmerge_a, merge_a, rate_a)\n",
    "print('n',total_n, notmerge_n, merge_n, rate_n)\n",
    "print('v',total_v, notmerge_v, merge_v, rate_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0/2644\n",
      "processing 1000/2644\n",
      "processing 2000/2644\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>sense1_key17</th>\n",
       "      <th>sense1_wn17</th>\n",
       "      <th>sense2_wn17</th>\n",
       "      <th>sense2_key17</th>\n",
       "      <th>sense1_wn21</th>\n",
       "      <th>sense2_wn21</th>\n",
       "      <th>offset1_wn21</th>\n",
       "      <th>offset2_wn21</th>\n",
       "      <th>sense1_wn17_def</th>\n",
       "      <th>sense1_wn21_def</th>\n",
       "      <th>sense2_wn17_def</th>\n",
       "      <th>sense2_wn21_def</th>\n",
       "      <th>Merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>wander%2:38:00::</td>\n",
       "      <td>wander.v.01</td>\n",
       "      <td>wander.v.03</td>\n",
       "      <td>wander%2:38:02::</td>\n",
       "      <td>roll.v.12</td>\n",
       "      <td>wander.v.03</td>\n",
       "      <td>01863577-v</td>\n",
       "      <td>02083938-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>move about aimlessly or without any destinatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>go via an indirect route or at no set pace</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe%2:41:02::</td>\n",
       "      <td>observe.v.06</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>observe%2:41:04::</td>\n",
       "      <td>observe.v.06</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>02553734-v</td>\n",
       "      <td>02554084-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>celebrate, as of holidays or rites</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conform one's action or practice to</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>observe%2:41:02::</td>\n",
       "      <td>observe.v.06</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>observe%2:31:00::</td>\n",
       "      <td>observe.v.06</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>02553734-v</td>\n",
       "      <td>00724225-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>celebrate, as of holidays or rites</td>\n",
       "      <td>NaN</td>\n",
       "      <td>observe correctly or closely</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>observe%2:41:04::</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>observe%2:31:00::</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>02554084-v</td>\n",
       "      <td>00724225-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conform one's action or practice to</td>\n",
       "      <td>NaN</td>\n",
       "      <td>observe correctly or closely</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>wash_up%2:35:02::</td>\n",
       "      <td>wash_up.v.03</td>\n",
       "      <td>wash_up.v.01</td>\n",
       "      <td>wash_up%2:29:00::</td>\n",
       "      <td>wash_up.v.03</td>\n",
       "      <td>wash_up.v.01</td>\n",
       "      <td>01520295-v</td>\n",
       "      <td>00024967-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wash dishes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wash one's face and hands</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:01::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.01</td>\n",
       "      <td>draw_up%2:41:00::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.01</td>\n",
       "      <td>01845990-v</td>\n",
       "      <td>02425073-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cause (a vehicle) to stop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>form or arrange in order or formation, as of a...</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:01::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>01845990-v</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cause (a vehicle) to stop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:41:00::</td>\n",
       "      <td>draw_up.v.01</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up.v.01</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>02425073-v</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>form or arrange in order or formation, as of a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>not-merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>v</td>\n",
       "      <td>discover%2:36:00::</td>\n",
       "      <td>discover.v.02</td>\n",
       "      <td>discover.v.04</td>\n",
       "      <td>discover%2:31:00::</td>\n",
       "      <td>discover.v.02</td>\n",
       "      <td>discover.v.04</td>\n",
       "      <td>01623484-v</td>\n",
       "      <td>00713143-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a discovery, make a new finding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a discovery</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>v</td>\n",
       "      <td>draw%2:37:03::</td>\n",
       "      <td>attract.v.02</td>\n",
       "      <td>attract.v.01</td>\n",
       "      <td>attract%2:35:00::</td>\n",
       "      <td></td>\n",
       "      <td>attract.v.01</td>\n",
       "      <td></td>\n",
       "      <td>01492358-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>direct toward itself or oneself by means of so...</td>\n",
       "      <td>merged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2644 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pos        sense1_key17    sense1_wn17    sense2_wn17  \\\n",
       "0      v    wander%2:38:00::    wander.v.01    wander.v.03   \n",
       "1      v   observe%2:41:02::   observe.v.06   observe.v.09   \n",
       "2      v   observe%2:41:02::   observe.v.06   observe.v.08   \n",
       "3      v   observe%2:41:04::   observe.v.09   observe.v.08   \n",
       "4      v   wash_up%2:35:02::   wash_up.v.03   wash_up.v.01   \n",
       "...   ..                 ...            ...            ...   \n",
       "2639   v   draw_up%2:38:01::   draw_up.v.03   draw_up.v.01   \n",
       "2640   v   draw_up%2:38:01::   draw_up.v.03   draw_up.v.02   \n",
       "2641   v   draw_up%2:41:00::   draw_up.v.01   draw_up.v.02   \n",
       "2642   v  discover%2:36:00::  discover.v.02  discover.v.04   \n",
       "2643   v      draw%2:37:03::   attract.v.02   attract.v.01   \n",
       "\n",
       "            sense2_key17    sense1_wn21    sense2_wn21 offset1_wn21  \\\n",
       "0       wander%2:38:02::      roll.v.12    wander.v.03   01863577-v   \n",
       "1      observe%2:41:04::   observe.v.06   observe.v.09   02553734-v   \n",
       "2      observe%2:31:00::   observe.v.06   observe.v.08   02553734-v   \n",
       "3      observe%2:31:00::   observe.v.09   observe.v.08   02554084-v   \n",
       "4      wash_up%2:29:00::   wash_up.v.03   wash_up.v.01   01520295-v   \n",
       "...                  ...            ...            ...          ...   \n",
       "2639   draw_up%2:41:00::   draw_up.v.03   draw_up.v.01   01845990-v   \n",
       "2640   draw_up%2:38:02::   draw_up.v.03   draw_up.v.02   01845990-v   \n",
       "2641   draw_up%2:38:02::   draw_up.v.01   draw_up.v.02   02425073-v   \n",
       "2642  discover%2:31:00::  discover.v.02  discover.v.04   01623484-v   \n",
       "2643   attract%2:35:00::                  attract.v.01                \n",
       "\n",
       "     offset2_wn21 sense1_wn17_def  \\\n",
       "0      02083938-v             NaN   \n",
       "1      02554084-v             NaN   \n",
       "2      00724225-v             NaN   \n",
       "3      00724225-v             NaN   \n",
       "4      00024967-v             NaN   \n",
       "...           ...             ...   \n",
       "2639   02425073-v             NaN   \n",
       "2640   01964369-v             NaN   \n",
       "2641   01964369-v             NaN   \n",
       "2642   00713143-v             NaN   \n",
       "2643   01492358-v             NaN   \n",
       "\n",
       "                                        sense1_wn21_def sense2_wn17_def  \\\n",
       "0     move about aimlessly or without any destinatio...             NaN   \n",
       "1                    celebrate, as of holidays or rites             NaN   \n",
       "2                    celebrate, as of holidays or rites             NaN   \n",
       "3                   conform one's action or practice to             NaN   \n",
       "4                                           wash dishes             NaN   \n",
       "...                                                 ...             ...   \n",
       "2639                          cause (a vehicle) to stop             NaN   \n",
       "2640                          cause (a vehicle) to stop             NaN   \n",
       "2641  form or arrange in order or formation, as of a...             NaN   \n",
       "2642               make a discovery, make a new finding             NaN   \n",
       "2643                                                                NaN   \n",
       "\n",
       "                                        sense2_wn21_def       Merge  \n",
       "0            go via an indirect route or at no set pace      merged  \n",
       "1                   conform one's action or practice to      merged  \n",
       "2                          observe correctly or closely  not-merged  \n",
       "3                          observe correctly or closely  not-merged  \n",
       "4                             wash one's face and hands      merged  \n",
       "...                                                 ...         ...  \n",
       "2639  form or arrange in order or formation, as of a...  not-merged  \n",
       "2640                                 straighten oneself  not-merged  \n",
       "2641                                 straighten oneself  not-merged  \n",
       "2642                                   make a discovery      merged  \n",
       "2643  direct toward itself or oneself by means of so...      merged  \n",
       "\n",
       "[2644 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map senseva wn1.7 to wn2.1 \n",
    "df_pairs=pd.read_csv('Senseval2_sense_pair.csv')\n",
    "\n",
    "df_wn17wn21=pd.DataFrame(columns=['Pos','sense1_key17','sense1_wn17','sense2_wn17','sense2_key17','sense1_wn21','sense2_wn21','offset1_wn21','offset2_wn21','sense1_wn17_def','sense1_wn21_def','sense2_wn17_def','sense2_wn21_def','Merge'])\n",
    "\n",
    "#df_pairs_n=df_pairs[df_pairs['Pos']=='n']\n",
    "#total_pair=len(df_pairs_n)\n",
    "total_pair=len(df_pairs)\n",
    "df_senseval2=pd.read_csv('Senseval2_raw.csv')\n",
    "#df_senseval2[df_senseval2['Synset']=='pull#v#12']['SenseKeys'].values[0]\n",
    "\n",
    "for i in range(total_pair):\n",
    "    if (i%1000 ==0):\n",
    "        print('processing {}/{}'.format(i,total_pair))\n",
    "    sense1_wn17 = df_pairs.loc[df_pairs.index[i],'Sense1']\n",
    "    headword=sense1_wn17.split('#')[0]\n",
    "    sense1_wn17 = sense1_wn17.replace('#','.')\n",
    "    sense2_wn17 = df_pairs.loc[df_pairs.index[i],'Sense2']\n",
    "    merge= df_pairs.loc[df_pairs.index[i],'Merge'] \n",
    "    sense2_wn17 = sense2_wn17.replace('#','.')\n",
    "    pos = df_pairs.loc[df_pairs.index[i],'Pos']\n",
    "    \n",
    "    lemmas=''\n",
    "    sense1_wn2=''\n",
    "    sense1_wn2_def=''\n",
    "    offset1_wn2=''\n",
    "    for lemma1 in wn17.synset(sense1_wn17).lemmas():\n",
    "        if len(sense1_wn2)>2:\n",
    "            break\n",
    "        sense1_wn17_def = wn17.synset(sense1_wn17).definition()\n",
    "        #get sense key for wn21 sense1\n",
    "        wn17_sense1_key=lemma1.key()\n",
    "        #enumerate all wn synsests for headword#pos,find the same sense key\n",
    "        for synset1 in wn2.synsets(headword, pos):\n",
    "            lemmas = synset1.lemmas()\n",
    "            if len(sense1_wn2)>2:\n",
    "                break\n",
    "            for l1 in lemmas:\n",
    "                #if sense key equal, keep the wn16 synset name\n",
    "                if l1.key() == wn17_sense1_key:\n",
    "                    sense1_wn2=synset1.name()\n",
    "                    sense1_wn2_def= synset1.definition()\n",
    "                    offset1_wn2 = str(synset1.offset()).zfill(8) +'-'+ synset1.pos()\n",
    "                    break\n",
    "\n",
    "    lemmas=''\n",
    "    sense2_wn2=''\n",
    "    offset2_wn2=''\n",
    "    \n",
    "    for lemma2 in wn17.synset(sense2_wn17).lemmas():\n",
    "        if len(sense2_wn2)>2:\n",
    "            break\n",
    "        sense2_wn17_def = wn17.synset(sense2_wn17).definition()\n",
    "        wn17_sense2_key=lemma2.key()\n",
    "        for synset2 in wn2.synsets(headword, pos):\n",
    "            lemmas = synset2.lemmas()\n",
    "            if len(sense2_wn2)>2:\n",
    "                break\n",
    "            for l2 in lemmas:\n",
    "                if l2.key() == wn17_sense2_key:\n",
    "                    sense2_wn2=synset2.name()\n",
    "                    sense2_wn2_def= synset2.definition()\n",
    "                    offset2_wn2 = str(synset2.offset()).zfill(8) +'-'+ synset2.pos()\n",
    "                    break\n",
    "\n",
    "    df_wn17wn21=df_wn17wn21.append({'Pos':pos,'sense1_key17':wn17_sense1_key,'sense1_wn17':sense1_wn17,'sense2_wn17':sense2_wn17,'sense2_key17':wn17_sense2_key,'sense1_wn21':sense1_wn2,'sense2_wn21':sense2_wn2,\n",
    "                                    'offset1_wn21':offset1_wn2,'offset2_wn21':offset2_wn2,'sense1_wn21_def':sense1_wn2_def,'sense2_wn21_def':sense2_wn2_def,'Merge':merge},\n",
    "                               ignore_index=True)\n",
    "   \n",
    "    \n",
    "df_wn17wn21.to_csv('Senseval_WN17WN21full.csv',index=False)\n",
    "\n",
    "df_wn17wn21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>sense1_key17</th>\n",
       "      <th>sense1_wn17</th>\n",
       "      <th>sense2_wn17</th>\n",
       "      <th>sense2_key17</th>\n",
       "      <th>sense1_wn21</th>\n",
       "      <th>sense2_wn21</th>\n",
       "      <th>offset1_wn21</th>\n",
       "      <th>offset2_wn21</th>\n",
       "      <th>sense1_wn17_def</th>\n",
       "      <th>sense1_wn21_def</th>\n",
       "      <th>sense2_wn17_def</th>\n",
       "      <th>sense2_wn21_def</th>\n",
       "      <th>Merge</th>\n",
       "      <th>sense12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>tune#n#01</td>\n",
       "      <td>air%1:10:01::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>appeal#n#03</td>\n",
       "      <td>appeal%1:04:00::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>entreaty#n#01</td>\n",
       "      <td>appeal%1:10:00::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>solicitation#n#02</td>\n",
       "      <td>appeal%1:10:02::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>appeal#n#02</td>\n",
       "      <td>appeal%1:07:00::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>water#n#04</td>\n",
       "      <td>water%1:27:02::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>water_system#n#02</td>\n",
       "      <td>water%1:06:00::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>urine#n#01</td>\n",
       "      <td>water%1:27:01::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>yew#n#02</td>\n",
       "      <td>yew%1:20:00::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>n</td>\n",
       "      <td>air%1:07:00::</td>\n",
       "      <td>air#n#04</td>\n",
       "      <td>yew#n#01</td>\n",
       "      <td>yew%1:20:02::</td>\n",
       "      <td>air.n.06</td>\n",
       "      <td>tune.n.01</td>\n",
       "      <td>04671829-n</td>\n",
       "      <td>06932144-n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a distinctive but intangible quality surroundi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a succession of notes forming a distinctive se...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>air.n.06-tune.n.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pos   sense1_key17 sense1_wn17        sense2_wn17      sense2_key17  \\\n",
       "548   n  air%1:07:00::    air#n#04          tune#n#01     air%1:10:01::   \n",
       "549   n  air%1:07:00::    air#n#04        appeal#n#03  appeal%1:04:00::   \n",
       "550   n  air%1:07:00::    air#n#04      entreaty#n#01  appeal%1:10:00::   \n",
       "551   n  air%1:07:00::    air#n#04  solicitation#n#02  appeal%1:10:02::   \n",
       "552   n  air%1:07:00::    air#n#04        appeal#n#02  appeal%1:07:00::   \n",
       "..   ..            ...         ...                ...               ...   \n",
       "779   n  air%1:07:00::    air#n#04         water#n#04   water%1:27:02::   \n",
       "780   n  air%1:07:00::    air#n#04  water_system#n#02   water%1:06:00::   \n",
       "781   n  air%1:07:00::    air#n#04         urine#n#01   water%1:27:01::   \n",
       "782   n  air%1:07:00::    air#n#04           yew#n#02     yew%1:20:00::   \n",
       "783   n  air%1:07:00::    air#n#04           yew#n#01     yew%1:20:02::   \n",
       "\n",
       "    sense1_wn21 sense2_wn21 offset1_wn21 offset2_wn21 sense1_wn17_def  \\\n",
       "548    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "549    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "550    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "551    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "552    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "..          ...         ...          ...          ...             ...   \n",
       "779    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "780    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "781    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "782    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "783    air.n.06   tune.n.01   04671829-n   06932144-n             NaN   \n",
       "\n",
       "                                       sense1_wn21_def sense2_wn17_def  \\\n",
       "548  a distinctive but intangible quality surroundi...             NaN   \n",
       "549  a distinctive but intangible quality surroundi...             NaN   \n",
       "550  a distinctive but intangible quality surroundi...             NaN   \n",
       "551  a distinctive but intangible quality surroundi...             NaN   \n",
       "552  a distinctive but intangible quality surroundi...             NaN   \n",
       "..                                                 ...             ...   \n",
       "779  a distinctive but intangible quality surroundi...             NaN   \n",
       "780  a distinctive but intangible quality surroundi...             NaN   \n",
       "781  a distinctive but intangible quality surroundi...             NaN   \n",
       "782  a distinctive but intangible quality surroundi...             NaN   \n",
       "783  a distinctive but intangible quality surroundi...             NaN   \n",
       "\n",
       "                                       sense2_wn21_def       Merge  \\\n",
       "548  a succession of notes forming a distinctive se...  not-merged   \n",
       "549  a succession of notes forming a distinctive se...  not-merged   \n",
       "550  a succession of notes forming a distinctive se...  not-merged   \n",
       "551  a succession of notes forming a distinctive se...  not-merged   \n",
       "552  a succession of notes forming a distinctive se...  not-merged   \n",
       "..                                                 ...         ...   \n",
       "779  a succession of notes forming a distinctive se...  not-merged   \n",
       "780  a succession of notes forming a distinctive se...  not-merged   \n",
       "781  a succession of notes forming a distinctive se...  not-merged   \n",
       "782  a succession of notes forming a distinctive se...  not-merged   \n",
       "783  a succession of notes forming a distinctive se...  not-merged   \n",
       "\n",
       "                sense12  \n",
       "548  air.n.06-tune.n.01  \n",
       "549  air.n.06-tune.n.01  \n",
       "550  air.n.06-tune.n.01  \n",
       "551  air.n.06-tune.n.01  \n",
       "552  air.n.06-tune.n.01  \n",
       "..                  ...  \n",
       "779  air.n.06-tune.n.01  \n",
       "780  air.n.06-tune.n.01  \n",
       "781  air.n.06-tune.n.01  \n",
       "782  air.n.06-tune.n.01  \n",
       "783  air.n.06-tune.n.01  \n",
       "\n",
       "[236 rows x 15 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "df_wn17wn21['sense12']=df_wn17wn21['sense1_wn21']+'-'+df_wn17wn21['sense2_wn21']\n",
    "\n",
    "print(len(set(df_wn17wn21['sense12'].to_list())))\n",
    "Counter(df_wn17wn21['sense12'].to_list())\n",
    "df_wn17wn21[df_wn17wn21['sense12']=='air.n.06-tune.n.01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>sense1_key17</th>\n",
       "      <th>sense1_wn17</th>\n",
       "      <th>sense2_wn17</th>\n",
       "      <th>sense2_key17</th>\n",
       "      <th>sense1_wn21</th>\n",
       "      <th>sense2_wn21</th>\n",
       "      <th>offset1_wn21</th>\n",
       "      <th>offset2_wn21</th>\n",
       "      <th>sense1_wn17_def</th>\n",
       "      <th>sense1_wn21_def</th>\n",
       "      <th>sense2_wn17_def</th>\n",
       "      <th>sense2_wn21_def</th>\n",
       "      <th>Merge</th>\n",
       "      <th>selse2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>wander%2:38:00::</td>\n",
       "      <td>wander.v.01</td>\n",
       "      <td>wander.v.03</td>\n",
       "      <td>wander%2:38:02::</td>\n",
       "      <td>roll#v#12</td>\n",
       "      <td>wander#v#03</td>\n",
       "      <td>01863577-v</td>\n",
       "      <td>02083938-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>move about aimlessly or without any destinatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>go via an indirect route or at no set pace</td>\n",
       "      <td>merged</td>\n",
       "      <td>roll#v#12-wander#v#03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe%2:41:02::</td>\n",
       "      <td>observe.v.06</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>observe%2:41:04::</td>\n",
       "      <td>observe#v#06</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>02553734-v</td>\n",
       "      <td>02554084-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>celebrate, as of holidays or rites</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conform one's action or practice to</td>\n",
       "      <td>merged</td>\n",
       "      <td>observe#v#06-observe#v#09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>observe%2:41:02::</td>\n",
       "      <td>observe.v.06</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>observe%2:31:00::</td>\n",
       "      <td>observe#v#06</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>02553734-v</td>\n",
       "      <td>00724225-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>celebrate, as of holidays or rites</td>\n",
       "      <td>NaN</td>\n",
       "      <td>observe correctly or closely</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>observe#v#06-observe#v#08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>observe%2:41:04::</td>\n",
       "      <td>observe.v.09</td>\n",
       "      <td>observe.v.08</td>\n",
       "      <td>observe%2:31:00::</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>02554084-v</td>\n",
       "      <td>00724225-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conform one's action or practice to</td>\n",
       "      <td>NaN</td>\n",
       "      <td>observe correctly or closely</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>observe#v#09-observe#v#08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>wash_up%2:35:02::</td>\n",
       "      <td>wash_up.v.03</td>\n",
       "      <td>wash_up.v.01</td>\n",
       "      <td>wash_up%2:29:00::</td>\n",
       "      <td>wash_up#v#03</td>\n",
       "      <td>wash_up#v#01</td>\n",
       "      <td>01520295-v</td>\n",
       "      <td>00024967-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wash dishes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wash one's face and hands</td>\n",
       "      <td>merged</td>\n",
       "      <td>wash_up#v#03-wash_up#v#01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:00::</td>\n",
       "      <td>draw_up.v.05</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>01845738-v</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>come to a halt after driving somewhere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw_up#v#05-draw_up#v#02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:01::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.01</td>\n",
       "      <td>draw_up%2:41:00::</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#01</td>\n",
       "      <td>01845990-v</td>\n",
       "      <td>02425073-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cause (a vehicle) to stop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>form or arrange in order or formation, as of a...</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw_up#v#03-draw_up#v#01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:38:01::</td>\n",
       "      <td>draw_up.v.03</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>01845990-v</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cause (a vehicle) to stop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw_up#v#03-draw_up#v#02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up%2:41:00::</td>\n",
       "      <td>draw_up.v.01</td>\n",
       "      <td>draw_up.v.02</td>\n",
       "      <td>draw_up%2:38:02::</td>\n",
       "      <td>draw_up#v#01</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>02425073-v</td>\n",
       "      <td>01964369-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>form or arrange in order or formation, as of a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straighten oneself</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>draw_up#v#01-draw_up#v#02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>v</td>\n",
       "      <td>discover%2:36:00::</td>\n",
       "      <td>discover.v.02</td>\n",
       "      <td>discover.v.04</td>\n",
       "      <td>discover%2:31:00::</td>\n",
       "      <td>discover#v#02</td>\n",
       "      <td>discover#v#04</td>\n",
       "      <td>01623484-v</td>\n",
       "      <td>00713143-v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a discovery, make a new finding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a discovery</td>\n",
       "      <td>merged</td>\n",
       "      <td>discover#v#02-discover#v#04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2423 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pos        sense1_key17    sense1_wn17    sense2_wn17  \\\n",
       "0      v    wander%2:38:00::    wander.v.01    wander.v.03   \n",
       "1      v   observe%2:41:02::   observe.v.06   observe.v.09   \n",
       "2      v   observe%2:41:02::   observe.v.06   observe.v.08   \n",
       "3      v   observe%2:41:04::   observe.v.09   observe.v.08   \n",
       "4      v   wash_up%2:35:02::   wash_up.v.03   wash_up.v.01   \n",
       "...   ..                 ...            ...            ...   \n",
       "2638   v   draw_up%2:38:00::   draw_up.v.05   draw_up.v.02   \n",
       "2639   v   draw_up%2:38:01::   draw_up.v.03   draw_up.v.01   \n",
       "2640   v   draw_up%2:38:01::   draw_up.v.03   draw_up.v.02   \n",
       "2641   v   draw_up%2:41:00::   draw_up.v.01   draw_up.v.02   \n",
       "2642   v  discover%2:36:00::  discover.v.02  discover.v.04   \n",
       "\n",
       "            sense2_key17    sense1_wn21    sense2_wn21 offset1_wn21  \\\n",
       "0       wander%2:38:02::      roll#v#12    wander#v#03   01863577-v   \n",
       "1      observe%2:41:04::   observe#v#06   observe#v#09   02553734-v   \n",
       "2      observe%2:31:00::   observe#v#06   observe#v#08   02553734-v   \n",
       "3      observe%2:31:00::   observe#v#09   observe#v#08   02554084-v   \n",
       "4      wash_up%2:29:00::   wash_up#v#03   wash_up#v#01   01520295-v   \n",
       "...                  ...            ...            ...          ...   \n",
       "2638   draw_up%2:38:02::   draw_up#v#05   draw_up#v#02   01845738-v   \n",
       "2639   draw_up%2:41:00::   draw_up#v#03   draw_up#v#01   01845990-v   \n",
       "2640   draw_up%2:38:02::   draw_up#v#03   draw_up#v#02   01845990-v   \n",
       "2641   draw_up%2:38:02::   draw_up#v#01   draw_up#v#02   02425073-v   \n",
       "2642  discover%2:31:00::  discover#v#02  discover#v#04   01623484-v   \n",
       "\n",
       "     offset2_wn21  sense1_wn17_def  \\\n",
       "0      02083938-v              NaN   \n",
       "1      02554084-v              NaN   \n",
       "2      00724225-v              NaN   \n",
       "3      00724225-v              NaN   \n",
       "4      00024967-v              NaN   \n",
       "...           ...              ...   \n",
       "2638   01964369-v              NaN   \n",
       "2639   02425073-v              NaN   \n",
       "2640   01964369-v              NaN   \n",
       "2641   01964369-v              NaN   \n",
       "2642   00713143-v              NaN   \n",
       "\n",
       "                                        sense1_wn21_def  sense2_wn17_def  \\\n",
       "0     move about aimlessly or without any destinatio...              NaN   \n",
       "1                    celebrate, as of holidays or rites              NaN   \n",
       "2                    celebrate, as of holidays or rites              NaN   \n",
       "3                   conform one's action or practice to              NaN   \n",
       "4                                           wash dishes              NaN   \n",
       "...                                                 ...              ...   \n",
       "2638             come to a halt after driving somewhere              NaN   \n",
       "2639                          cause (a vehicle) to stop              NaN   \n",
       "2640                          cause (a vehicle) to stop              NaN   \n",
       "2641  form or arrange in order or formation, as of a...              NaN   \n",
       "2642               make a discovery, make a new finding              NaN   \n",
       "\n",
       "                                        sense2_wn21_def       Merge  \\\n",
       "0            go via an indirect route or at no set pace      merged   \n",
       "1                   conform one's action or practice to      merged   \n",
       "2                          observe correctly or closely  not-merged   \n",
       "3                          observe correctly or closely  not-merged   \n",
       "4                             wash one's face and hands      merged   \n",
       "...                                                 ...         ...   \n",
       "2638                                 straighten oneself  not-merged   \n",
       "2639  form or arrange in order or formation, as of a...  not-merged   \n",
       "2640                                 straighten oneself  not-merged   \n",
       "2641                                 straighten oneself  not-merged   \n",
       "2642                                   make a discovery      merged   \n",
       "\n",
       "                           selse2  \n",
       "0           roll#v#12-wander#v#03  \n",
       "1       observe#v#06-observe#v#09  \n",
       "2       observe#v#06-observe#v#08  \n",
       "3       observe#v#09-observe#v#08  \n",
       "4       wash_up#v#03-wash_up#v#01  \n",
       "...                           ...  \n",
       "2638    draw_up#v#05-draw_up#v#02  \n",
       "2639    draw_up#v#03-draw_up#v#01  \n",
       "2640    draw_up#v#03-draw_up#v#02  \n",
       "2641    draw_up#v#01-draw_up#v#02  \n",
       "2642  discover#v#02-discover#v#04  \n",
       "\n",
       "[2423 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove sense pair in OntoNotes\n",
    "features_path='/Users/gary/Documents/2020Fall/IntroNLP/project/'\n",
    "df_onetonote_features= pd.read_csv(features_path+'feature_space.csv')\n",
    "df_otf=df_onetonote_features[['Sense1','Sense2']]\n",
    "df_otf['se1se2']=df_otf['Sense1']+'-'+df_otf['Sense2']\n",
    "\n",
    "otf_set=set(df_otf['se1se2'].to_list())\n",
    "\n",
    "df_senseva_WN21=pd.read_csv('Senseval_WN17WN21full.csv')\n",
    "df_senseva_WN21['sense1_wn21']=df_senseva_WN21['sense1_wn21'].str.replace('.','#')\n",
    "df_senseva_WN21['sense2_wn21']=df_senseva_WN21['sense2_wn21'].str.replace('.','#')\n",
    "\n",
    "df_senseva_WN21['selse2']=df_senseva_WN21['sense1_wn21']+'-'+df_senseva_WN21['sense2_wn21']\n",
    "\n",
    "senseva_set=set(df_senseva_WN21['selse2'].to_list())\n",
    "\n",
    "#delete overlap from senseva set\n",
    "\n",
    "\n",
    "senseva_set - otf_set\n",
    "\n",
    "df_senseva_WN21=df_senseva_WN21[df_senseva_WN21['selse2'].isin(list(senseva_set - otf_set))]\n",
    "\n",
    "\n",
    "df_senseva_WN21=df_senseva_WN21[df_senseva_WN21['sense1_wn21'].notna()]\n",
    "df_senseva_WN21=df_senseva_WN21[df_senseva_WN21['sense2_wn21'].notna()]\n",
    "df_senseva_WN21.to_csv('Senseval2_sense_pair_W21_nooverlap.csv',index=False)\n",
    "\n",
    "df_senseva_WN21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 2 2 0 0.0\n",
      "n 305 243 62 0.20327868852459016\n",
      "v 2115 1916 199 0.09408983451536643\n"
     ]
    }
   ],
   "source": [
    "#analysis of paris after removing overlap\n",
    "df_senseva_pair=df_senseva_WN21[['Pos','sense1_wn21','sense2_wn21','Merge']]\n",
    "total_a=len(df_senseva_pair[df_senseva_pair['Pos']=='a'])\n",
    "total_n=len(df_senseva_pair[df_senseva_pair['Pos']=='n'])\n",
    "total_v=len(df_senseva_pair[df_senseva_pair['Pos']=='v'])\n",
    "\n",
    "notmerge_a=len(df_senseva_pair[(df_senseva_pair['Pos']=='a')&(df_senseva_pair['Merge']=='not-merged')])\n",
    "notmerge_n=len(df_senseva_pair[(df_senseva_pair['Pos']=='n')&(df_senseva_pair['Merge']=='not-merged')])\n",
    "notmerge_v=len(df_senseva_pair[(df_senseva_pair['Pos']=='v')&(df_senseva_pair['Merge']=='not-merged')])\n",
    "\n",
    "merge_a=len(df_senseva_pair[(df_senseva_pair['Pos']=='a')&(df_senseva_pair['Merge']=='merged')])\n",
    "merge_n=len(df_senseva_pair[(df_senseva_pair['Pos']=='n')&(df_senseva_pair['Merge']=='merged')])\n",
    "merge_v=len(df_senseva_pair[(df_senseva_pair['Pos']=='v')&(df_senseva_pair['Merge']=='merged')])\n",
    "\n",
    "rate_a=merge_a/total_a\n",
    "rate_n=merge_n/total_n\n",
    "rate_v=merge_v/total_v\n",
    "print('a',total_a, notmerge_a, merge_a, rate_a)\n",
    "print('n',total_n, notmerge_n, merge_n, rate_n)\n",
    "print('v',total_v, notmerge_v, merge_v, rate_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features using perl language\n",
    "df_senseva_pair21=df_senseva_WN21[['Pos','sense1_wn21','sense2_wn21','Merge']]\n",
    "df_senseva_pair21.columns=['Pos','Sense1','Sense2','Merge']\n",
    "df_senseva_pair21.to_csv('Senseva2_sensepairs_wn21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build feature space:\n",
    "1, twin: the number of shared synonyms between two synsets\n",
    "2, antonym: whether two synsets share an antonym\n",
    "3, pertainym: whether two synsets share an pertainym\n",
    "4, deriv: whether two synsets share derivationally related forms\n",
    "5, verbgrp: whether two verb synsets are linked in a VERBGROUP (indicating semantic similarity)\n",
    "6, verbfrm: whether two verb synsets share a VERBFRAM (indicating syntactic similarity) \n",
    "\n",
    "'''\n",
    "#wordnet 2.1\n",
    "df_pairs=pd.read_csv('Senseva2_sensepairs_wn21.csv')\n",
    "#df_pairs\n",
    "for i in range(len(df_pairs)):\n",
    "    pertainyms1=[]\n",
    "    antonyms1=[]\n",
    "    deriv1=[]\n",
    "    lemmas1=[]\n",
    "    verbgroup1=[]\n",
    "    verbframe1=[]\n",
    "    pertainyms2=[]\n",
    "    antonyms2=[]\n",
    "    deriv2=[]\n",
    "    lemmas2=[]\n",
    "    verbgroup2=[]\n",
    "    verbframe2=[]\n",
    "    pertainymsflag=''\n",
    "    antonymsflag=''\n",
    "    derivflag=''\n",
    "    lemmasflag=''\n",
    "    verbgroupflag=''\n",
    "    verbframeflag=''\n",
    "    hyper_min=''\n",
    "    hyper_max=''\n",
    "\n",
    "    sense1 = df_pairs.loc[df_pairs.index[i],'Sense1']\n",
    "    sense1 = sense1.replace('#','.')\n",
    "    sense2 = df_pairs.loc[df_pairs.index[i],'Sense2']\n",
    "    sense2 = sense2.replace('#','.')\n",
    "    pos = df_pairs.loc[df_pairs.index[i],'Pos']\n",
    "    \n",
    "    # sense1's twin, pertainyms, antonyms, derivationally_related_forms,\n",
    "    # verb group, frame id, hyper_min, hyper_max\n",
    "    try:\n",
    "        \n",
    "        #WN, WNMax feature\n",
    "        #calculate hyper distance\n",
    "        sense1_hyper = wn2.synset(sense1)\n",
    "        sense1_hypers = lambda s: s.hypernyms()\n",
    "        hyper1= list(sense1_hyper.closure(sense1_hypers))\n",
    "        \n",
    "        sense2_hyper = wn2.synset(sense2)\n",
    "        sense2_hypers = lambda s: s.hypernyms()\n",
    "        hyper2 = list(sense2_hyper.closure(sense2_hypers))\n",
    "           \n",
    "        #find the nearest hyper, and average the distance as the least distance\n",
    "        for h1 in hyper1:\n",
    "            if h1 in hyper2:\n",
    "                hyper_min = (hyper1.index(h1)+hyper2.index(h1))/2\n",
    "                break\n",
    "        \n",
    "\n",
    "\n",
    "        #find the farest hyper, and average the distance as the largest distance\n",
    "        for h1 in reversed(hyper1):\n",
    "            if h1 in hyper2:\n",
    "                hyper_max = (hyper1.index(h1)+hyper2.index(h1))/2\n",
    "                break\n",
    "                \n",
    "        \n",
    "        for lemma in wn2.synset(sense1).lemmas():\n",
    "            \n",
    "            pertainyms1=pertainyms1+lemma.pertainyms()\n",
    "            antonyms1=antonyms1+lemma.antonyms()\n",
    "            deriv1=deriv1+lemma.derivationally_related_forms()\n",
    "            lemmas1.append(lemma.name())\n",
    "            \n",
    "            if pos=='v':\n",
    "                verbgroup1=verbgroup1+lemma.verb_groups()\n",
    "                verbframe1=verbframe1+lemma.frame_ids()\n",
    "\n",
    "                \n",
    "        # sense2's pertainyms, antonyms, derivationally_related_forms\n",
    "        for lemma in wn2.synset(sense2).lemmas():\n",
    "            \n",
    "            pertainyms2=pertainyms2+lemma.pertainyms()\n",
    "            antonyms2=antonyms2+lemma.antonyms()\n",
    "            deriv2=deriv2+lemma.derivationally_related_forms()\n",
    "            lemmas2.append(lemma.name())\n",
    "             \n",
    "            #verb group, verb frame\n",
    "            if pos=='v':\n",
    "                verbgroup2=verbgroup2+lemma.verb_groups()\n",
    "                verbframe2=verbframe2+lemma.frame_ids()\n",
    "                \n",
    "      \n",
    "        if (len(set(pertainyms1)&set(pertainyms2))>0):\n",
    "            pertainymsflag=1\n",
    "        else:\n",
    "            pertainymsflag=0\n",
    "\n",
    "        if (len(set(antonyms1)&set(antonyms2))>0):\n",
    "            antonymsflag=1\n",
    "        else:\n",
    "            antonymsflag=0 \n",
    "\n",
    "        if (len(set(deriv1)&set(deriv2))>0):\n",
    "            derivflag=1\n",
    "        else:\n",
    "            derivflag=0 \n",
    "        \n",
    "        #verb group, verb frame\n",
    "        if pos=='v':\n",
    "            if (len(set(verbgroup1)&set(verbgroup2))>0):\n",
    "                verbgroupflag=1\n",
    "            else:\n",
    "                verbgroupflag=0\n",
    "                \n",
    "            if (len(set(verbframe1)&set(verbframe2))>0):\n",
    "                verbframeflag=1\n",
    "            else:\n",
    "                verbframeflag=0\n",
    "            \n",
    "\n",
    "        \n",
    "        lemmasflag=len(set(lemmas1)&set(lemmas2))\n",
    "       \n",
    "        \n",
    "        df_pairs.loc[df_pairs.index[i],'pertainyms']=pertainymsflag\n",
    "        df_pairs.loc[df_pairs.index[i],'antonyms']=antonymsflag\n",
    "        df_pairs.loc[df_pairs.index[i],'deriv']=derivflag\n",
    "        df_pairs.loc[df_pairs.index[i],'lemmas']=lemmasflag\n",
    "        \n",
    "        df_pairs.loc[df_pairs.index[i],'verbgroup']=verbgroupflag\n",
    "        df_pairs.loc[df_pairs.index[i],'verbframe']=verbframeflag\n",
    "         \n",
    "        df_pairs.loc[df_pairs.index[i],'hyper_min']=hyper_min\n",
    "        df_pairs.loc[df_pairs.index[i],'hyper_max']=hyper_max\n",
    "\n",
    "        \n",
    "    except:\n",
    "        None\n",
    "    \n",
    "df_pairs.to_csv('Senseva2_SensesPairs_WNFeatures.csv')    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0/2423\n",
      "processing 1000/2423\n",
      "processing 2000/2423\n",
      "processing 0/2423\n",
      "processing 1000/2423\n",
      "processing 2000/2423\n"
     ]
    }
   ],
   "source": [
    "#create feature based on mapping between WN and OED, the mapping is based on WN21\n",
    "#read mapping file\n",
    "file_map_wnoed=open('/Users/gary/Documents/2020Fall/IntroNLP/project/FeatureSpace/sense_clusters-21.senses','r')\n",
    "\n",
    "#read sysnset pairs\n",
    "df_pairs=pd.read_csv('Senseva2_sensepairs_wn21.csv')\n",
    "\n",
    "\n",
    "#df_pairs_n=df_pairs[df_pairs['Pos']=='n']\n",
    "#total_pair=len(df_pairs_n)\n",
    "total_pair=len(df_pairs)\n",
    "for i in range(total_pair):\n",
    "    if (i%1000 ==0):\n",
    "        print('processing {}/{}'.format(i,total_pair))\n",
    "    sense1_wn21 = df_pairs.loc[df_pairs.index[i],'Sense1']\n",
    "    headword=sense1_wn21.split('#')[0]\n",
    "    sense1_wn21 = sense1_wn21.replace('#','.')\n",
    "    sense2_wn21 = df_pairs.loc[df_pairs.index[i],'Sense2']\n",
    "    sense2_wn21 = sense2_wn21.replace('#','.')\n",
    "    pos = df_pairs.loc[df_pairs.index[i],'Pos']\n",
    "    \n",
    "    sense1_keys=''\n",
    "    sense2_keys=''\n",
    "    try:\n",
    "        for lemma1 in wn2.synset(sense1_wn21).lemmas():\n",
    "            #get sense key for wn21 sense1\n",
    "            sense1_keys=sense1_keys+lemma1.key()+';'\n",
    "        for lemma2 in wn2.synset(sense2_wn21).lemmas():\n",
    "            #get sense key for wn21 sense1\n",
    "            sense2_keys=sense2_keys+lemma1.key()+';'\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    df_pairs.loc[df_pairs.index[i],'Sense1_keys']=sense1_keys\n",
    "    df_pairs.loc[df_pairs.index[i],'Sense2_keys']=sense2_keys\n",
    "\n",
    "#filter sense keys is null\n",
    "df_pairs = df_pairs[(df_pairs['Sense1_keys'].str.len()>5)&(df_pairs['Sense1_keys'].str.len()>5)]\n",
    "\n",
    "#get sense clusterings from the mapping between WN and OED\n",
    "lines = file_map_wnoed.readlines()\n",
    "count=0\n",
    "wnoed_lst=[] # a list of sets.\n",
    "for line in lines:\n",
    "    count+=1\n",
    "    line=line.strip('\\n')\n",
    "    if ' ' in line:\n",
    "        wnoed_lst.append(set(line.split(' ')))\n",
    "\n",
    "for i in range(len(df_pairs)):\n",
    "    if i%1000 ==0:\n",
    "        print('processing {}/{}'.format(i,total_pair))\n",
    "    sense1_key=df_pairs.loc[df_pairs.index[i],'Sense1_keys']\n",
    "    sk1_set=set(sense1_key.split(';'))\n",
    "    \n",
    "    sense2_key=df_pairs.loc[df_pairs.index[i],'Sense2_keys']\n",
    "    sk2_set=set(sense2_key.split(';'))\n",
    "    \n",
    "    feature=0\n",
    "    for  w in wnoed_lst:\n",
    "        sk1_len=len(sk1_set & w)\n",
    "        sk2_len=len(sk2_set & w)\n",
    "        if (sk1_len>0 and sk2_len>0):\n",
    "            feature=1\n",
    "            break\n",
    "    \n",
    "    df_pairs.loc[df_pairs.index[i],'wn_oed_feature']=feature\n",
    "    \n",
    "df_pairs.to_csv('WN_OED_Map_Feature_Senseva2_SensesPairs.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sensepari 2423\n",
      "df_wnpackage 2464\n",
      "df_features_tmp 2423\n",
      "df_wncorpus 2423\n",
      "df_features_tmp1 2423\n",
      "df_wnoedfea 2423\n",
      "df_features 2423\n"
     ]
    }
   ],
   "source": [
    "#concatnate all the features together.\n",
    "\n",
    "features_path='/Users/gary/Documents/2020Fall/IntroNLP/project/'\n",
    "feature_files=['Senseva2_sensepairs_wn21.csv','Senseva2_pairs_wnsimilarity.csv','Senseva2_SensesPairs_WNFeatures.csv','WN_OED_Map_Feature_Senseva2_SensesPairs.csv']\n",
    "\n",
    "\n",
    "df_sensepari=pd.read_csv(features_path+feature_files[0])\n",
    "df_sensepari= df_sensepari[['Pos', 'Sense1', 'Sense2', 'Merge']]\n",
    "df_sensepari=df_sensepari.drop_duplicates()\n",
    "\n",
    "print('df_sensepari',len(df_sensepari))\n",
    "df_wnpackage=pd.read_csv(features_path+feature_files[1])\n",
    "df_wnpackage=df_wnpackage.drop_duplicates()\n",
    "print('df_wnpackage',len(df_wnpackage))\n",
    "\n",
    "df_features_tmp = df_sensepari.merge(df_wnpackage,left_on=['Sense1', 'Sense2','Merge'], \n",
    "                              right_on=['sense1', 'sense2','merge'], how='left')\n",
    "\n",
    "\n",
    "df_features_tmp=df_features_tmp[['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "       'lch', 'hso', 'jcn', 'leskvalue', 'linvalue', 'resvalue', 'vecvalue',\n",
    "       'wupvalue']]\n",
    "df_features_tmp=df_features_tmp.drop_duplicates()\n",
    "print('df_features_tmp',len(df_features_tmp))\n",
    "\n",
    "df_wncorpus=pd.read_csv(features_path+feature_files[2])\n",
    "\n",
    "print('df_wncorpus',len(df_wncorpus))\n",
    "\n",
    "df_features_tmp1 = df_features_tmp.merge(df_wncorpus,left_on=['Pos', 'Sense1', 'Sense2', 'Merge'], \n",
    "                              right_on=['Pos', 'Sense1', 'Sense2', 'Merge'], how='left')\n",
    "\n",
    "df_features_tmp1.columns\n",
    "print('df_features_tmp1',len(df_features_tmp1))\n",
    "\n",
    "\n",
    "df_features_tmp1=df_features_tmp1[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
    "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', \n",
    "        'pertainyms', 'antonyms', 'deriv', 'lemmas',\n",
    "       'verbgroup', 'verbframe', 'hyper_min', 'hyper_max']]\n",
    "\n",
    "df_features_tmp1=df_features_tmp1.drop_duplicates() \n",
    "\n",
    "#wn-oed mapping feature\n",
    "df_wnoedfea=pd.read_csv(features_path+feature_files[3])\n",
    "df_wnoedfea\n",
    "print('df_wnoedfea',len(df_wnoedfea))\n",
    "df_features = df_features_tmp1.merge(df_wnoedfea[['Pos', 'Sense1', 'Sense2', 'Merge','wn_oed_feature']],left_on=['Pos', 'Sense1', 'Sense2','Merge'], \n",
    "                              right_on=['Pos', 'Sense1', 'Sense2', 'Merge'], how='left')\n",
    "\n",
    "df_features=df_features[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
    "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', 'pertainyms',\n",
    "       'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe', 'hyper_min',\n",
    "       'hyper_max', 'wn_oed_feature']]\n",
    "\n",
    "df_features=df_features.drop_duplicates()\n",
    "df_features\n",
    "print('df_features',len(df_features))\n",
    "df_features.to_csv('senseval2_feature_space.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Sense1</th>\n",
       "      <th>Sense2</th>\n",
       "      <th>Merge</th>\n",
       "      <th>lch</th>\n",
       "      <th>hso</th>\n",
       "      <th>jcn</th>\n",
       "      <th>leskvalue</th>\n",
       "      <th>linvalue</th>\n",
       "      <th>resvalue</th>\n",
       "      <th>...</th>\n",
       "      <th>wupvalue</th>\n",
       "      <th>pertainyms</th>\n",
       "      <th>antonyms</th>\n",
       "      <th>deriv</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>verbgroup</th>\n",
       "      <th>verbframe</th>\n",
       "      <th>hyper_min</th>\n",
       "      <th>hyper_max</th>\n",
       "      <th>wn_oed_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>roll#v#12</td>\n",
       "      <td>wander#v#03</td>\n",
       "      <td>merged</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.318246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#06</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>merged</td>\n",
       "      <td>2.233592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056615</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#06</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>2.233592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056311</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>2.233592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>wash_up#v#03</td>\n",
       "      <td>wash_up#v#01</td>\n",
       "      <td>merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#01</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.540445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043985</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046550</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#01</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.252763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048102</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>v</td>\n",
       "      <td>discover#v#02</td>\n",
       "      <td>discover#v#04</td>\n",
       "      <td>merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2420 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pos         Sense1         Sense2       Merge       lch  hso       jcn  \\\n",
       "0      v      roll#v#12    wander#v#03      merged  1.945910  4.0  0.000000   \n",
       "1      v   observe#v#06   observe#v#09      merged  2.233592  0.0  0.056615   \n",
       "2      v   observe#v#06   observe#v#08  not-merged  2.233592  0.0  0.056311   \n",
       "3      v   observe#v#09   observe#v#08  not-merged  2.233592  0.0  0.053396   \n",
       "4      v   wash_up#v#03   wash_up#v#01      merged  1.386294  2.0  0.000000   \n",
       "...   ..            ...            ...         ...       ...  ...       ...   \n",
       "2418   v   draw_up#v#05   draw_up#v#02  not-merged  1.386294  0.0  0.051186   \n",
       "2419   v   draw_up#v#03   draw_up#v#01  not-merged  1.540445  0.0  0.043985   \n",
       "2420   v   draw_up#v#03   draw_up#v#02  not-merged  1.386294  0.0  0.046550   \n",
       "2421   v   draw_up#v#01   draw_up#v#02  not-merged  1.252763  0.0  0.048102   \n",
       "2422   v  discover#v#02  discover#v#04      merged  1.386294  0.0  0.064222   \n",
       "\n",
       "      leskvalue  linvalue  resvalue  ...  wupvalue  pertainyms  antonyms  \\\n",
       "0          10.0       0.0  3.318246  ...  0.571429         0.0       0.0   \n",
       "1           9.0       0.0  0.000000  ...  0.500000         0.0       0.0   \n",
       "2          11.0       0.0  0.000000  ...  0.500000         0.0       0.0   \n",
       "3           9.0       0.0  0.000000  ...  0.500000         0.0       0.0   \n",
       "4           5.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "...         ...       ...       ...  ...       ...         ...       ...   \n",
       "2418        4.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "2419        3.0       0.0  0.000000  ...  0.285714         0.0       0.0   \n",
       "2420        4.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "2421        2.0       0.0  0.000000  ...  0.222222         0.0       0.0   \n",
       "2422       12.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "\n",
       "      deriv  lemmas  verbgroup  verbframe  hyper_min  hyper_max  \\\n",
       "0       0.0     1.0        0.0        1.0        0.5        0.5   \n",
       "1       1.0     2.0        0.0        1.0        NaN        NaN   \n",
       "2       0.0     2.0        0.0        1.0        NaN        NaN   \n",
       "3       0.0     2.0        0.0        1.0        NaN        NaN   \n",
       "4       1.0     1.0        0.0        1.0        NaN        NaN   \n",
       "...     ...     ...        ...        ...        ...        ...   \n",
       "2418    0.0     2.0        0.0        0.0        NaN        NaN   \n",
       "2419    0.0     1.0        0.0        1.0        NaN        NaN   \n",
       "2420    0.0     2.0        0.0        0.0        NaN        NaN   \n",
       "2421    0.0     1.0        0.0        1.0        NaN        NaN   \n",
       "2422    1.0     2.0        0.0        1.0        NaN        NaN   \n",
       "\n",
       "      wn_oed_feature  \n",
       "0                0.0  \n",
       "1                1.0  \n",
       "2                1.0  \n",
       "3                1.0  \n",
       "4                0.0  \n",
       "...              ...  \n",
       "2418             0.0  \n",
       "2419             0.0  \n",
       "2420             0.0  \n",
       "2421             0.0  \n",
       "2422             1.0  \n",
       "\n",
       "[2420 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analyze the feature space\n",
    "features_path='/Users/gary/Documents/2020Fall/IntroNLP/project/'\n",
    "df_features= pd.read_csv(features_path+'senseval2_feature_space.csv')\n",
    "#remove wrong senses (no sense number)\n",
    "df_features = df_features[df_features['Sense1'].str.contains('[0-9]')]\n",
    "df_features = df_features[df_features['Sense2'].str.contains('[0-9]')]\n",
    "\n",
    "#df_features[~(df_features['Sense1'].str.contains('[0-9]'))]\n",
    "#df_features[~(df_features['Sense2'].str.contains('[0-9]'))]\n",
    "\n",
    "#deal with NaN columns\n",
    "#remove 'lch' is NaN rows, as all other features are also NaN\n",
    "df_features = df_features[~(df_features['lch'].isna())]\n",
    "df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v->total:2827, not_merged:2613, merged:214,merge rate:0.07569862044570216\n",
      "n->total:432,not_merged:347, merged:85,merge rate:0.19675925925925927\n"
     ]
    }
   ],
   "source": [
    "#for each PoS, split the features into a training set (70%) and a testing set (30%)\n",
    "# I remove wn_domain_feature,as it is same (1) for all pairs\n",
    "# I remove 'hyper_min', 'hyper_max', 'topic_similarity' as they have NaN values \n",
    "# I remove 'pertainyms' as it is for adj.\n",
    "features_path='/Users/gary/Documents/2020Fall/IntroNLP/project/'\n",
    "df_features1= pd.read_csv(features_path+'senseval2_feature_space_sf1.csv')\n",
    "\n",
    "df_features3= pd.read_csv(features_path+'senseval2_feature_space.csv')\n",
    "\n",
    "df_features=pd.concat([df_features1,df_features3])\n",
    "#df_features= pd.read_csv(features_path+'senseval2_feature_space.csv')\n",
    "\n",
    "\n",
    "\n",
    "df_features = df_features[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn',\n",
    "       'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'pertainyms', 'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe',\n",
    "       'hyper_min', 'hyper_max', 'wn_oed_feature']]\n",
    "df_features_n = df_features[df_features['Pos']=='n'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "        'lch', 'hso', 'jcn', 'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas','wn_oed_feature']]\n",
    "df_features_v = df_features[df_features['Pos']=='v'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "       'lch', 'hso', 'jcn','leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe','wn_oed_feature']]\n",
    "\n",
    "#replace labels with 0 or 1\n",
    "df_features_v=df_features_v.replace(['not-merged','merged'],[0,1])\n",
    "v_nm=len(df_features_v[df_features_v['Merge']==0])\n",
    "v_m=len(df_features_v[df_features_v['Merge']==1])\n",
    "print('v->total:{}, not_merged:{}, merged:{},merge rate:{}'.format(v_nm+v_m,v_nm,v_m,v_m/(v_nm+v_m)))\n",
    "\n",
    "df_features_n=df_features_n.replace(['not-merged','merged'],[0,1])\n",
    "n_nm=len(df_features_n[df_features_n['Merge']==0])\n",
    "n_m=len(df_features_n[df_features_n['Merge']==1])\n",
    "print('n->total:{},not_merged:{}, merged:{},merge rate:{}'.format(n_nm+n_m, n_nm,n_m,n_m/(n_nm+n_m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v_m/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if NaN exist\n",
    "df_features_n.columns[df_features_n.isna().any()].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Sense1</th>\n",
       "      <th>Sense2</th>\n",
       "      <th>Merge</th>\n",
       "      <th>lch</th>\n",
       "      <th>hso</th>\n",
       "      <th>jcn</th>\n",
       "      <th>leskvalue</th>\n",
       "      <th>linvalue</th>\n",
       "      <th>resvalue</th>\n",
       "      <th>...</th>\n",
       "      <th>wupvalue</th>\n",
       "      <th>pertainyms</th>\n",
       "      <th>antonyms</th>\n",
       "      <th>deriv</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>verbgroup</th>\n",
       "      <th>verbframe</th>\n",
       "      <th>hyper_min</th>\n",
       "      <th>hyper_max</th>\n",
       "      <th>wn_oed_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>roll#v#12</td>\n",
       "      <td>wander#v#03</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.318246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>observe#v#08</td>\n",
       "      <td>observe#v#09</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>2.233592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#07</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.540445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047931</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#10</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045832</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>draw#v#19</td>\n",
       "      <td>draw#v#05</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.540445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048413</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#05</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#01</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.540445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043985</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#03</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046550</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>v</td>\n",
       "      <td>draw_up#v#01</td>\n",
       "      <td>draw_up#v#02</td>\n",
       "      <td>not-merged</td>\n",
       "      <td>1.252763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048102</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>v</td>\n",
       "      <td>discover#v#02</td>\n",
       "      <td>discover#v#04</td>\n",
       "      <td>merged</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3297 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pos         Sense1         Sense2       Merge       lch  hso       jcn  \\\n",
       "0      v      roll#v#12    wander#v#03  not-merged  1.945910  4.0  0.000000   \n",
       "1      v   observe#v#08   observe#v#09  not-merged  2.233592  0.0  0.053396   \n",
       "2      v      draw#v#19      draw#v#07  not-merged  1.540445  0.0  0.047931   \n",
       "3      v      draw#v#19      draw#v#10  not-merged  1.386294  0.0  0.045832   \n",
       "4      v      draw#v#19      draw#v#05  not-merged  1.540445  0.0  0.048413   \n",
       "...   ..            ...            ...         ...       ...  ...       ...   \n",
       "2418   v   draw_up#v#05   draw_up#v#02  not-merged  1.386294  0.0  0.051186   \n",
       "2419   v   draw_up#v#03   draw_up#v#01  not-merged  1.540445  0.0  0.043985   \n",
       "2420   v   draw_up#v#03   draw_up#v#02  not-merged  1.386294  0.0  0.046550   \n",
       "2421   v   draw_up#v#01   draw_up#v#02  not-merged  1.252763  0.0  0.048102   \n",
       "2422   v  discover#v#02  discover#v#04      merged  1.386294  0.0  0.064222   \n",
       "\n",
       "      leskvalue  linvalue  resvalue  ...  wupvalue  pertainyms  antonyms  \\\n",
       "0          10.0       0.0  3.318246  ...  0.571429         0.0       0.0   \n",
       "1           9.0       0.0  0.000000  ...  0.500000         0.0       0.0   \n",
       "2           8.0       0.0  0.000000  ...  0.285714         0.0       0.0   \n",
       "3           2.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "4           5.0       0.0  0.000000  ...  0.285714         0.0       0.0   \n",
       "...         ...       ...       ...  ...       ...         ...       ...   \n",
       "2418        4.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "2419        3.0       0.0  0.000000  ...  0.285714         0.0       0.0   \n",
       "2420        4.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "2421        2.0       0.0  0.000000  ...  0.222222         0.0       0.0   \n",
       "2422       12.0       0.0  0.000000  ...  0.250000         0.0       0.0   \n",
       "\n",
       "      deriv  lemmas  verbgroup  verbframe  hyper_min  hyper_max  \\\n",
       "0       0.0     1.0        0.0        1.0        0.5        0.5   \n",
       "1       0.0     2.0        0.0        1.0        NaN        NaN   \n",
       "2       1.0     1.0        0.0        0.0        NaN        NaN   \n",
       "3       0.0     1.0        0.0        0.0        NaN        NaN   \n",
       "4       1.0     1.0        0.0        0.0        NaN        NaN   \n",
       "...     ...     ...        ...        ...        ...        ...   \n",
       "2418    0.0     2.0        0.0        0.0        NaN        NaN   \n",
       "2419    0.0     1.0        0.0        1.0        NaN        NaN   \n",
       "2420    0.0     2.0        0.0        0.0        NaN        NaN   \n",
       "2421    0.0     1.0        0.0        1.0        NaN        NaN   \n",
       "2422    1.0     2.0        0.0        1.0        NaN        NaN   \n",
       "\n",
       "      wn_oed_feature  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                1.0  \n",
       "3                1.0  \n",
       "4                1.0  \n",
       "...              ...  \n",
       "2418             0.0  \n",
       "2419             0.0  \n",
       "2420             0.0  \n",
       "2421             0.0  \n",
       "2422             1.0  \n",
       "\n",
       "[3297 rows x 21 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v->not_merged:9876, merged:2751\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 14)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 14)\n",
      "y_pred sum: 0\n",
      "Y_train sum: 1900\n",
      "verb:0.436747435706853\n",
      "Baseline: 0.18340517241379312\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 12)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 12)\n",
      "y_pred sum: 5\n",
      "Y_train sum: 258\n",
      "noun:0.4751180836707153\n",
      "Baseline: 0.1092184368737475\n",
      "n: 0.4751180836707153 v: 0.436747435706853\n"
     ]
    }
   ],
   "source": [
    "#apply SVM for Verb F1=0.4989\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,LeaveOneOut\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "\n",
    "def split_data(X,Y,rate):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=rate,random_state=42)\n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "\n",
    "def cross_validation(X_train, Y_train, C_list, K):\n",
    "    for c in C_list:\n",
    "        clf1 = SVC(kernel='linear',C=c).fit(X_train,Y_train)\n",
    "\n",
    "        strat_scores = cross_val_score(clf1,X_train,Y_train,cv=StratifiedKFold(K,random_state=10,shuffle=True))\n",
    "        print('C=',c)\n",
    "        print(\"The Stratified Cross Validation Score :\"+str(strat_scores))\n",
    "        print(\"The Average Stratified Cross Validation Score :\"+str(strat_scores.mean()))\n",
    "\n",
    "def predict(X_train, Y_train, X_test, c):\n",
    "    \n",
    "    clf1 = SVC(kernel='rbf', C=c).fit(X_train,Y_train)\n",
    "    #print('features important:')\n",
    "    #f_importances(clf1.coef_, features_names)\n",
    "    print(\"shape of Y :\"+str(Y_train.shape))\n",
    "    print(\"shape of X :\"+str(X_train.shape))\n",
    "    y_pred = clf1.predict(X_test)\n",
    "    return clf1, y_pred\n",
    "\n",
    "def cal_FScore(y_true, y_pred):\n",
    "    F1=f1_score(y_true, y_pred, average='macro')\n",
    "    return F1\n",
    "\n",
    "def Fscore(df_pos, tune_flag, pos, c=None):\n",
    "    '''\n",
    "    tune_flag: Y-> tune c\n",
    "    '''\n",
    "    Y=np.array(df_pos['Merge'])\n",
    "    X=df_pos.drop(['Pos','Sense1','Sense2','Merge'],axis=1)\n",
    "    X=np.array(X)\n",
    "    features_names=['1','2','3','4','5','6','7','8','9','10','11','12','13','14']\n",
    "    print(\"shape of Y :\"+str(Y.shape))\n",
    "    print(\"shape of X :\"+str(X.shape))\n",
    "    \n",
    "    #normalize data, Snow did not mention this\n",
    "    #scaler = MinMaxScaler()\n",
    "    #X = scaler.fit_transform(X)\n",
    "    #x_idex=X\n",
    "    x_idex=0\n",
    "    #split data\n",
    "    rate=0.3\n",
    "    X_train,X_test,Y_train,Y_test = split_data(X,Y,rate)\n",
    "    if tune_flag=='Y':\n",
    "        C_list=[1e2,1e3,1e4,1e5,1e6,1e7]\n",
    "        K=10\n",
    "        cross_validation(X_train, Y_train, C_list, K)\n",
    "    else:\n",
    "        #predict\n",
    "        clf1, y_pred= predict(X_train, Y_train, X_test, c)\n",
    "        print('y_pred sum:',np.sum(y_pred))\n",
    "        print('Y_train sum:',np.sum(Y_train))\n",
    "        #F1 score\n",
    "        F1= cal_FScore(Y_test,y_pred)\n",
    "        print('{}:{}'.format(pos,F1))\n",
    "        #base line\n",
    "\n",
    "        y_pre_baseline=np.ones(len(Y_test))\n",
    "        F1_base= cal_FScore(Y_test,y_pre_baseline)\n",
    "        print('Baseline:',F1_base)\n",
    "        return F1, X_test, Y_test, y_pred, x_idex\n",
    "\n",
    "\n",
    "\n",
    "''' \n",
    "\n",
    "Y_v = np.array(df_features_v['Merge'])\n",
    "X_v = df_features_v.drop(['Pos','Sense1','Sense2','Merge'],axis=1)\n",
    "X_v = np.array(X_v)\n",
    "features_names=['1','2','3','4','5','6','7','8','9','10','11','12','13','14']\n",
    "print(\"shape of Y :\"+str(Y_v.shape))\n",
    "print(\"shape of X :\"+str(X_v.shape))\n",
    "\n",
    "#normalize data, Snow did not mention this\n",
    "scaler = MinMaxScaler()\n",
    "X_v = scaler.fit_transform(X_v)\n",
    "\n",
    "#split data\n",
    "rate=0.3\n",
    "X_train,X_test,Y_train,Y_test = split_data(X_v,Y_v,rate)\n",
    "\n",
    "#cross validation c=3 average score=0.75233236151603\n",
    "C_list=[1,2,3,5,10]\n",
    "K=10\n",
    "#cross_validation(X_train, Y_train, C_list, K)\n",
    "c=3\n",
    "\n",
    "#predict\n",
    "clf1, y_pred= predict(X_train, Y_train, X_test, c)\n",
    "\n",
    "#F1 score\n",
    "F1= cal_FScore(Y_test,y_pred)\n",
    "\n",
    "print('Verb:',F1)\n",
    "#base line\n",
    "y_pre_baseline_v=np.ones(len(Y_test))\n",
    "F1= cal_FScore(Y_test,y_pre_baseline_v)\n",
    "\n",
    "print('Baseline:',F1)\n",
    "''' \n",
    "#for each PoS, split the features into a training set (70%) and a testing set (30%)\n",
    "# I remove wn_domain_feature,as it is same (1) for all pairs\n",
    "# I remove 'hyper_min', 'hyper_max', 'topic_similarity' as they have NaN values \n",
    "# I remove 'pertainyms' as it is for adj.\n",
    "features_path='/Users/gary/Documents/2020Fall/IntroNLP/project/'\n",
    "\n",
    "#combine OntoNotes and SENSEVAL-2\n",
    "df_features1= pd.read_csv(features_path+'feature_space.csv')\n",
    "\n",
    "df_features4= pd.read_csv(features_path+'senseval2_feature_space.csv')\n",
    "\n",
    "df_features3= pd.read_csv(features_path+'senseval2_feature_space_sf1.csv')\n",
    "\n",
    "df_features2=pd.concat([df_features4,df_features3])\n",
    "\n",
    "\n",
    "df_features1 = df_features1[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn',\n",
    "       'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'pertainyms', 'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe',\n",
    "       'hyper_min', 'hyper_max', 'wn_oed_feature']]\n",
    "\n",
    "df_features2 = df_features2[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn',\n",
    "       'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'pertainyms', 'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe',\n",
    "       'hyper_min', 'hyper_max', 'wn_oed_feature']]\n",
    "\n",
    "df_features = pd.concat([df_features1,df_features2])\n",
    "\n",
    "df_features_n = df_features[df_features['Pos']=='n'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "        'lch', 'hso', 'jcn', 'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas','wn_oed_feature']]\n",
    "df_features_v = df_features[df_features['Pos']=='v'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "       'lch', 'hso', 'jcn','leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe','wn_oed_feature']]\n",
    "\n",
    "#replace labels with 0 or 1\n",
    "df_features_v=df_features_v.replace(['not-merged','merged'],[0,1])\n",
    "v_nm=len(df_features_v[df_features_v['Merge']==0])\n",
    "v_m=len(df_features_v[df_features_v['Merge']==1])\n",
    "print('v->not_merged:{}, merged:{}'.format(v_nm,v_m))\n",
    "\n",
    "df_features_n=df_features_n.replace(['not-merged','merged'],[0,1])\n",
    "n_nm=len(df_features_n[df_features_n['Merge']==0])\n",
    "n_m=len(df_features_n[df_features_n['Merge']==1])\n",
    "print('n->not_merged:{}, merged:{}'.format(n_nm,n_m))\n",
    "#df_features_v\n",
    "\n",
    "#Fscore(df_pos=df_features_v, tune_flag='Y', c=None)\n",
    "c=10\n",
    "F1_v,x_test_v, y_test_v, y_pred_v,x_idex_v=Fscore(df_pos=df_features_v, tune_flag='N', c=c,pos='verb')\n",
    "\n",
    "F1_n,x_test_n, y_trest_n, y_pred_n,x_idex_n=Fscore(df_pos=df_features_n, tune_flag='N', c=c, pos='noun')\n",
    "\n",
    "print('n:',F1_n,'v:',F1_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v->not_merged:9876, merged:2751\n",
      "n->not_merged:2596, merged:367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5327393941863595"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,LeaveOneOut\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def split_data(X,Y,rate):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=rate,random_state=42)\n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "\n",
    "def cal_FScore(y_true, y_pred):\n",
    "    F1=f1_score(y_true, y_pred, average='macro')\n",
    "    return F1\n",
    "\n",
    "features_path='/Users/gary/Documents/2020Fall/IntroNLP/project/'\n",
    "\n",
    "#combine OntoNotes and SENSEVAL-2\n",
    "df_features1= pd.read_csv(features_path+'feature_space.csv')\n",
    "\n",
    "df_features4= pd.read_csv(features_path+'senseval2_feature_space.csv')\n",
    "\n",
    "df_features3= pd.read_csv(features_path+'senseval2_feature_space_sf1.csv')\n",
    "\n",
    "df_features2=pd.concat([df_features4,df_features3])\n",
    "\n",
    "\n",
    "df_features1 = df_features1[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn',\n",
    "       'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'pertainyms', 'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe',\n",
    "       'hyper_min', 'hyper_max', 'wn_oed_feature']]\n",
    "\n",
    "df_features2 = df_features2[['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn',\n",
    "       'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'pertainyms', 'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe',\n",
    "       'hyper_min', 'hyper_max', 'wn_oed_feature']]\n",
    "\n",
    "df_features = pd.concat([df_features1,df_features2])\n",
    "\n",
    "df_features_n = df_features[df_features['Pos']=='n'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "        'lch', 'hso', 'jcn', 'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas','wn_oed_feature']]\n",
    "df_features_v = df_features[df_features['Pos']=='v'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "       'lch', 'hso', 'jcn','leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe','wn_oed_feature']]\n",
    "\n",
    "#replace labels with 0 or 1\n",
    "df_features_v=df_features_v.replace(['not-merged','merged'],[0,1])\n",
    "v_nm=len(df_features_v[df_features_v['Merge']==0])\n",
    "v_m=len(df_features_v[df_features_v['Merge']==1])\n",
    "print('v->not_merged:{}, merged:{}'.format(v_nm,v_m))\n",
    "\n",
    "df_features_n=df_features_n.replace(['not-merged','merged'],[0,1])\n",
    "n_nm=len(df_features_n[df_features_n['Merge']==0])\n",
    "n_m=len(df_features_n[df_features_n['Merge']==1])\n",
    "print('n->not_merged:{}, merged:{}'.format(n_nm,n_m))\n",
    "\n",
    "df_pos=df_features_v\n",
    "\n",
    "Y=np.array(df_pos['Merge'])\n",
    "X=df_pos.drop(['Pos','Sense1','Sense2','Merge'],axis=1)\n",
    "X=np.array(X)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = split_data(X,Y,0.3)\n",
    "\n",
    "#train\n",
    "clf1 = SVC(kernel='rbf', C=10).fit(X_train,Y_train)\n",
    "\n",
    "y_pred = clf1.predict(X_test)\n",
    "cal_FScore(Y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4692537313432836"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf1.predict(X_test1)\n",
    "cal_FScore(Y_test1, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.0\n"
     ]
    }
   ],
   "source": [
    "len(Y_test)\n",
    "\n",
    "# scikit-learn bootstrap\n",
    "from sklearn.utils import resample\n",
    "# data sample\n",
    "data = list(range(len(Y_test)))\n",
    "# prepare bootstrap sample\n",
    "\n",
    "diff=[]\n",
    "alpha=2*(0.5174-0.4228)\n",
    "#alpha=2*(0.5327-0.4319)\n",
    "snow=0.4228\n",
    "for i in range(3000):\n",
    "    boot = resample(data, replace=True, n_samples=len(Y_test))\n",
    "\n",
    "    X_test1 = []\n",
    "    Y_test1=[]\n",
    "    for i in boot:\n",
    "        X_test1.append(X_test[i])\n",
    "        Y_test1.append(Y_test[i])\n",
    "\n",
    "    y_pred = clf1.predict(X_test1)\n",
    "    f1=cal_FScore(Y_test1, y_pred) \n",
    "    diff.append(f1-snow)\n",
    "s=0\n",
    "for j in diff:\n",
    "    if j>(alpha):\n",
    "        s+=1\n",
    "print('p:',s/30)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_v.to_csv(features_path+'feature_space_v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pos     Sense1     Sense2  Merge       lch  hso       jcn  leskvalue  \\\n",
      "3089   n  share#n#2  share#n#4      1  1.239691  0.0  0.071893       28.0   \n",
      "\n",
      "      linvalue  resvalue  vecvalue  wupvalue  antonyms  deriv  lemmas  \\\n",
      "3089  0.069092  0.516189  0.773146  0.444444       0.0    0.0     2.0   \n",
      "\n",
      "      wn_oed_feature  \n",
      "3089             1.0  \n",
      "     Pos      Sense1      Sense2  Merge       lch  hso       jcn  leskvalue  \\\n",
      "5875   v  retain#v#1  retain#v#2      0  1.252763  0.0  0.049058        7.0   \n",
      "\n",
      "      linvalue  resvalue  vecvalue  wupvalue  antonyms  deriv  lemmas  \\\n",
      "5875       0.0       0.0  0.539534  0.222222       0.0    0.0     1.0   \n",
      "\n",
      "      verbgroup  verbframe  wn_oed_feature  \n",
      "5875        0.0        0.0             0.0  \n"
     ]
    }
   ],
   "source": [
    "#error data analysis\n",
    "#output index for nouns error data\n",
    "differ_lst_n =[]\n",
    "for i in range(len(y_pred_n)):\n",
    "    if y_pred_n[i] != y_trest_n[i]:\n",
    "        differ_lst_n.append(i)\n",
    "differ_lst_n\n",
    "\n",
    "for j in range(len(x_idex_n)):\n",
    "    if np.array_equal(x_idex_n[j],x_test_n[888]):\n",
    "        n_idx=j\n",
    "print(df_features_n[n_idx:n_idx+1])\n",
    "\n",
    "#output index for verbs error data\n",
    "differ_lst_v =[]\n",
    "for i in range(len(y_pred_v)):\n",
    "    if y_pred_v[i] != y_test_v[i]:\n",
    "        differ_lst_v.append(i)\n",
    "differ_lst_v\n",
    "\n",
    "for j in range(len(x_idex_v)):\n",
    "    if np.array_equal(x_idex_v[j],x_test_v[12]):\n",
    "        v_idx=j\n",
    "        break\n",
    "\n",
    "print(df_features_v[v_idx:v_idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Pos       Sense1       Sense2  Merge       lch  hso       jcn  leskvalue  \\\n",
      "677   n  extent#n#02  extent#n#01      1  1.691676  0.0  0.086012       51.0   \n",
      "\n",
      "     linvalue  resvalue  vecvalue  wupvalue  antonyms  deriv  lemmas  \\\n",
      "677  0.291538   2.39217  0.575617     0.625       0.0    1.0     1.0   \n",
      "\n",
      "     wn_oed_feature  \n",
      "677             0.0  \n"
     ]
    }
   ],
   "source": [
    "print(df_features_n[2601:2602])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "12\n",
      "22\n",
      "23\n",
      "33\n",
      "38\n",
      "58\n",
      "59\n",
      "72\n",
      "73\n",
      "79\n",
      "90\n",
      "95\n",
      "102\n",
      "113\n",
      "136\n",
      "139\n",
      "143\n",
      "144\n",
      "149\n",
      "179\n",
      "180\n",
      "183\n",
      "184\n",
      "200\n",
      "216\n",
      "232\n",
      "238\n",
      "241\n",
      "249\n",
      "255\n",
      "272\n",
      "275\n",
      "281\n",
      "294\n",
      "296\n",
      "301\n",
      "321\n",
      "332\n",
      "348\n",
      "350\n",
      "362\n",
      "367\n",
      "380\n",
      "399\n",
      "401\n",
      "404\n",
      "416\n",
      "423\n",
      "442\n",
      "452\n",
      "460\n",
      "473\n",
      "480\n",
      "485\n",
      "496\n",
      "502\n",
      "522\n",
      "533\n",
      "535\n",
      "552\n",
      "553\n",
      "559\n",
      "609\n",
      "618\n",
      "619\n",
      "622\n",
      "624\n",
      "634\n",
      "648\n",
      "649\n",
      "654\n",
      "657\n",
      "659\n",
      "660\n",
      "677\n",
      "683\n",
      "686\n",
      "698\n",
      "703\n",
      "711\n",
      "714\n",
      "715\n",
      "723\n",
      "731\n",
      "732\n",
      "742\n",
      "745\n",
      "747\n",
      "749\n",
      "779\n",
      "799\n",
      "800\n",
      "802\n",
      "805\n",
      "806\n",
      "809\n",
      "824\n",
      "855\n",
      "863\n",
      "869\n",
      "877\n",
      "888\n"
     ]
    }
   ],
   "source": [
    "#x_test_v[8]\n",
    "for d in differ_lst_n:\n",
    "    if y_pred_n[d]==1:\n",
    "        None\n",
    "    else:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Sense1</th>\n",
       "      <th>Sense2</th>\n",
       "      <th>Merge</th>\n",
       "      <th>lch</th>\n",
       "      <th>hso</th>\n",
       "      <th>jcn</th>\n",
       "      <th>leskvalue</th>\n",
       "      <th>linvalue</th>\n",
       "      <th>resvalue</th>\n",
       "      <th>vecvalue</th>\n",
       "      <th>wupvalue</th>\n",
       "      <th>antonyms</th>\n",
       "      <th>deriv</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>wn_oed_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#1</td>\n",
       "      <td>share#n#5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.502092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054223</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351156</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#1</td>\n",
       "      <td>share#n#2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.072637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082931</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.078864</td>\n",
       "      <td>0.516189</td>\n",
       "      <td>0.421149</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#1</td>\n",
       "      <td>share#n#3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075192</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.072035</td>\n",
       "      <td>0.516189</td>\n",
       "      <td>0.340837</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#1</td>\n",
       "      <td>share#n#4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.929536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071123</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.068403</td>\n",
       "      <td>0.516189</td>\n",
       "      <td>0.325394</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#5</td>\n",
       "      <td>share#n#2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054669</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857963</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#5</td>\n",
       "      <td>share#n#3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051196</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534629</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#5</td>\n",
       "      <td>share#n#4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049276</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722776</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#2</td>\n",
       "      <td>share#n#3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076053</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.516189</td>\n",
       "      <td>0.602785</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#2</td>\n",
       "      <td>share#n#4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.239691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071893</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.069092</td>\n",
       "      <td>0.516189</td>\n",
       "      <td>0.773146</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>n</td>\n",
       "      <td>share#n#3</td>\n",
       "      <td>share#n#4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.691676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082918</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.254765</td>\n",
       "      <td>2.061433</td>\n",
       "      <td>0.495092</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pos     Sense1     Sense2  Merge       lch  hso       jcn  leskvalue  \\\n",
       "3081   n  share#n#1  share#n#5      0  0.502092  0.0  0.054223       19.0   \n",
       "3082   n  share#n#1  share#n#2      0  1.072637  0.0  0.082931       52.0   \n",
       "3083   n  share#n#1  share#n#3      0  0.929536  0.0  0.075192       23.0   \n",
       "3084   n  share#n#1  share#n#4      0  0.929536  0.0  0.071123       27.0   \n",
       "3085   n  share#n#5  share#n#2      0  0.693147  0.0  0.054669       36.0   \n",
       "3086   n  share#n#5  share#n#3      0  0.593064  0.0  0.051196       11.0   \n",
       "3087   n  share#n#5  share#n#4      0  0.593064  0.0  0.049276       11.0   \n",
       "3088   n  share#n#2  share#n#3      1  1.239691  0.0  0.076053       26.0   \n",
       "3089   n  share#n#2  share#n#4      1  1.239691  0.0  0.071893       28.0   \n",
       "3090   n  share#n#3  share#n#4      1  1.691676  0.0  0.082918       18.0   \n",
       "\n",
       "      linvalue  resvalue  vecvalue  wupvalue  antonyms  deriv  lemmas  \\\n",
       "3081  0.000000  0.000000  0.351156  0.153846       0.0    0.0     1.0   \n",
       "3082  0.078864  0.516189  0.421149  0.400000       0.0    0.0     1.0   \n",
       "3083  0.072035  0.516189  0.340837  0.363636       0.0    0.0     1.0   \n",
       "3084  0.068403  0.516189  0.325394  0.363636       0.0    0.0     1.0   \n",
       "3085  0.000000  0.000000  0.857963  0.181818       0.0    0.0     1.0   \n",
       "3086  0.000000  0.000000  0.534629  0.166667       0.0    0.0     1.0   \n",
       "3087  0.000000  0.000000  0.722776  0.166667       0.0    0.0     1.0   \n",
       "3088  0.072800  0.516189  0.602785  0.444444       0.0    1.0     2.0   \n",
       "3089  0.069092  0.516189  0.773146  0.444444       0.0    0.0     2.0   \n",
       "3090  0.254765  2.061433  0.495092  0.700000       0.0    0.0     1.0   \n",
       "\n",
       "      wn_oed_feature  \n",
       "3081             0.0  \n",
       "3082             0.0  \n",
       "3083             0.0  \n",
       "3084             0.0  \n",
       "3085             0.0  \n",
       "3086             0.0  \n",
       "3087             0.0  \n",
       "3088             1.0  \n",
       "3089             1.0  \n",
       "3090             1.0  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_features_v[(df_features_v['Sense1'].str.contains('find')) & (df_features_v['Merge']==1)]\n",
    "\n",
    "df_features_n[df_features_n['Sense1'].str.contains('share')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'any one of a number of individual efforts in a common endeavor'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.corpus import wordnet as wn\n",
    "wn2.synset('share.n.4').definition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: lch\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 12\n",
      "Y_train sum: 258\n",
      "noun:0.5166960104141209\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.0007346289103663528\n",
      "feature: hso\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 12\n",
      "Y_train sum: 258\n",
      "noun:0.5166960104141209\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.0007346289103663528\n",
      "feature: jcn\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 10\n",
      "Y_train sum: 258\n",
      "noun:0.5091656915930929\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.008264947731394412\n",
      "feature: leskvalue\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 8\n",
      "Y_train sum: 258\n",
      "noun:0.5013764748864087\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.01605416443807861\n",
      "feature: linvalue\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 8\n",
      "Y_train sum: 258\n",
      "noun:0.4922274193797373\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.02520321994475\n",
      "feature: resvalue\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 10\n",
      "Y_train sum: 258\n",
      "noun:0.5091656915930929\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.008264947731394412\n",
      "feature: vecvalue\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 11\n",
      "Y_train sum: 258\n",
      "noun:0.5084941696823482\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.008936469642139122\n",
      "feature: wupvalue\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 11\n",
      "Y_train sum: 258\n",
      "noun:0.5174306393244873\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.0\n",
      "feature: antonyms\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 10\n",
      "Y_train sum: 258\n",
      "noun:0.5091656915930929\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.008264947731394412\n",
      "feature: deriv\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 19\n",
      "Y_train sum: 258\n",
      "noun:0.5453977272727273\n",
      "Baseline: 0.1092184368737475\n",
      "diff: -0.027967087948239988\n",
      "feature: lemmas\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 10\n",
      "Y_train sum: 258\n",
      "noun:0.5181718256923022\n",
      "Baseline: 0.1092184368737475\n",
      "diff: -0.00074118636781495\n",
      "feature: wn_oed_feature\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 11)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 11)\n",
      "y_pred sum: 10\n",
      "Y_train sum: 258\n",
      "noun:0.5091656915930929\n",
      "Baseline: 0.1092184368737475\n",
      "diff: 0.008264947731394412\n"
     ]
    }
   ],
   "source": [
    "#nouns features analysis\n",
    "n_columns=['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
    "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', 'antonyms', 'deriv',\n",
    "       'lemmas', 'wn_oed_feature']\n",
    "#n_columns.remove('lch')\n",
    "df_features_n = df_features[df_features['Pos']=='n'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "        'lch', 'hso', 'jcn', 'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas','wn_oed_feature']]\n",
    "\n",
    "for feature in n_columns[4:]:\n",
    "    print('feature:',feature)\n",
    "    pru_col=['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
    "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', 'antonyms', 'deriv',\n",
    "       'lemmas', 'wn_oed_feature']\n",
    "    df_features_n = df_features[df_features['Pos']=='n'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "        'lch', 'hso', 'jcn', 'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas','wn_oed_feature']]\n",
    "    pru_col.remove(feature)\n",
    "    df_features_n = df_features_n[pru_col]\n",
    "    df_features_n=df_features_n.replace(['not-merged','merged'],[0,1])\n",
    "    n_nm=len(df_features_n[df_features_n['Merge']==0])\n",
    "    n_m=len(df_features_n[df_features_n['Merge']==1])\n",
    "    print('n->not_merged:{}, merged:{}'.format(n_nm,n_m))\n",
    "    #df_features_v\n",
    "\n",
    "    #Fscore(df_pos=df_features_v, tune_flag='Y', c=None)\n",
    "    #c=1\n",
    "    #Fscore(df_pos=df_features_v, tune_flag='N', c=1,pos='verb')\n",
    "    c=10\n",
    "    F1_n_c=Fscore(df_pos=df_features_n, tune_flag='N', c=c, pos='noun')\n",
    "    print('diff:',F1_n-F1_n_c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v->not_merged:9876, merged:2751\n",
      "n->not_merged:2596, merged:367\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 14)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 14)\n",
      "verb:0.4777483363645664\n",
      "Baseline: 0.18340517241379312\n",
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 12)\n",
      "shape of Y :(2074,)\n",
      "shape of X :(2074, 12)\n",
      "noun:0.4673457159976034\n",
      "Baseline: 0.1092184368737475\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pos',\n",
       " 'Sense1',\n",
       " 'Sense2',\n",
       " 'Merge',\n",
       " 'lch',\n",
       " 'jcn',\n",
       " 'leskvalue',\n",
       " 'linvalue',\n",
       " 'resvalue',\n",
       " 'vecvalue',\n",
       " 'wupvalue',\n",
       " 'antonyms',\n",
       " 'deriv',\n",
       " 'lemmas',\n",
       " 'wn_oed_feature']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pru_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=np.array([1,0,0,1,1])\n",
    "np.sum(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: lch\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 144\n",
      "Y_train sum: 1900\n",
      "verb:0.535491982165117\n",
      "Baseline: 0.18340517241379312\n",
      "differ: -0.002752587978757437\n",
      "feature: hso\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 129\n",
      "Y_train sum: 1900\n",
      "verb:0.5230085183512628\n",
      "Baseline: 0.18340517241379312\n",
      "differ: 0.009730875835096686\n",
      "feature: jcn\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 147\n",
      "Y_train sum: 1900\n",
      "verb:0.5395621029292629\n",
      "Baseline: 0.18340517241379312\n",
      "differ: -0.006822708742903361\n",
      "feature: leskvalue\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 144\n",
      "Y_train sum: 1900\n",
      "verb:0.5366489137163869\n",
      "Baseline: 0.18340517241379312\n",
      "differ: -0.003909519530027383\n",
      "feature: linvalue\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 117\n",
      "Y_train sum: 1900\n",
      "verb:0.5215251122141509\n",
      "Baseline: 0.18340517241379312\n",
      "differ: 0.01121428197220864\n",
      "feature: resvalue\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 134\n",
      "Y_train sum: 1900\n",
      "verb:0.5279871514242532\n",
      "Baseline: 0.18340517241379312\n",
      "differ: 0.004752242762106285\n",
      "feature: vecvalue\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 136\n",
      "Y_train sum: 1900\n",
      "verb:0.5322953516331334\n",
      "Baseline: 0.18340517241379312\n",
      "differ: 0.0004440425532261649\n",
      "feature: wupvalue\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 141\n",
      "Y_train sum: 1900\n",
      "verb:0.5348793996062029\n",
      "Baseline: 0.18340517241379312\n",
      "differ: -0.0021400054198433827\n",
      "feature: antonyms\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 136\n",
      "Y_train sum: 1900\n",
      "verb:0.5322953516331334\n",
      "Baseline: 0.18340517241379312\n",
      "differ: 0.0004440425532261649\n",
      "feature: deriv\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 136\n",
      "Y_train sum: 1900\n",
      "verb:0.518316632379329\n",
      "Baseline: 0.18340517241379312\n",
      "differ: 0.014422761807030504\n",
      "feature: lemmas\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 124\n",
      "Y_train sum: 1900\n",
      "verb:0.5273994338237864\n",
      "Baseline: 0.18340517241379312\n",
      "differ: 0.005339960362573115\n",
      "feature: verbgroup\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 140\n",
      "Y_train sum: 1900\n",
      "verb:0.5327393941863595\n",
      "Baseline: 0.18340517241379312\n",
      "differ: 0.0\n",
      "feature: verbframe\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 148\n",
      "Y_train sum: 1900\n",
      "verb:0.534764586748171\n",
      "Baseline: 0.18340517241379312\n",
      "differ: -0.0020251925618114397\n",
      "feature: wn_oed_feature\n",
      "shape of Y :(12627,)\n",
      "shape of X :(12627, 13)\n",
      "shape of Y :(8838,)\n",
      "shape of X :(8838, 13)\n",
      "y_pred sum: 158\n",
      "Y_train sum: 1900\n",
      "verb:0.5398175440671648\n",
      "Baseline: 0.18340517241379312\n",
      "differ: -0.00707814988080524\n"
     ]
    }
   ],
   "source": [
    "#verb features analysis\n",
    "df_features_v = df_features[df_features['Pos']=='v'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "       'lch', 'hso', 'jcn','leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe','wn_oed_feature']]\n",
    "\n",
    "v_columns=['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
    "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', 'antonyms', 'deriv',\n",
    "       'lemmas', 'verbgroup', 'verbframe', 'wn_oed_feature']\n",
    "#replace labels with 0 or 1\n",
    "for feature in v_columns[4:]:\n",
    "    print('feature:',feature)\n",
    "    df_features_v = df_features[df_features['Pos']=='v'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "       'lch', 'hso', 'jcn','leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas', 'verbgroup', 'verbframe','wn_oed_feature']]\n",
    "    pru_col_v=['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
    "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', 'antonyms', 'deriv',\n",
    "       'lemmas', 'verbgroup', 'verbframe', 'wn_oed_feature']\n",
    "\n",
    "    pru_col_v.remove(feature)\n",
    "    df_features_v=df_features_v[pru_col_v]\n",
    "    df_features_v=df_features_v.replace(['not-merged','merged'],[0,1])\n",
    "    v_nm=len(df_features_v[df_features_v['Merge']==0])\n",
    "    v_m=len(df_features_v[df_features_v['Merge']==1])\n",
    "    F1_v_c=Fscore(df_pos=df_features_v, tune_flag='N', c=10,pos='verb')\n",
    "    print('differ:',F1_v-F1_v_c)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pos', 'Sense1', 'Sense2', 'Merge', 'lch', 'hso', 'jcn', 'leskvalue',\n",
       "       'linvalue', 'resvalue', 'vecvalue', 'wupvalue', 'antonyms', 'deriv',\n",
       "       'lemmas', 'verbgroup', 'verbframe', 'wn_oed_feature'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_v.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by adding SENSEVAL-2 data, F1 score decreasing\n",
    "verb: 0.4819\n",
    "noun: 0.4654 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Y :(2963,)\n",
      "shape of X :(2963, 12)\n",
      "c:10000.0,correct:1910,ones:94\n",
      "c:100000.0,correct:1944,ones:133\n",
      "c:1000000.0,correct:1970,ones:159\n",
      "c:10000000.0,correct:1944,ones:167\n"
     ]
    }
   ],
   "source": [
    "#solve always the same value\n",
    "df_features_n = df_features[df_features['Pos']=='n'][['Pos', 'Sense1', 'Sense2', 'Merge', \n",
    "        'lch', 'hso', 'jcn', 'leskvalue', 'linvalue', 'resvalue', 'vecvalue', 'wupvalue',\n",
    "       'antonyms', 'deriv', 'lemmas','wn_oed_feature']]\n",
    "\n",
    "df_features_n=df_features_n.replace(['not-merged','merged'],[0,1])\n",
    "df_pos= df_features_n\n",
    "Y=np.array(df_pos['Merge'])\n",
    "X=df_pos.drop(['Pos','Sense1','Sense2','Merge'],axis=1)\n",
    "X=np.array(X)\n",
    "#features_names=['1','2','3','4','5','6','7','8','9','10','11','12','13','14']\n",
    "print(\"shape of Y :\"+str(Y.shape))\n",
    "print(\"shape of X :\"+str(X.shape))\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "C=[1e4,1e5,1e6,1e7,1e8]\n",
    "gammas=[1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "X_train,X_test,Y_train,Y_test = split_data(X,Y,0.3)\n",
    "for c in C:\n",
    "    \n",
    "    #for g in gammas:\n",
    "        #print('c=',c,'gamma=',g)\n",
    "    clf1 = SVC(kernel='rbf',C=c)\n",
    "    clf1.fit(X_train,Y_train)\n",
    "    y_pr=clf1.predict(X_train)\n",
    "    correct=0\n",
    "    ones=0\n",
    "    for i in range(len(Y_train)):\n",
    "        if Y_train[i]==y_pr[i]:\n",
    "            correct+=1\n",
    "            if y_pr[i]==1:\n",
    "                ones+=1\n",
    "    print('c:{},correct:{},ones:{}'.format(c,correct,ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
